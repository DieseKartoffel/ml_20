{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "#must be very first statement\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import Callback, LearningRateScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.datasets import mnist\n",
    "import PIL\n",
    "\n",
    "\n",
    "def getDigits(show=False):\n",
    "    (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "    #print(x_train.shape)\n",
    "\n",
    "    if show:\n",
    "        showImages(x_train, t_train,5)\n",
    "\n",
    "    return x_train,t_train,x_test, t_test\n",
    "\n",
    "\n",
    "#### CALLBACKS (https://github.com/davidADSP/GDL_code/blob/master/utils/callbacks.py)\n",
    "class CustomCallback(Callback):\n",
    "\n",
    "    def __init__(self, run_folder, print_every_n_batches, initial_epoch, vae):\n",
    "        self.epoch = initial_epoch\n",
    "        self.run_folder = run_folder\n",
    "        self.print_every_n_batches = print_every_n_batches\n",
    "        self.vae = vae\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if batch % self.print_every_n_batches == 0:\n",
    "            z_new = np.random.normal(size=(1, self.vae.z_dim))\n",
    "            reconst = self.vae.decoder.predict(np.array(z_new))[0].squeeze()\n",
    "\n",
    "            filepath = os.path.join(self.run_folder, 'images',\n",
    "                                    'img_' + str(self.epoch).zfill(3) + '_' + str(batch) + '.jpg')\n",
    "            if len(reconst.shape) == 2:\n",
    "                plt.imsave(filepath, reconst, cmap='gray_r')\n",
    "            else:\n",
    "                plt.imsave(filepath, reconst)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch += 1\n",
    "\n",
    "#### CALLBACKS (https://github.com/davidADSP/GDL_code/blob/master/utils/callbacks.py)\n",
    "def step_decay_schedule(initial_lr, decay_factor=0.5, step_size=1):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "\n",
    "    def schedule(epoch):\n",
    "        new_lr = initial_lr * (decay_factor ** np.floor(epoch / step_size))\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "    return LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "class VariationalAutoencoder():\n",
    "    def __init__(self\n",
    "                 , input_dim\n",
    "                 , encoder_conv_filters\n",
    "                 , encoder_conv_kernel_size\n",
    "                 , encoder_conv_strides\n",
    "                 , decoder_conv_t_filters\n",
    "                 , decoder_conv_t_kernel_size\n",
    "                 , decoder_conv_t_strides\n",
    "                 , z_dim\n",
    "                 , use_batch_norm=False\n",
    "                 , use_dropout=False\n",
    "                 ):\n",
    "\n",
    "        self.name = 'variational_autoencoder'\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_conv_filters = encoder_conv_filters\n",
    "        self.encoder_conv_kernel_size = encoder_conv_kernel_size\n",
    "        self.encoder_conv_strides = encoder_conv_strides\n",
    "        self.decoder_conv_t_filters = decoder_conv_t_filters\n",
    "        self.decoder_conv_t_kernel_size = decoder_conv_t_kernel_size\n",
    "        self.decoder_conv_t_strides = decoder_conv_t_strides\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        self.n_layers_encoder = len(encoder_conv_filters)\n",
    "        self.n_layers_decoder = len(decoder_conv_t_filters)\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        ### THE ENCODER\n",
    "        encoder_input = Input(shape=self.input_dim, name='encoder_input')\n",
    "\n",
    "        x = encoder_input\n",
    "\n",
    "        for i in range(self.n_layers_encoder):\n",
    "            conv_layer = Conv2D(\n",
    "                filters=self.encoder_conv_filters[i]\n",
    "                , kernel_size=self.encoder_conv_kernel_size[i]\n",
    "                , strides=self.encoder_conv_strides[i]\n",
    "                , padding='same'\n",
    "                , name='encoder_conv_' + str(i)\n",
    "            )\n",
    "\n",
    "            x = conv_layer(x)\n",
    "\n",
    "            if self.use_batch_norm:\n",
    "                x = BatchNormalization()(x)\n",
    "\n",
    "            x = LeakyReLU()(x)\n",
    "\n",
    "            if self.use_dropout:\n",
    "                x = Dropout(rate=0.25)(x)\n",
    "\n",
    "        shape_before_flattening = K.int_shape(x)[1:]\n",
    "#-----------------------------\n",
    "        print(\"shape_bef_flat\",shape_before_flattening)\n",
    "        x = Flatten()(x)\n",
    "        print(\"shape_aft_flat\",x)\n",
    "        self.mu = Dense(self.z_dim, name='mu')(x)\n",
    "        self.log_var = Dense(self.z_dim, name='log_var')(x)\n",
    "\n",
    "        self.encoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\n",
    "\n",
    "        def sampling(args):\n",
    "            mu, log_var = args\n",
    "            epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)\n",
    "            return mu + K.exp(log_var / 2) * epsilon\n",
    "\n",
    "        encoder_output = Lambda(sampling, name='encoder_output')([self.mu, self.log_var])\n",
    "        print(\"encoder_output: \",encoder_output)\n",
    "        self.encoder = Model(encoder_input, encoder_output)\n",
    "\n",
    "        ### THE DECODER\n",
    "\n",
    "        decoder_input = Input(shape=(self.z_dim,), name='decoder_input')\n",
    "        print(\"dec_input\",decoder_input)\n",
    "        x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "        x = Reshape(shape_before_flattening)(x)\n",
    "\n",
    "        for i in range(self.n_layers_decoder):\n",
    "            conv_t_layer = Conv2DTranspose(\n",
    "                filters=self.decoder_conv_t_filters[i]\n",
    "                , kernel_size=self.decoder_conv_t_kernel_size[i]\n",
    "                , strides=self.decoder_conv_t_strides[i]\n",
    "                , padding='same'\n",
    "                , name='decoder_conv_t_' + str(i)\n",
    "            )\n",
    "\n",
    "            x = conv_t_layer(x)\n",
    "\n",
    "            if i < self.n_layers_decoder - 1:\n",
    "                if self.use_batch_norm:\n",
    "                    x = BatchNormalization()(x)\n",
    "                x = LeakyReLU()(x)\n",
    "                if self.use_dropout:\n",
    "                    x = Dropout(rate=0.25)(x)\n",
    "            else:\n",
    "                x = Activation('sigmoid')(x)\n",
    "\n",
    "        decoder_output = x\n",
    "        print(\"dec_inputtii\",decoder_input)\n",
    "        self.decoder = Model(decoder_input, decoder_output)\n",
    "\n",
    "        ### THE FULL VAE\n",
    "        model_input = encoder_input\n",
    "        model_output = self.decoder(encoder_output)\n",
    "        \n",
    "        self.model = Model(model_input, model_output)\n",
    "\n",
    "    def compile(self, learning_rate, r_loss_factor):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        ### COMPILATION\n",
    "        def vae_r_loss(y_true, y_pred):\n",
    "            r_loss = K.mean(K.square(y_true - y_pred), axis=[1, 2, 3])\n",
    "            return r_loss_factor * r_loss\n",
    "\n",
    "        def vae_kl_loss(y_true, y_pred):\n",
    "            kl_loss = -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis=1)\n",
    "            return kl_loss\n",
    "\n",
    "        def vae_loss(y_true, y_pred):\n",
    "            r_loss = vae_r_loss(y_true, y_pred)\n",
    "            kl_loss = vae_kl_loss(y_true, y_pred)\n",
    "            return r_loss + kl_loss\n",
    "\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss=vae_loss, metrics=[vae_r_loss, vae_kl_loss])\n",
    "\n",
    "    def save(self, folder):\n",
    "\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            os.makedirs(os.path.join(folder, 'viz'))\n",
    "            os.makedirs(os.path.join(folder, 'weights'))\n",
    "            os.makedirs(os.path.join(folder, 'images'))\n",
    "\n",
    "        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n",
    "            pickle.dump([\n",
    "                self.input_dim\n",
    "                , self.encoder_conv_filters\n",
    "                , self.encoder_conv_kernel_size\n",
    "                , self.encoder_conv_strides\n",
    "                , self.decoder_conv_t_filters\n",
    "                , self.decoder_conv_t_kernel_size\n",
    "                , self.decoder_conv_t_strides\n",
    "                , self.z_dim\n",
    "                , self.use_batch_norm\n",
    "                , self.use_dropout\n",
    "            ], f)\n",
    "\n",
    "        self.plot_model(folder)\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "\n",
    "    def train(self, x_train, batch_size, epochs, run_folder, print_every_n_batches=100, initial_epoch=0, lr_decay=1):\n",
    "\n",
    "        custom_callback = CustomCallback(run_folder, print_every_n_batches, initial_epoch, self)\n",
    "        lr_sched = step_decay_schedule(initial_lr=self.learning_rate, decay_factor=lr_decay, step_size=1)\n",
    "\n",
    "        checkpoint_filepath = os.path.join(run_folder, \"weights/weights-{epoch:03d}-{loss:.2f}.h5\")\n",
    "        checkpoint1 = ModelCheckpoint(checkpoint_filepath, save_weights_only=True, verbose=1)\n",
    "        checkpoint2 = ModelCheckpoint(os.path.join(run_folder, 'weights/weights.h5'), save_weights_only=True, verbose=1)\n",
    "\n",
    "        callbacks_list = [checkpoint1, checkpoint2, custom_callback, lr_sched]\n",
    "\n",
    "        self.model.fit(\n",
    "            x_train\n",
    "            , x_train\n",
    "            , batch_size=batch_size\n",
    "            , shuffle=True\n",
    "            , epochs=epochs\n",
    "            , initial_epoch=initial_epoch\n",
    "            , callbacks=callbacks_list\n",
    "        )\n",
    "\n",
    "    def train_with_generator(self, data_flow, epochs, steps_per_epoch, run_folder, print_every_n_batches=100,\n",
    "                             initial_epoch=0, lr_decay=1, ):\n",
    "\n",
    "        custom_callback = utilities.CustomCallback(run_folder, print_every_n_batches, initial_epoch, self)\n",
    "        lr_sched = utilities.step_decay_schedule(initial_lr=self.learning_rate, decay_factor=lr_decay, step_size=1)\n",
    "\n",
    "        checkpoint_filepath = os.path.join(run_folder, \"weights/weights-{epoch:03d}-{loss:.2f}.h5\")\n",
    "        checkpoint1 = ModelCheckpoint(checkpoint_filepath, save_weights_only=True, verbose=1)\n",
    "        checkpoint2 = ModelCheckpoint(os.path.join(run_folder, 'weights/weights.h5'), save_weights_only=True, verbose=1)\n",
    "\n",
    "        callbacks_list = [checkpoint1, checkpoint2, custom_callback, lr_sched]\n",
    "\n",
    "        self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n",
    "\n",
    "        self.model.fit_generator(\n",
    "            data_flow\n",
    "            , shuffle=True\n",
    "            , epochs=epochs\n",
    "            , initial_epoch=initial_epoch\n",
    "            , callbacks=callbacks_list\n",
    "            , steps_per_epoch=steps_per_epoch\n",
    "        )\n",
    "\n",
    "    def plot_model(self, run_folder):\n",
    "        plot_model(self.model, to_file=os.path.join(run_folder, 'viz/model.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "        plot_model(self.encoder, to_file=os.path.join(run_folder, 'viz/encoder.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "        plot_model(self.decoder, to_file=os.path.join(run_folder, 'viz/decoder.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 33s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([[[[ 59,  62,  63],\n",
       "           [ 43,  46,  45],\n",
       "           [ 50,  48,  43],\n",
       "           ...,\n",
       "           [158, 132, 108],\n",
       "           [152, 125, 102],\n",
       "           [148, 124, 103]],\n",
       "  \n",
       "          [[ 16,  20,  20],\n",
       "           [  0,   0,   0],\n",
       "           [ 18,   8,   0],\n",
       "           ...,\n",
       "           [123,  88,  55],\n",
       "           [119,  83,  50],\n",
       "           [122,  87,  57]],\n",
       "  \n",
       "          [[ 25,  24,  21],\n",
       "           [ 16,   7,   0],\n",
       "           [ 49,  27,   8],\n",
       "           ...,\n",
       "           [118,  84,  50],\n",
       "           [120,  84,  50],\n",
       "           [109,  73,  42]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[208, 170,  96],\n",
       "           [201, 153,  34],\n",
       "           [198, 161,  26],\n",
       "           ...,\n",
       "           [160, 133,  70],\n",
       "           [ 56,  31,   7],\n",
       "           [ 53,  34,  20]],\n",
       "  \n",
       "          [[180, 139,  96],\n",
       "           [173, 123,  42],\n",
       "           [186, 144,  30],\n",
       "           ...,\n",
       "           [184, 148,  94],\n",
       "           [ 97,  62,  34],\n",
       "           [ 83,  53,  34]],\n",
       "  \n",
       "          [[177, 144, 116],\n",
       "           [168, 129,  94],\n",
       "           [179, 142,  87],\n",
       "           ...,\n",
       "           [216, 184, 140],\n",
       "           [151, 118,  84],\n",
       "           [123,  92,  72]]],\n",
       "  \n",
       "  \n",
       "         [[[154, 177, 187],\n",
       "           [126, 137, 136],\n",
       "           [105, 104,  95],\n",
       "           ...,\n",
       "           [ 91,  95,  71],\n",
       "           [ 87,  90,  71],\n",
       "           [ 79,  81,  70]],\n",
       "  \n",
       "          [[140, 160, 169],\n",
       "           [145, 153, 154],\n",
       "           [125, 125, 118],\n",
       "           ...,\n",
       "           [ 96,  99,  78],\n",
       "           [ 77,  80,  62],\n",
       "           [ 71,  73,  61]],\n",
       "  \n",
       "          [[140, 155, 164],\n",
       "           [139, 146, 149],\n",
       "           [115, 115, 112],\n",
       "           ...,\n",
       "           [ 79,  82,  64],\n",
       "           [ 68,  70,  55],\n",
       "           [ 67,  69,  55]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[175, 167, 166],\n",
       "           [156, 154, 160],\n",
       "           [154, 160, 170],\n",
       "           ...,\n",
       "           [ 42,  34,  36],\n",
       "           [ 61,  53,  57],\n",
       "           [ 93,  83,  91]],\n",
       "  \n",
       "          [[165, 154, 128],\n",
       "           [156, 152, 130],\n",
       "           [159, 161, 142],\n",
       "           ...,\n",
       "           [103,  93,  96],\n",
       "           [123, 114, 120],\n",
       "           [131, 121, 131]],\n",
       "  \n",
       "          [[163, 148, 120],\n",
       "           [158, 148, 122],\n",
       "           [163, 156, 133],\n",
       "           ...,\n",
       "           [143, 133, 139],\n",
       "           [143, 134, 142],\n",
       "           [143, 133, 144]]],\n",
       "  \n",
       "  \n",
       "         [[[255, 255, 255],\n",
       "           [253, 253, 253],\n",
       "           [253, 253, 253],\n",
       "           ...,\n",
       "           [253, 253, 253],\n",
       "           [253, 253, 253],\n",
       "           [253, 253, 253]],\n",
       "  \n",
       "          [[255, 255, 255],\n",
       "           [255, 255, 255],\n",
       "           [255, 255, 255],\n",
       "           ...,\n",
       "           [255, 255, 255],\n",
       "           [255, 255, 255],\n",
       "           [255, 255, 255]],\n",
       "  \n",
       "          [[255, 255, 255],\n",
       "           [254, 254, 254],\n",
       "           [254, 254, 254],\n",
       "           ...,\n",
       "           [254, 254, 254],\n",
       "           [254, 254, 254],\n",
       "           [254, 254, 254]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[113, 120, 112],\n",
       "           [111, 118, 111],\n",
       "           [105, 112, 106],\n",
       "           ...,\n",
       "           [ 72,  81,  80],\n",
       "           [ 72,  80,  79],\n",
       "           [ 72,  80,  79]],\n",
       "  \n",
       "          [[111, 118, 110],\n",
       "           [104, 111, 104],\n",
       "           [ 99, 106,  98],\n",
       "           ...,\n",
       "           [ 68,  75,  73],\n",
       "           [ 70,  76,  75],\n",
       "           [ 78,  84,  82]],\n",
       "  \n",
       "          [[106, 113, 105],\n",
       "           [ 99, 106,  98],\n",
       "           [ 95, 102,  94],\n",
       "           ...,\n",
       "           [ 78,  85,  83],\n",
       "           [ 79,  85,  83],\n",
       "           [ 80,  86,  84]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[ 35, 178, 235],\n",
       "           [ 40, 176, 239],\n",
       "           [ 42, 176, 241],\n",
       "           ...,\n",
       "           [ 99, 177, 219],\n",
       "           [ 79, 147, 197],\n",
       "           [ 89, 148, 189]],\n",
       "  \n",
       "          [[ 57, 182, 234],\n",
       "           [ 44, 184, 250],\n",
       "           [ 50, 183, 240],\n",
       "           ...,\n",
       "           [156, 182, 200],\n",
       "           [141, 177, 206],\n",
       "           [116, 149, 175]],\n",
       "  \n",
       "          [[ 98, 197, 237],\n",
       "           [ 64, 189, 252],\n",
       "           [ 69, 192, 245],\n",
       "           ...,\n",
       "           [188, 195, 206],\n",
       "           [119, 135, 147],\n",
       "           [ 61,  79,  90]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 73,  79,  77],\n",
       "           [ 53,  63,  68],\n",
       "           [ 54,  68,  80],\n",
       "           ...,\n",
       "           [ 17,  40,  64],\n",
       "           [ 21,  36,  51],\n",
       "           [ 33,  48,  49]],\n",
       "  \n",
       "          [[ 61,  68,  75],\n",
       "           [ 55,  70,  86],\n",
       "           [ 57,  79, 103],\n",
       "           ...,\n",
       "           [ 24,  48,  72],\n",
       "           [ 17,  35,  53],\n",
       "           [  7,  23,  32]],\n",
       "  \n",
       "          [[ 44,  56,  73],\n",
       "           [ 46,  66,  88],\n",
       "           [ 49,  77, 105],\n",
       "           ...,\n",
       "           [ 27,  52,  77],\n",
       "           [ 21,  43,  66],\n",
       "           [ 12,  31,  50]]],\n",
       "  \n",
       "  \n",
       "         [[[189, 211, 240],\n",
       "           [186, 208, 236],\n",
       "           [185, 207, 235],\n",
       "           ...,\n",
       "           [175, 195, 224],\n",
       "           [172, 194, 222],\n",
       "           [169, 194, 220]],\n",
       "  \n",
       "          [[194, 210, 239],\n",
       "           [191, 207, 236],\n",
       "           [190, 206, 235],\n",
       "           ...,\n",
       "           [173, 192, 220],\n",
       "           [171, 191, 218],\n",
       "           [167, 190, 216]],\n",
       "  \n",
       "          [[208, 219, 244],\n",
       "           [205, 216, 240],\n",
       "           [204, 215, 239],\n",
       "           ...,\n",
       "           [175, 191, 217],\n",
       "           [172, 190, 216],\n",
       "           [169, 191, 215]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[207, 199, 181],\n",
       "           [203, 195, 175],\n",
       "           [203, 196, 173],\n",
       "           ...,\n",
       "           [135, 132, 127],\n",
       "           [162, 158, 150],\n",
       "           [168, 163, 151]],\n",
       "  \n",
       "          [[198, 190, 170],\n",
       "           [189, 181, 159],\n",
       "           [180, 172, 147],\n",
       "           ...,\n",
       "           [178, 171, 160],\n",
       "           [175, 169, 156],\n",
       "           [175, 169, 154]],\n",
       "  \n",
       "          [[198, 189, 173],\n",
       "           [189, 181, 162],\n",
       "           [178, 170, 149],\n",
       "           ...,\n",
       "           [195, 184, 169],\n",
       "           [196, 189, 171],\n",
       "           [195, 190, 171]]],\n",
       "  \n",
       "  \n",
       "         [[[229, 229, 239],\n",
       "           [236, 237, 247],\n",
       "           [234, 236, 247],\n",
       "           ...,\n",
       "           [217, 219, 233],\n",
       "           [221, 223, 234],\n",
       "           [222, 223, 233]],\n",
       "  \n",
       "          [[222, 221, 229],\n",
       "           [239, 239, 249],\n",
       "           [233, 234, 246],\n",
       "           ...,\n",
       "           [223, 223, 236],\n",
       "           [227, 228, 238],\n",
       "           [210, 211, 220]],\n",
       "  \n",
       "          [[213, 206, 211],\n",
       "           [234, 232, 239],\n",
       "           [231, 233, 244],\n",
       "           ...,\n",
       "           [220, 220, 232],\n",
       "           [220, 219, 232],\n",
       "           [202, 203, 215]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[150, 143, 135],\n",
       "           [140, 135, 127],\n",
       "           [132, 127, 120],\n",
       "           ...,\n",
       "           [224, 222, 218],\n",
       "           [230, 228, 225],\n",
       "           [241, 241, 238]],\n",
       "  \n",
       "          [[137, 132, 126],\n",
       "           [130, 127, 120],\n",
       "           [125, 121, 115],\n",
       "           ...,\n",
       "           [181, 180, 178],\n",
       "           [202, 201, 198],\n",
       "           [212, 211, 207]],\n",
       "  \n",
       "          [[122, 119, 114],\n",
       "           [118, 116, 110],\n",
       "           [120, 116, 111],\n",
       "           ...,\n",
       "           [179, 177, 173],\n",
       "           [164, 164, 162],\n",
       "           [163, 163, 161]]]], dtype=uint8),\n",
       "  array([[6],\n",
       "         [9],\n",
       "         [9],\n",
       "         ...,\n",
       "         [9],\n",
       "         [1],\n",
       "         [1]], dtype=uint8)),\n",
       " (array([[[[158, 112,  49],\n",
       "           [159, 111,  47],\n",
       "           [165, 116,  51],\n",
       "           ...,\n",
       "           [137,  95,  36],\n",
       "           [126,  91,  36],\n",
       "           [116,  85,  33]],\n",
       "  \n",
       "          [[152, 112,  51],\n",
       "           [151, 110,  40],\n",
       "           [159, 114,  45],\n",
       "           ...,\n",
       "           [136,  95,  31],\n",
       "           [125,  91,  32],\n",
       "           [119,  88,  34]],\n",
       "  \n",
       "          [[151, 110,  47],\n",
       "           [151, 109,  33],\n",
       "           [158, 111,  36],\n",
       "           ...,\n",
       "           [139,  98,  34],\n",
       "           [130,  95,  34],\n",
       "           [120,  89,  33]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 68, 124, 177],\n",
       "           [ 42, 100, 148],\n",
       "           [ 31,  88, 137],\n",
       "           ...,\n",
       "           [ 38,  97, 146],\n",
       "           [ 13,  64, 108],\n",
       "           [ 40,  85, 127]],\n",
       "  \n",
       "          [[ 61, 116, 168],\n",
       "           [ 49, 102, 148],\n",
       "           [ 35,  85, 132],\n",
       "           ...,\n",
       "           [ 26,  82, 130],\n",
       "           [ 29,  82, 126],\n",
       "           [ 20,  64, 107]],\n",
       "  \n",
       "          [[ 54, 107, 160],\n",
       "           [ 56, 105, 149],\n",
       "           [ 45,  89, 132],\n",
       "           ...,\n",
       "           [ 24,  77, 124],\n",
       "           [ 34,  84, 129],\n",
       "           [ 21,  67, 110]]],\n",
       "  \n",
       "  \n",
       "         [[[235, 235, 235],\n",
       "           [231, 231, 231],\n",
       "           [232, 232, 232],\n",
       "           ...,\n",
       "           [233, 233, 233],\n",
       "           [233, 233, 233],\n",
       "           [232, 232, 232]],\n",
       "  \n",
       "          [[238, 238, 238],\n",
       "           [235, 235, 235],\n",
       "           [235, 235, 235],\n",
       "           ...,\n",
       "           [236, 236, 236],\n",
       "           [236, 236, 236],\n",
       "           [235, 235, 235]],\n",
       "  \n",
       "          [[237, 237, 237],\n",
       "           [234, 234, 234],\n",
       "           [234, 234, 234],\n",
       "           ...,\n",
       "           [235, 235, 235],\n",
       "           [235, 235, 235],\n",
       "           [234, 234, 234]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 87,  99,  89],\n",
       "           [ 43,  51,  37],\n",
       "           [ 19,  23,  11],\n",
       "           ...,\n",
       "           [169, 184, 179],\n",
       "           [182, 197, 193],\n",
       "           [188, 202, 201]],\n",
       "  \n",
       "          [[ 82,  96,  82],\n",
       "           [ 46,  57,  36],\n",
       "           [ 36,  44,  22],\n",
       "           ...,\n",
       "           [174, 189, 183],\n",
       "           [185, 200, 196],\n",
       "           [187, 202, 200]],\n",
       "  \n",
       "          [[ 85, 101,  83],\n",
       "           [ 62,  75,  48],\n",
       "           [ 58,  67,  38],\n",
       "           ...,\n",
       "           [168, 183, 178],\n",
       "           [180, 195, 191],\n",
       "           [186, 200, 199]]],\n",
       "  \n",
       "  \n",
       "         [[[158, 190, 222],\n",
       "           [158, 187, 218],\n",
       "           [139, 166, 194],\n",
       "           ...,\n",
       "           [228, 231, 234],\n",
       "           [237, 239, 243],\n",
       "           [238, 241, 246]],\n",
       "  \n",
       "          [[170, 200, 229],\n",
       "           [172, 199, 226],\n",
       "           [151, 176, 201],\n",
       "           ...,\n",
       "           [232, 232, 236],\n",
       "           [246, 246, 250],\n",
       "           [246, 247, 251]],\n",
       "  \n",
       "          [[174, 201, 225],\n",
       "           [176, 200, 222],\n",
       "           [157, 179, 199],\n",
       "           ...,\n",
       "           [230, 229, 232],\n",
       "           [250, 249, 251],\n",
       "           [245, 244, 247]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 31,  40,  45],\n",
       "           [ 30,  39,  44],\n",
       "           [ 26,  35,  40],\n",
       "           ...,\n",
       "           [ 37,  40,  46],\n",
       "           [  9,  13,  14],\n",
       "           [  4,   7,   5]],\n",
       "  \n",
       "          [[ 23,  34,  39],\n",
       "           [ 27,  38,  43],\n",
       "           [ 25,  36,  41],\n",
       "           ...,\n",
       "           [ 19,  20,  24],\n",
       "           [  4,   6,   3],\n",
       "           [  5,   7,   3]],\n",
       "  \n",
       "          [[ 28,  41,  47],\n",
       "           [ 30,  43,  50],\n",
       "           [ 32,  45,  52],\n",
       "           ...,\n",
       "           [  5,   6,   8],\n",
       "           [  4,   5,   3],\n",
       "           [  7,   8,   7]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[ 20,  15,  12],\n",
       "           [ 19,  14,  11],\n",
       "           [ 15,  14,  11],\n",
       "           ...,\n",
       "           [ 10,   9,   7],\n",
       "           [ 12,  11,   9],\n",
       "           [ 13,  12,  10]],\n",
       "  \n",
       "          [[ 21,  16,  13],\n",
       "           [ 20,  16,  13],\n",
       "           [ 18,  17,  12],\n",
       "           ...,\n",
       "           [ 10,   9,   7],\n",
       "           [ 10,   9,   7],\n",
       "           [ 12,  11,   9]],\n",
       "  \n",
       "          [[ 21,  16,  13],\n",
       "           [ 21,  17,  12],\n",
       "           [ 20,  18,  11],\n",
       "           ...,\n",
       "           [ 12,  11,   9],\n",
       "           [ 12,  11,   9],\n",
       "           [ 13,  12,  10]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 33,  25,  13],\n",
       "           [ 34,  26,  15],\n",
       "           [ 34,  26,  15],\n",
       "           ...,\n",
       "           [ 28,  25,  52],\n",
       "           [ 29,  25,  58],\n",
       "           [ 23,  20,  42]],\n",
       "  \n",
       "          [[ 33,  25,  14],\n",
       "           [ 34,  26,  15],\n",
       "           [ 34,  26,  15],\n",
       "           ...,\n",
       "           [ 27,  24,  52],\n",
       "           [ 27,  24,  56],\n",
       "           [ 25,  22,  47]],\n",
       "  \n",
       "          [[ 31,  23,  12],\n",
       "           [ 32,  24,  13],\n",
       "           [ 33,  25,  14],\n",
       "           ...,\n",
       "           [ 24,  23,  50],\n",
       "           [ 26,  23,  53],\n",
       "           [ 25,  20,  47]]],\n",
       "  \n",
       "  \n",
       "         [[[ 25,  40,  12],\n",
       "           [ 15,  36,   3],\n",
       "           [ 23,  41,  18],\n",
       "           ...,\n",
       "           [ 61,  82,  78],\n",
       "           [ 92, 113, 112],\n",
       "           [ 75,  89,  92]],\n",
       "  \n",
       "          [[ 12,  25,   6],\n",
       "           [ 20,  37,   7],\n",
       "           [ 24,  36,  15],\n",
       "           ...,\n",
       "           [115, 134, 138],\n",
       "           [149, 168, 177],\n",
       "           [104, 117, 131]],\n",
       "  \n",
       "          [[ 12,  25,  11],\n",
       "           [ 15,  29,   6],\n",
       "           [ 34,  40,  24],\n",
       "           ...,\n",
       "           [154, 172, 182],\n",
       "           [157, 175, 192],\n",
       "           [116, 129, 151]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[100, 129,  81],\n",
       "           [103, 132,  84],\n",
       "           [104, 134,  86],\n",
       "           ...,\n",
       "           [ 97, 128,  84],\n",
       "           [ 98, 126,  84],\n",
       "           [ 91, 121,  79]],\n",
       "  \n",
       "          [[103, 132,  83],\n",
       "           [104, 131,  83],\n",
       "           [107, 135,  87],\n",
       "           ...,\n",
       "           [101, 132,  87],\n",
       "           [ 99, 127,  84],\n",
       "           [ 92, 121,  79]],\n",
       "  \n",
       "          [[ 95, 126,  78],\n",
       "           [ 95, 123,  76],\n",
       "           [101, 128,  81],\n",
       "           ...,\n",
       "           [ 93, 124,  80],\n",
       "           [ 95, 123,  81],\n",
       "           [ 92, 120,  80]]],\n",
       "  \n",
       "  \n",
       "         [[[ 73,  78,  75],\n",
       "           [ 98, 103, 113],\n",
       "           [ 99, 106, 114],\n",
       "           ...,\n",
       "           [135, 150, 152],\n",
       "           [135, 149, 154],\n",
       "           [203, 215, 223]],\n",
       "  \n",
       "          [[ 69,  73,  70],\n",
       "           [ 84,  89,  97],\n",
       "           [ 68,  75,  81],\n",
       "           ...,\n",
       "           [ 85,  95,  89],\n",
       "           [ 71,  82,  80],\n",
       "           [120, 133, 135]],\n",
       "  \n",
       "          [[ 69,  73,  70],\n",
       "           [ 90,  95, 100],\n",
       "           [ 62,  71,  74],\n",
       "           ...,\n",
       "           [ 74,  81,  70],\n",
       "           [ 53,  62,  54],\n",
       "           [ 62,  74,  69]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[123, 128,  96],\n",
       "           [132, 132, 102],\n",
       "           [129, 128, 100],\n",
       "           ...,\n",
       "           [108, 107,  88],\n",
       "           [ 62,  60,  55],\n",
       "           [ 27,  27,  28]],\n",
       "  \n",
       "          [[115, 121,  91],\n",
       "           [123, 124,  95],\n",
       "           [129, 126,  99],\n",
       "           ...,\n",
       "           [115, 116,  94],\n",
       "           [ 66,  65,  59],\n",
       "           [ 27,  27,  27]],\n",
       "  \n",
       "          [[116, 120,  90],\n",
       "           [121, 122,  94],\n",
       "           [129, 128, 101],\n",
       "           ...,\n",
       "           [116, 115,  94],\n",
       "           [ 68,  65,  58],\n",
       "           [ 27,  26,  26]]]], dtype=uint8),\n",
       "  array([[3],\n",
       "         [8],\n",
       "         [8],\n",
       "         ...,\n",
       "         [5],\n",
       "         [1],\n",
       "         [7]])))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "cifar10.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining new Labeling:\n",
      "{0: 'frog'}\n",
      "Training Data:\n",
      "\n",
      "x_train shape: (5000, 32, 32, 3)\n",
      "5000 samples, 5000 labels\n",
      "\n",
      "Class  |  Counts:\n",
      "frog \t 5000\n",
      "\n",
      "\n",
      "Testing Data:\n",
      "\n",
      "x_test shape: (1000, 32, 32, 3)\n",
      "1000 samples, 1000 labels\n",
      "\n",
      "Class  |  Counts:\n",
      "frog \t 1000\n"
     ]
    }
   ],
   "source": [
    "my_labels = [6]\n",
    "all_label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "label_names = []\n",
    "for label_index in my_labels:\n",
    "    label_names.append(all_label_names[label_index])  \n",
    "\n",
    "print(\"Defining new Labeling:\")\n",
    "print(dict(zip(range(len(my_labels)),label_names)))\n",
    "\n",
    "#if my_labels = [5,6,8] then 5 returns 0, 6 returns 1, 8 returns 2, ...\n",
    "def convert_label(label):\n",
    "    return dict(zip(my_labels,range(len(my_labels))))[label]\n",
    "\n",
    "def label_name(num):\n",
    "    return label_names[num]\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train_all, y_train_all), (x_test_all, y_test_all) = cifar10.load_data()\n",
    "    \n",
    "#temp lists\n",
    "x_train = []\n",
    "y_train_numerical = []\n",
    "\n",
    "#filter training data for my_labels\n",
    "for i in range(len(x_train_all)):\n",
    "    if y_train_all[i] in my_labels:\n",
    "        x_train.append(x_train_all[i])\n",
    "        y_train_numerical.append(convert_label(y_train_all[i][0]))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train_numerical = np.array(y_train_numerical)\n",
    "\n",
    "print(\"Training Data:\\n\")\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'samples,', y_train_numerical.shape[0], 'labels')\n",
    "print(\"\\nClass  |  Counts:\")\n",
    "(unique, counts) = np.unique(y_train_numerical, return_counts=True)\n",
    "for i, label in enumerate(unique):\n",
    "    print(label_name(label),\"\\t\", counts[i])\n",
    "\n",
    "\n",
    "x_test = []\n",
    "y_test_numerical = []\n",
    "\n",
    "#filter test data\n",
    "for i in range(len(x_test_all)):\n",
    "    if y_test_all[i] in my_labels:\n",
    "        x_test.append(x_test_all[i])\n",
    "        y_test_numerical.append(convert_label(y_test_all[i][0]))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test_numerical = np.array(y_test_numerical)\n",
    "\n",
    "print(\"\\n\\nTesting Data:\\n\")\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_test.shape[0], 'samples,', y_test_numerical.shape[0], 'labels')\n",
    "print(\"\\nClass  |  Counts:\")\n",
    "(unique, counts) = np.unique(y_test_numerical, return_counts=True)\n",
    "for i, label in enumerate(unique):\n",
    "    print(label_name(label),\"\\t\", counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# compare https://github.com/davidADSP/GDL_code/blob/master/03_03_vae_digits_train.ipynb\n",
    "#\n",
    "import os\n",
    "\n",
    "x_train, y_train, x_test, y_test = getDigits(show=False)\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "x_train=x_train[:100]\n",
    "x_test=x_test[:100]\n",
    "y_train=y_train[:100]\n",
    "y_test=y_test[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adoerr\\Desktop\\Machine Learning\\Aufgabe 2\n",
      "shape_bef_flat (8, 8, 64)\n",
      "shape_aft_flat Tensor(\"flatten_11/Reshape:0\", shape=(None, None), dtype=float32)\n",
      "encoder_output:  Tensor(\"encoder_output_10/add:0\", shape=(None, 3), dtype=float32)\n",
      "dec_input Tensor(\"decoder_input_10:0\", shape=(None, 3), dtype=float32)\n",
      "dec_inputtii Tensor(\"decoder_input_10:0\", shape=(None, 3), dtype=float32)\n",
      "Model: \"model_42\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)         (None, 32, 32, 32)   896         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, 32, 32, 32)   0           encoder_conv_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)         (None, 16, 16, 64)   18496       leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, 16, 16, 64)   0           encoder_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)         (None, 8, 8, 64)     36928       leaky_re_lu_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)      (None, 8, 8, 64)     0           encoder_conv_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)         (None, 8, 8, 64)     36928       leaky_re_lu_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)      (None, 8, 8, 64)     0           encoder_conv_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 4096)         0           leaky_re_lu_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mu (Dense)                      (None, 3)            12291       flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "log_var (Dense)                 (None, 3)            12291       flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 3)            0           mu[0][0]                         \n",
      "                                                                 log_var[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 117,830\n",
      "Trainable params: 117,830\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_t_0 (Conv2DTran (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_t_1 (Conv2DTran (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_t_2 (Conv2DTran (None, 32, 32, 32)        18464     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_t_3 (Conv2DTran (None, 32, 32, 1)         289       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32, 32, 1)         0         \n",
      "=================================================================\n",
      "Total params: 108,993\n",
      "Trainable params: 108,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "5000/5000 [==============================] - 33s 7ms/step - loss: 35.2195 - vae_r_loss: 33.8059 - vae_kl_loss: 1.4136\n",
      "\n",
      "Epoch 00001: saving model to run\\weights/weights-001-35.22.h5\n",
      "\n",
      "Epoch 00001: saving model to run\\weights/weights.h5\n",
      "Epoch 2/200\n",
      "5000/5000 [==============================] - 32s 6ms/step - loss: 0.0757 - vae_r_loss: 0.0549 - vae_kl_loss: 0.0208\n",
      "\n",
      "Epoch 00002: saving model to run\\weights/weights-002-0.08.h5\n",
      "\n",
      "Epoch 00002: saving model to run\\weights/weights.h5\n",
      "Epoch 3/200\n",
      "5000/5000 [==============================] - 32s 6ms/step - loss: 0.0152 - vae_r_loss: 0.0134 - vae_kl_loss: 0.0018\n",
      "\n",
      "Epoch 00003: saving model to run\\weights/weights-003-0.02.h5\n",
      "\n",
      "Epoch 00003: saving model to run\\weights/weights.h5\n",
      "Epoch 4/200\n",
      "5000/5000 [==============================] - 30s 6ms/step - loss: 0.0080 - vae_r_loss: 0.0077 - vae_kl_loss: 2.8805e-04A: 25s - loss: 0.0098 -\n",
      "\n",
      "Epoch 00004: saving model to run\\weights/weights-004-0.01.h5\n",
      "\n",
      "Epoch 00004: saving model to run\\weights/weights.h5\n",
      "Epoch 5/200\n",
      "5000/5000 [==============================] - 32s 6ms/step - loss: 0.0058 - vae_r_loss: 0.0057 - vae_kl_loss: 3.2110e-05\n",
      "\n",
      "Epoch 00005: saving model to run\\weights/weights-005-0.01.h5\n",
      "\n",
      "Epoch 00005: saving model to run\\weights/weights.h5\n",
      "Epoch 6/200\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 0.0049 - vae_r_loss: 0.0049 - vae_kl_loss: 2.6690e-06 7s - loss: 0.0049 - vae_r_loss: 0.00 - ETA: 5s -\n",
      "\n",
      "Epoch 00006: saving model to run\\weights/weights-006-0.00.h5\n",
      "\n",
      "Epoch 00006: saving model to run\\weights/weights.h5\n",
      "Epoch 7/200\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 0.0044 - vae_r_loss: 0.0044 - vae_kl_loss: 2.9079e-07A: 14s - los\n",
      "\n",
      "Epoch 00007: saving model to run\\weights/weights-007-0.00.h5\n",
      "\n",
      "Epoch 00007: saving model to run\\weights/weights.h5\n",
      "Epoch 8/200\n",
      "5000/5000 [==============================] - 29s 6ms/step - loss: 0.0041 - vae_r_loss: 0.0041 - vae_kl_loss: 8.5568e-08\n",
      "\n",
      "Epoch 00008: saving model to run\\weights/weights-008-0.00.h5\n",
      "\n",
      "Epoch 00008: saving model to run\\weights/weights.h5\n",
      "Epoch 9/200\n",
      "5000/5000 [==============================] - 39s 8ms/step - loss: 0.0039 - vae_r_loss: 0.0039 - vae_kl_loss: 5.6499e-08\n",
      "\n",
      "Epoch 00009: saving model to run\\weights/weights-009-0.00.h5\n",
      "\n",
      "Epoch 00009: saving model to run\\weights/weights.h5\n",
      "Epoch 10/200\n",
      "5000/5000 [==============================] - 34s 7ms/step - loss: 0.0038 - vae_r_loss: 0.0038 - vae_kl_loss: 4.6837e-08\n",
      "\n",
      "Epoch 00010: saving model to run\\weights/weights-010-0.00.h5\n",
      "\n",
      "Epoch 00010: saving model to run\\weights/weights.h5\n",
      "Epoch 11/200\n",
      "5000/5000 [==============================] - 25s 5ms/step - loss: 0.0037 - vae_r_loss: 0.0037 - vae_kl_loss: 4.2200e-08 1s - loss: 0.0037 - vae_r_loss: 0.0037 - vae\n",
      "\n",
      "Epoch 00011: saving model to run\\weights/weights-011-0.00.h5\n",
      "\n",
      "Epoch 00011: saving model to run\\weights/weights.h5\n",
      "Epoch 12/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 27s 5ms/step - loss: 0.0037 - vae_r_loss: 0.0037 - vae_kl_loss: 3.8683e-08\n",
      "\n",
      "Epoch 00012: saving model to run\\weights/weights-012-0.00.h5\n",
      "\n",
      "Epoch 00012: saving model to run\\weights/weights.h5\n",
      "Epoch 13/200\n",
      "5000/5000 [==============================] - 28s 6ms/step - loss: 0.0036 - vae_r_loss: 0.0036 - vae_kl_loss: 3.7825e-08\n",
      "\n",
      "Epoch 00013: saving model to run\\weights/weights-013-0.00.h5\n",
      "\n",
      "Epoch 00013: saving model to run\\weights/weights.h5\n",
      "Epoch 14/200\n",
      "5000/5000 [==============================] - 25s 5ms/step - loss: 0.0036 - vae_r_loss: 0.0036 - vae_kl_loss: 3.6466e-08 0s - loss: 0.0036 - vae_r_loss: 0.0036 - vae_kl_loss: 3.65\n",
      "\n",
      "Epoch 00014: saving model to run\\weights/weights-014-0.00.h5\n",
      "\n",
      "Epoch 00014: saving model to run\\weights/weights.h5\n",
      "Epoch 15/200\n",
      "5000/5000 [==============================] - 42s 8ms/step - loss: 0.0036 - vae_r_loss: 0.0036 - vae_kl_loss: 3.6436e-08\n",
      "\n",
      "Epoch 00015: saving model to run\\weights/weights-015-0.00.h5\n",
      "\n",
      "Epoch 00015: saving model to run\\weights/weights.h5\n",
      "Epoch 16/200\n",
      "5000/5000 [==============================] - 41s 8ms/step - loss: 0.0035 - vae_r_loss: 0.0035 - vae_kl_loss: 3.5751e-08\n",
      "\n",
      "Epoch 00016: saving model to run\\weights/weights-016-0.00.h5\n",
      "\n",
      "Epoch 00016: saving model to run\\weights/weights.h5\n",
      "Epoch 17/200\n",
      "1792/5000 [=========>....................] - ETA: 29s - loss: 0.0035 - vae_r_loss: 0.0035 - vae_kl_loss: 3.7136e-08"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5445dd2ca803>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;33m,\u001b[0m \u001b[0mrun_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRUN_FOLDER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;33m,\u001b[0m \u001b[0mprint_every_n_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPRINT_EVERY_N_BATCHES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mINITIAL_EPOCH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-277bce510899>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, batch_size, epochs, run_folder, print_every_n_batches, initial_epoch, lr_decay)\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         )\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MachineLearningVLP37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MachineLearningVLP37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MachineLearningVLP37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MachineLearningVLP37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MachineLearningVLP37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MachineLearningVLP37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MachineLearningVLP37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\MachineLearningVLP37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train=x_train.reshape(x_train.shape[0],32,32,3)\n",
    "x_test=x_test.reshape(x_test.shape[0],32,32,3)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "\n",
    "SECTION = 'vae'\n",
    "RUN_ID = '0002'\n",
    "DATA_NAME = 'digits'\n",
    "RUN_FOLDER = 'run'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #\n",
    "\n",
    "\n",
    "\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [32,64,64, 64]\n",
    "    , encoder_conv_kernel_size = [3,3,3,3]\n",
    "    , encoder_conv_strides = [1,2,2,1]\n",
    "    , decoder_conv_t_filters = [64,64,32,1]\n",
    "    , decoder_conv_t_kernel_size = [3,3,3,3]\n",
    "    , decoder_conv_t_strides = [1,2,2,1]\n",
    "    , z_dim = 3\n",
    ")\n",
    "vae.save(RUN_FOLDER)\n",
    "\n",
    "vae.encoder.summary()\n",
    "vae.decoder.summary()\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "R_LOSS_FACTOR = 1000\n",
    "vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "vae.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape_bef_flat (8, 8, 64)\n",
      "shape_aft_flat Tensor(\"flatten_10/Reshape:0\", shape=(None, None), dtype=float32)\n",
      "encoder_output:  Tensor(\"encoder_output_9/add:0\", shape=(None, 3), dtype=float32)\n",
      "dec_input Tensor(\"decoder_input_9:0\", shape=(None, 3), dtype=float32)\n",
      "dec_inputtii Tensor(\"decoder_input_9:0\", shape=(None, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [32,64,64, 64]\n",
    "    , encoder_conv_kernel_size = [3,3,3,3]\n",
    "    , encoder_conv_strides = [1,2,2,1]\n",
    "    , decoder_conv_t_filters = [64,64,32,1]\n",
    "    , decoder_conv_t_kernel_size = [3,3,3,3]\n",
    "    , decoder_conv_t_strides = [1,2,2,1]\n",
    "    , z_dim = 2\n",
    ")\n",
    "\n",
    "#vae.load_weights(\"C:\\\\Users\\\\adoerr\\\\Desktop\\\\Machine Learning\\\\Aufgabe 2\\\\run\\\\weights\\\\weights-033-34.17.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct label = frog\n",
      "Prediction: [[0.4922768 0.9658645 1.9623417]]\n",
      "Prediction: [[[[0.5001168 ]\n",
      "   [0.50038844]\n",
      "   [0.49992537]\n",
      "   ...\n",
      "   [0.4998758 ]\n",
      "   [0.49952337]\n",
      "   [0.500005  ]]\n",
      "\n",
      "  [[0.49995875]\n",
      "   [0.49975753]\n",
      "   [0.50003266]\n",
      "   ...\n",
      "   [0.50027245]\n",
      "   [0.5001225 ]\n",
      "   [0.50004476]]\n",
      "\n",
      "  [[0.49998128]\n",
      "   [0.5000301 ]\n",
      "   [0.49993324]\n",
      "   ...\n",
      "   [0.500278  ]\n",
      "   [0.49978343]\n",
      "   [0.49974477]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.49930277]\n",
      "   [0.49986932]\n",
      "   [0.50042546]\n",
      "   ...\n",
      "   [0.501049  ]\n",
      "   [0.5004857 ]\n",
      "   [0.5007073 ]]\n",
      "\n",
      "  [[0.4994687 ]\n",
      "   [0.49975967]\n",
      "   [0.5000619 ]\n",
      "   ...\n",
      "   [0.49873197]\n",
      "   [0.50022465]\n",
      "   [0.4997117 ]]\n",
      "\n",
      "  [[0.5000465 ]\n",
      "   [0.4994598 ]\n",
      "   [0.499811  ]\n",
      "   ...\n",
      "   [0.50058126]\n",
      "   [0.50017315]\n",
      "   [0.49980602]]]]\n",
      "<bound method Model.predict of <keras.engine.training.Model object at 0x00000205372D04C8>>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testindex = 50\n",
    "img = x_test[testindex]\n",
    "print(\"Correct label =\",label_name(y_test_numerical[testindex]))\n",
    "plt.imshow(img)\n",
    "img = img.reshape((1,) + img.shape)\n",
    "print(\"Prediction:\",vae.encoder.predict(img))\n",
    "print(\"Prediction:\",vae.decoder.predict(vae.encoder.predict(img)))\n",
    "#print(vae.decoder.predict(vae.encoder.predict(img)).type\n",
    "print(vae.encoder.predict)\n",
    "#plt.imshow(vae.decoder.predict(vae.encoder.predict(img)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "testindex = 50\n",
    "img = x_test[testindex]\n",
    "plt.imshow(img)\n",
    "print(label_name(y_test_numerical[testindex]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
