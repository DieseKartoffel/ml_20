{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLOSS Factor:\n",
    "\n",
    "- R_LOSS = Input Image vs Output Image\n",
    "- KL_LOSS = Distribution concluded from Input Image vs. Standard Distribution\n",
    "\n",
    "``\n",
    "LOSS = R_LOSS_FACTOR * R_LOSS + KL_LOSS\n",
    "``\n",
    "\n",
    "Hence, \n",
    "- Low R_LOSS_FACTOR -> high KL_LOSS weighing:\n",
    " - output images try to be similar to all training images (blurry)\n",
    "\n",
    "\n",
    "- High R_LOSS_FACTOR -> low KL_LOSS weighing:\n",
    " - output images are similar to input images and have less variation leeway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#must be very first statement\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#utils'\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.datasets import mnist\n",
    "import PIL\n",
    "\n",
    "\n",
    "def getDigits(show=False):\n",
    "    (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "    #print(x_train.shape)\n",
    "\n",
    "    if show:\n",
    "        showImages(x_train, t_train,5)\n",
    "\n",
    "    return x_train,t_train,x_test, t_test\n",
    "\n",
    "\n",
    "#### CALLBACKS (https://github.com/davidADSP/GDL_code/blob/master/utils/callbacks.py)\n",
    "class CustomCallback(Callback):\n",
    "\n",
    "    def __init__(self, run_folder, print_every_n_batches, initial_epoch, vae):\n",
    "        self.epoch = initial_epoch\n",
    "        self.run_folder = run_folder\n",
    "        self.print_every_n_batches = print_every_n_batches\n",
    "        self.vae = vae\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if batch % self.print_every_n_batches == 0:\n",
    "            z_new = np.random.normal(size=(1, self.vae.z_dim))\n",
    "            reconst = self.vae.decoder.predict(np.array(z_new))[0].squeeze()\n",
    "\n",
    "            filepath = os.path.join(self.run_folder, 'images',\n",
    "                                    'img_' + str(self.epoch).zfill(3) + '_' + str(batch) + '.jpg')\n",
    "            if len(reconst.shape) == 2:\n",
    "                plt.imsave(filepath, reconst, cmap='gray_r')\n",
    "            else:\n",
    "                plt.imsave(filepath, reconst)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch += 1\n",
    "\n",
    "#### CALLBACKS (https://github.com/davidADSP/GDL_code/blob/master/utils/callbacks.py)\n",
    "def step_decay_schedule(initial_lr, decay_factor=0.5, step_size=1):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "\n",
    "    def schedule(epoch):\n",
    "        new_lr = initial_lr * (decay_factor ** np.floor(epoch / step_size))\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "    return LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining new Labeling:\n",
      "{0: 'automobile'}\n",
      "Training Data:\n",
      "\n",
      "x_train shape: (5000, 32, 32, 3)\n",
      "5000 samples, 5000 labels\n",
      "\n",
      "Class  |  Counts:\n",
      "automobile \t 5000\n",
      "\n",
      "\n",
      "Testing Data:\n",
      "\n",
      "x_test shape: (1000, 32, 32, 3)\n",
      "1000 samples, 1000 labels\n",
      "\n",
      "Class  |  Counts:\n",
      "automobile \t 1000\n"
     ]
    }
   ],
   "source": [
    "#data prep\n",
    "\n",
    "# changes:\n",
    "#\n",
    "# - keras load cifar 10 inst. getDigits (MNIST) from utils\n",
    "#\n",
    "# - allow selection of training subset based on label\n",
    "# - for softmax classification new label order: 2,3,8 -> 0,1,2\n",
    "#\n",
    "# - reshape:\n",
    "#   x_train=x_train.reshape(x_train.shape[0],32,32,3)\n",
    "#   x_test=x_test.reshape(x_test.shape[0],32,32,3)\n",
    "#   (instead of 28,28,1)\n",
    "#\n",
    "# Aktivierung:\n",
    "# My data was not best approximated by a Bernoulli but a Gaussian. \n",
    "# So, using a GaussianReconstructionDistribution with a TANH gave better results.\n",
    "# The DL4J JavaDocs state: \n",
    "# \"For activation functions, identity and perhaps tanh are typical - though tanh (unlike identity) \n",
    "# implies a minimum/maximum possible value for mean and log variance. Asymmetric activation functions \n",
    "# such as sigmoid or relu should be avoided\". \n",
    "\n",
    "from keras.datasets import cifar10\n",
    "cifar10.load_data()\n",
    "\n",
    "my_labels = [1]\n",
    "all_label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "label_names = []\n",
    "for label_index in my_labels:\n",
    "    label_names.append(all_label_names[label_index])  \n",
    "\n",
    "print(\"Defining new Labeling:\")\n",
    "print(dict(zip(range(len(my_labels)),label_names)))\n",
    "\n",
    "#if my_labels = [5,6,8] then 5 returns 0, 6 returns 1, 8 returns 2, ...\n",
    "def convert_label(label):\n",
    "    return dict(zip(my_labels,range(len(my_labels))))[label]\n",
    "\n",
    "def label_name(num):\n",
    "    return label_names[num]\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train_all, y_train_all), (x_test_all, y_test_all) = cifar10.load_data()\n",
    "    \n",
    "#temp lists\n",
    "x_train = []\n",
    "y_train_numerical = []\n",
    "\n",
    "#filter training data for my_labels\n",
    "for i in range(len(x_train_all)):\n",
    "    if y_train_all[i] in my_labels:\n",
    "        x_train.append(x_train_all[i])\n",
    "        y_train_numerical.append(convert_label(y_train_all[i][0]))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train_numerical = np.array(y_train_numerical)\n",
    "\n",
    "print(\"Training Data:\\n\")\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'samples,', y_train_numerical.shape[0], 'labels')\n",
    "print(\"\\nClass  |  Counts:\")\n",
    "(unique, counts) = np.unique(y_train_numerical, return_counts=True)\n",
    "for i, label in enumerate(unique):\n",
    "    print(label_name(label),\"\\t\", counts[i])\n",
    "\n",
    "\n",
    "x_test = []\n",
    "y_test_numerical = []\n",
    "\n",
    "#filter test data\n",
    "for i in range(len(x_test_all)):\n",
    "    if y_test_all[i] in my_labels:\n",
    "        x_test.append(x_test_all[i])\n",
    "        y_test_numerical.append(convert_label(y_test_all[i][0]))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test_numerical = np.array(y_test_numerical)\n",
    "\n",
    "print(\"\\n\\nTesting Data:\\n\")\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_test.shape[0], 'samples,', y_test_numerical.shape[0], 'labels')\n",
    "print(\"\\nClass  |  Counts:\")\n",
    "(unique, counts) = np.unique(y_test_numerical, return_counts=True)\n",
    "for i, label in enumerate(unique):\n",
    "    print(label_name(label),\"\\t\", counts[i])\n",
    "    \n",
    "x_train=x_train.reshape(x_train.shape[0],32,32,3)\n",
    "x_test=x_test.reshape(x_test.shape[0],32,32,3)\n",
    "\n",
    "x_train = x_train.astype('float32')[:500]\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model class\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras import callbacks\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Verteilung + KL:\n",
    "# https://stats.stackexchange.com/questions/402569/why-do-we-use-gaussian-distributions-in-variational-autoencoder\n",
    "#\n",
    "# TODO:\n",
    "# sigmoid vs tanh in decoder activation\n",
    "# batchnormalization: ON\n",
    "# (multivariate distribution instead of gaussian normal)\n",
    "\n",
    "class VariationalAutoencoder():\n",
    "    def __init__(self\n",
    "                 , input_dim\n",
    "                 , encoder_conv_filters\n",
    "                 , encoder_conv_kernel_size\n",
    "                 , encoder_conv_strides\n",
    "                 , decoder_conv_t_filters\n",
    "                 , decoder_conv_t_kernel_size\n",
    "                 , decoder_conv_t_strides\n",
    "                 , z_dim\n",
    "                 , use_batch_norm=False\n",
    "                 , use_dropout=False\n",
    "                 ):\n",
    "\n",
    "        self.name = 'variational_autoencoder'\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_conv_filters = encoder_conv_filters\n",
    "        self.encoder_conv_kernel_size = encoder_conv_kernel_size\n",
    "        self.encoder_conv_strides = encoder_conv_strides\n",
    "        self.decoder_conv_t_filters = decoder_conv_t_filters\n",
    "        self.decoder_conv_t_kernel_size = decoder_conv_t_kernel_size\n",
    "        self.decoder_conv_t_strides = decoder_conv_t_strides\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        self.n_layers_encoder = len(encoder_conv_filters)\n",
    "        self.n_layers_decoder = len(decoder_conv_t_filters)\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        ### THE ENCODER\n",
    "        encoder_input = Input(shape=self.input_dim, name='encoder_input')\n",
    "\n",
    "        x = encoder_input\n",
    "\n",
    "        for i in range(self.n_layers_encoder):\n",
    "            conv_layer = Conv2D(\n",
    "                filters=self.encoder_conv_filters[i]\n",
    "                , kernel_size=self.encoder_conv_kernel_size[i]\n",
    "                , strides=self.encoder_conv_strides[i]\n",
    "                , padding='same'\n",
    "                , name='encoder_conv_' + str(i)\n",
    "            )\n",
    "\n",
    "            x = conv_layer(x)\n",
    "\n",
    "            if self.use_batch_norm:\n",
    "                x = BatchNormalization()(x)\n",
    "\n",
    "            x = LeakyReLU()(x) # for encoding\n",
    "\n",
    "            if self.use_dropout:\n",
    "                x = Dropout(rate=0.25)(x)\n",
    "\n",
    "        shape_before_flattening = K.int_shape(x)[1:]\n",
    "#-----------------------------\n",
    "        print(\"shape_bef_flat\",shape_before_flattening)\n",
    "        x = Flatten()(x)\n",
    "        print(\"shape_aft_flat\",x)\n",
    "        self.mu = Dense(self.z_dim, name='mu')(x)\n",
    "        self.log_var = Dense(self.z_dim, name='log_var')(x)\n",
    "\n",
    "        self.encoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\n",
    "\n",
    "        def sampling(args):\n",
    "            mu, log_var = args\n",
    "            epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1) #gaussian standardnormalvert.\n",
    "            return mu + K.exp(log_var / 2) * epsilon #try without /2\n",
    "    \n",
    "        encoder_output = Lambda(sampling, name='encoder_output')([self.mu, self.log_var])\n",
    "        print(\"encoder_output: \",encoder_output)\n",
    "        self.encoder = Model(encoder_input, encoder_output)\n",
    "\n",
    "        ### THE DECODER\n",
    "\n",
    "        decoder_input = Input(shape=(self.z_dim,), name='decoder_input')\n",
    "        print(\"dec_input\",decoder_input)\n",
    "        x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "        x = Reshape(shape_before_flattening)(x)\n",
    "\n",
    "        for i in range(self.n_layers_decoder):\n",
    "            conv_t_layer = Conv2DTranspose(\n",
    "                filters=self.decoder_conv_t_filters[i]\n",
    "                , kernel_size=self.decoder_conv_t_kernel_size[i]\n",
    "                , strides=self.decoder_conv_t_strides[i]\n",
    "                , padding='same'\n",
    "                , name='decoder_conv_t_' + str(i)\n",
    "            )\n",
    "\n",
    "            x = conv_t_layer(x)\n",
    "\n",
    "            if i < self.n_layers_decoder - 1:\n",
    "                if self.use_batch_norm:\n",
    "                    x = BatchNormalization()(x)\n",
    "                x = LeakyReLU()(x)\n",
    "                if self.use_dropout:\n",
    "                    x = Dropout(rate=0.25)(x)\n",
    "            else:\n",
    "                x = Activation('sigmoid')(x)\n",
    "\n",
    "        decoder_output = x\n",
    "        self.decoder = Model(decoder_input, decoder_output)\n",
    "\n",
    "        ### THE FULL VAE\n",
    "        model_input = encoder_input\n",
    "        model_output = self.decoder(encoder_output)\n",
    "        \n",
    "        self.model = Model(model_input, model_output)\n",
    "\n",
    "    def compile(self, learning_rate, r_loss_factor):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        ### COMPILATION\n",
    "        def vae_r_loss(y_true, y_pred):\n",
    "            r_loss = K.mean(K.square(y_true - y_pred), axis=[1, 2, 3])\n",
    "            return r_loss * r_loss_factor\n",
    "\n",
    "        def vae_kl_loss(y_true, y_pred):\n",
    "            kl_loss = -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis=1)\n",
    "            return kl_loss\n",
    "\n",
    "        def vae_loss(y_true, y_pred):\n",
    "            r_loss = vae_r_loss(y_true, y_pred)\n",
    "            kl_loss = vae_kl_loss(y_true, y_pred)\n",
    "            return r_loss + kl_loss\n",
    "\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss=vae_loss, metrics=[vae_r_loss, vae_kl_loss])\n",
    "\n",
    "    def save(self, folder=\"run\"):\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            os.makedirs(os.path.join(folder, 'viz'))\n",
    "            os.makedirs(os.path.join(folder, 'weights'))\n",
    "            os.makedirs(os.path.join(folder, 'images'))\n",
    "\n",
    "        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n",
    "            pickle.dump([\n",
    "                self.input_dim\n",
    "                , self.encoder_conv_filters\n",
    "                , self.encoder_conv_kernel_size\n",
    "                , self.encoder_conv_strides\n",
    "                , self.decoder_conv_t_filters\n",
    "                , self.decoder_conv_t_kernel_size\n",
    "                , self.decoder_conv_t_strides\n",
    "                , self.z_dim\n",
    "                , self.use_batch_norm\n",
    "                , self.use_dropout\n",
    "            ], f)\n",
    "\n",
    "        self.plot_model(folder)\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "\n",
    "    def train(self, x_train, batch_size, epochs, run_folder, print_every_n_batches=100, initial_epoch=0, lr_decay=1):\n",
    "\n",
    "        custom_callback = CustomCallback(run_folder, print_every_n_batches, initial_epoch, self)\n",
    "        lr_sched = step_decay_schedule(initial_lr=self.learning_rate, decay_factor=lr_decay, step_size=1)\n",
    "\n",
    "        checkpoint_filepath = os.path.join(run_folder, \"weights/weights.h5\")\n",
    "        \n",
    "        model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_acc',\n",
    "            mode='max')\n",
    "\n",
    "        \n",
    "        #checkpoint1 = ModelCheckpoint(checkpoint_filepath, save_weights_only=True, verbose=1)\n",
    "        #checkpoint2 = ModelCheckpoint(os.path.join(run_folder, 'weights/weights.h5'), save_weights_only=True, verbose=1)\n",
    "\n",
    "        callbacks_list = [custom_callback, lr_sched, model_checkpoint_callback]\n",
    "\n",
    "        self.model.fit(\n",
    "            x_train\n",
    "            , x_train\n",
    "            , batch_size=batch_size\n",
    "            , shuffle=True\n",
    "            , epochs=epochs\n",
    "            , initial_epoch=initial_epoch\n",
    "            , callbacks=callbacks_list\n",
    "        )\n",
    "\n",
    "    def plot_model(self, run_folder):\n",
    "        plot_model(self.model, to_file=os.path.join(run_folder, 'viz/model.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "        plot_model(self.encoder, to_file=os.path.join(run_folder, 'viz/encoder.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "        plot_model(self.decoder, to_file=os.path.join(run_folder, 'viz/decoder.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:\\Hub\\ML_SS_2020\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "SECTION = 'vae'\n",
    "RUN_ID = '0002'\n",
    "DATA_NAME = 'digits'\n",
    "RUN_FOLDER = 'run_losscompare'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# größeres modell\n",
    "#Wichtig: Beim Output Filteranzahl = 3 -> Dreidimensionales Ergebnis für RGB Interpretation\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [128,128,128,512]\n",
    "    , encoder_conv_kernel_size = [2,2,3,4]\n",
    "    , encoder_conv_strides = [1,1,2,2]\n",
    "    , decoder_conv_t_filters = [512,128,128,3]\n",
    "    , decoder_conv_t_kernel_size = [4,3,2,2]\n",
    "    , decoder_conv_t_strides = [2,2,1,1]\n",
    "    , z_dim = 128\n",
    ")\n",
    "vae.save(RUN_FOLDER)\n",
    "\n",
    "vae.encoder.summary()\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "R_LOSS_FACTOR = 100\n",
    "vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "vae.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")\n",
    "vae.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "shape_bef_flat (8, 8, 512)\n",
      "shape_aft_flat Tensor(\"flatten_4/Reshape:0\", shape=(None, None), dtype=float32)\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "encoder_output:  Tensor(\"encoder_output_3/add:0\", shape=(None, 2), dtype=float32)\n",
      "dec_input Tensor(\"decoder_input_3:0\", shape=(None, 2), dtype=float32)\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)         (None, 32, 32, 128)  1664        encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 32, 32, 128)  0           encoder_conv_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)         (None, 32, 32, 128)  65664       leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 32, 32, 128)  0           encoder_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)         (None, 16, 16, 128)  147584      leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 16, 16, 128)  0           encoder_conv_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)         (None, 8, 8, 512)    1049088     leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 8, 8, 512)    0           encoder_conv_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 32768)        0           leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "mu (Dense)                      (None, 2)            65538       flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "log_var (Dense)                 (None, 2)            65538       flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 2)            0           mu[0][0]                         \n",
      "                                                                 log_var[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,395,076\n",
      "Trainable params: 1,395,076\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "Executing op __inference_keras_scratch_graph_262297 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      " 32/500 [>.............................] - ETA: 1:49 - loss: 375.0701 - vae_r_loss: 375.0679 - vae_kl_loss: 0.0022Executing op __inference_keras_scratch_graph_262434 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "160/500 [========>.....................] - ETA: 18s - loss: 401.4303 - vae_r_loss: 401.4284 - vae_kl_loss: 0.0018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\ml20\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.485000). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 9s 19ms/step - loss: 379.5561 - vae_r_loss: 379.5302 - vae_kl_loss: 0.0259\n",
      "Epoch 2/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 370.0878 - vae_r_loss: 369.8786 - vae_kl_loss: 0.2093\n",
      "Epoch 3/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 349.3964 - vae_r_loss: 348.2130 - vae_kl_loss: 1.1835\n",
      "Epoch 4/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 318.6367 - vae_r_loss: 312.4237 - vae_kl_loss: 6.2130\n",
      "Epoch 5/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 298.1541 - vae_r_loss: 293.8375 - vae_kl_loss: 4.3165\n",
      "Epoch 6/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 278.0506 - vae_r_loss: 272.5572 - vae_kl_loss: 5.4934\n",
      "Epoch 7/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 261.8338 - vae_r_loss: 256.0896 - vae_kl_loss: 5.7442\n",
      "Epoch 8/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 251.1158 - vae_r_loss: 245.6971 - vae_kl_loss: 5.4187\n",
      "Epoch 9/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 244.2576 - vae_r_loss: 238.8188 - vae_kl_loss: 5.4387\n",
      "Epoch 10/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 240.1954 - vae_r_loss: 234.8941 - vae_kl_loss: 5.3013\n",
      "Epoch 11/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 236.7214 - vae_r_loss: 231.4847 - vae_kl_loss: 5.2367\n",
      "Epoch 12/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 236.3597 - vae_r_loss: 230.9294 - vae_kl_loss: 5.4303\n",
      "Epoch 13/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 234.4474 - vae_r_loss: 229.1660 - vae_kl_loss: 5.2814\n",
      "Epoch 14/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 233.1973 - vae_r_loss: 228.0318 - vae_kl_loss: 5.1655\n",
      "Epoch 15/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 232.5065 - vae_r_loss: 227.2551 - vae_kl_loss: 5.2514\n",
      "Epoch 16/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 232.0660 - vae_r_loss: 227.0310 - vae_kl_loss: 5.0350\n",
      "Epoch 17/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 231.0351 - vae_r_loss: 225.8883 - vae_kl_loss: 5.1468\n",
      "Epoch 18/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 231.1752 - vae_r_loss: 226.0508 - vae_kl_loss: 5.1244\n",
      "Epoch 19/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 231.1873 - vae_r_loss: 225.9890 - vae_kl_loss: 5.1983\n",
      "Epoch 20/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 230.3909 - vae_r_loss: 225.4585 - vae_kl_loss: 4.9324\n",
      "Epoch 21/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 230.6979 - vae_r_loss: 225.6337 - vae_kl_loss: 5.0642\n",
      "Epoch 22/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 230.1544 - vae_r_loss: 224.9822 - vae_kl_loss: 5.1722\n",
      "Epoch 23/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 229.8461 - vae_r_loss: 224.9283 - vae_kl_loss: 4.9178\n",
      "Epoch 24/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 229.2865 - vae_r_loss: 224.2755 - vae_kl_loss: 5.0110\n",
      "Epoch 25/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 229.3097 - vae_r_loss: 224.2247 - vae_kl_loss: 5.0850\n",
      "Epoch 26/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 229.0458 - vae_r_loss: 224.0814 - vae_kl_loss: 4.9644\n",
      "Epoch 27/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 229.2869 - vae_r_loss: 224.2128 - vae_kl_loss: 5.0741\n",
      "Epoch 28/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 228.6661 - vae_r_loss: 223.6466 - vae_kl_loss: 5.0195\n",
      "Epoch 29/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 228.7243 - vae_r_loss: 223.7644 - vae_kl_loss: 4.9598\n",
      "Epoch 30/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 228.9484 - vae_r_loss: 223.8538 - vae_kl_loss: 5.0946\n",
      "Epoch 31/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 228.9530 - vae_r_loss: 223.9836 - vae_kl_loss: 4.9693\n",
      "Epoch 32/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 229.4252 - vae_r_loss: 224.5149 - vae_kl_loss: 4.9103\n",
      "Epoch 33/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 228.2719 - vae_r_loss: 223.3077 - vae_kl_loss: 4.9642\n",
      "Epoch 34/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 228.3314 - vae_r_loss: 223.2262 - vae_kl_loss: 5.1052\n",
      "Epoch 35/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 228.0727 - vae_r_loss: 223.1742 - vae_kl_loss: 4.8985\n",
      "Epoch 36/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 227.8239 - vae_r_loss: 222.7104 - vae_kl_loss: 5.1136\n",
      "Epoch 37/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 227.4781 - vae_r_loss: 222.5478 - vae_kl_loss: 4.9303\n",
      "Epoch 38/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 227.6902 - vae_r_loss: 222.5804 - vae_kl_loss: 5.1099\n",
      "Epoch 39/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 227.3623 - vae_r_loss: 222.4428 - vae_kl_loss: 4.9194\n",
      "Epoch 40/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 227.3932 - vae_r_loss: 222.4037 - vae_kl_loss: 4.9895\n",
      "Epoch 41/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.8902 - vae_r_loss: 221.8221 - vae_kl_loss: 5.0681\n",
      "Epoch 42/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.9254 - vae_r_loss: 221.9296 - vae_kl_loss: 4.9959\n",
      "Epoch 43/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.6080 - vae_r_loss: 221.6963 - vae_kl_loss: 4.9117\n",
      "Epoch 44/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.6008 - vae_r_loss: 221.5711 - vae_kl_loss: 5.0297\n",
      "Epoch 45/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.4315 - vae_r_loss: 221.3086 - vae_kl_loss: 5.1229: 0s - loss: 227.5269 - vae_r_loss: 222.4421 - vae_kl_loss\n",
      "Epoch 46/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.6931 - vae_r_loss: 221.8111 - vae_kl_loss: 4.8820\n",
      "Epoch 47/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.5186 - vae_r_loss: 221.5143 - vae_kl_loss: 5.0043\n",
      "Epoch 48/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.4760 - vae_r_loss: 221.6573 - vae_kl_loss: 4.8187\n",
      "Epoch 49/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.4827 - vae_r_loss: 221.5791 - vae_kl_loss: 4.9036\n",
      "Epoch 50/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.6721 - vae_r_loss: 221.8170 - vae_kl_loss: 4.8552\n",
      "Epoch 51/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 225.5419 - vae_r_loss: 220.3810 - vae_kl_loss: 5.1609\n",
      "Epoch 52/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.2510 - vae_r_loss: 221.4535 - vae_kl_loss: 4.7976\n",
      "Epoch 53/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 227.0821 - vae_r_loss: 222.2448 - vae_kl_loss: 4.8373\n",
      "Epoch 54/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.3909 - vae_r_loss: 221.3341 - vae_kl_loss: 5.0568\n",
      "Epoch 55/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 226.0433 - vae_r_loss: 220.9886 - vae_kl_loss: 5.0547\n",
      "Epoch 56/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 225.4337 - vae_r_loss: 220.4309 - vae_kl_loss: 5.0029\n",
      "Epoch 57/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 225.0677 - vae_r_loss: 220.1078 - vae_kl_loss: 4.9599\n",
      "Epoch 58/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 225.3437 - vae_r_loss: 220.3266 - vae_kl_loss: 5.0171\n",
      "Epoch 59/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 225.0247 - vae_r_loss: 220.1709 - vae_kl_loss: 4.8538\n",
      "Epoch 60/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.7182 - vae_r_loss: 219.6667 - vae_kl_loss: 5.0516\n",
      "Epoch 61/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.9179 - vae_r_loss: 219.8237 - vae_kl_loss: 5.0941\n",
      "Epoch 62/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.6518 - vae_r_loss: 219.8093 - vae_kl_loss: 4.8424\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 224.4668 - vae_r_loss: 219.5621 - vae_kl_loss: 4.9046\n",
      "Epoch 64/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.5367 - vae_r_loss: 219.6679 - vae_kl_loss: 4.8687\n",
      "Epoch 65/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.3741 - vae_r_loss: 219.2871 - vae_kl_loss: 5.0870\n",
      "Epoch 66/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.5665 - vae_r_loss: 219.5808 - vae_kl_loss: 4.9856\n",
      "Epoch 67/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.1684 - vae_r_loss: 219.0671 - vae_kl_loss: 5.1014\n",
      "Epoch 68/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.3047 - vae_r_loss: 219.4182 - vae_kl_loss: 4.8865\n",
      "Epoch 69/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.6350 - vae_r_loss: 219.6636 - vae_kl_loss: 4.9713\n",
      "Epoch 70/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.1327 - vae_r_loss: 219.2229 - vae_kl_loss: 4.9098\n",
      "Epoch 71/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 223.9787 - vae_r_loss: 218.9126 - vae_kl_loss: 5.0662\n",
      "Epoch 72/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.5328 - vae_r_loss: 219.3768 - vae_kl_loss: 5.1560\n",
      "Epoch 73/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 223.6809 - vae_r_loss: 218.7362 - vae_kl_loss: 4.9448\n",
      "Epoch 74/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 223.9822 - vae_r_loss: 218.9068 - vae_kl_loss: 5.0754\n",
      "Epoch 75/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.0973 - vae_r_loss: 219.1766 - vae_kl_loss: 4.9208\n",
      "Epoch 76/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 223.9966 - vae_r_loss: 218.9471 - vae_kl_loss: 5.0495\n",
      "Epoch 77/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 223.9884 - vae_r_loss: 218.8118 - vae_kl_loss: 5.1766\n",
      "Epoch 78/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.3891 - vae_r_loss: 219.4933 - vae_kl_loss: 4.8958\n",
      "Epoch 79/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 224.6249 - vae_r_loss: 219.7201 - vae_kl_loss: 4.9048\n",
      "Epoch 80/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 223.7098 - vae_r_loss: 218.6513 - vae_kl_loss: 5.0585\n",
      "Epoch 81/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 223.5119 - vae_r_loss: 218.6363 - vae_kl_loss: 4.8756\n",
      "Epoch 82/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.9245 - vae_r_loss: 217.9028 - vae_kl_loss: 5.0216\n",
      "Epoch 83/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.6396 - vae_r_loss: 217.6739 - vae_kl_loss: 4.9658\n",
      "Epoch 84/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.8912 - vae_r_loss: 217.8165 - vae_kl_loss: 5.0747\n",
      "Epoch 85/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.7361 - vae_r_loss: 217.6962 - vae_kl_loss: 5.0400\n",
      "Epoch 86/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.6450 - vae_r_loss: 217.5488 - vae_kl_loss: 5.0962\n",
      "Epoch 87/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.6942 - vae_r_loss: 217.6488 - vae_kl_loss: 5.0453\n",
      "Epoch 88/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 223.6162 - vae_r_loss: 218.5257 - vae_kl_loss: 5.0905\n",
      "Epoch 89/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.8936 - vae_r_loss: 217.9139 - vae_kl_loss: 4.9797\n",
      "Epoch 90/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.5395 - vae_r_loss: 217.3876 - vae_kl_loss: 5.1519\n",
      "Epoch 91/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.1769 - vae_r_loss: 217.3164 - vae_kl_loss: 4.8605\n",
      "Epoch 92/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.3397 - vae_r_loss: 217.2556 - vae_kl_loss: 5.0842\n",
      "Epoch 93/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 222.2929 - vae_r_loss: 217.1695 - vae_kl_loss: 5.1234\n",
      "Epoch 94/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.5332 - vae_r_loss: 216.7017 - vae_kl_loss: 4.8315\n",
      "Epoch 95/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.9190 - vae_r_loss: 216.8098 - vae_kl_loss: 5.1092\n",
      "Epoch 96/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.7047 - vae_r_loss: 216.6216 - vae_kl_loss: 5.0831\n",
      "Epoch 97/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.9727 - vae_r_loss: 216.8820 - vae_kl_loss: 5.0906\n",
      "Epoch 98/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.9536 - vae_r_loss: 216.6858 - vae_kl_loss: 5.2678\n",
      "Epoch 99/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.2205 - vae_r_loss: 216.2353 - vae_kl_loss: 4.9852\n",
      "Epoch 100/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.2896 - vae_r_loss: 216.0890 - vae_kl_loss: 5.2006\n",
      "Epoch 101/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.8349 - vae_r_loss: 215.8039 - vae_kl_loss: 5.0310\n",
      "Epoch 102/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.8142 - vae_r_loss: 215.6926 - vae_kl_loss: 5.1217\n",
      "Epoch 103/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.4108 - vae_r_loss: 216.3399 - vae_kl_loss: 5.0709\n",
      "Epoch 104/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.7415 - vae_r_loss: 216.7302 - vae_kl_loss: 5.0113\n",
      "Epoch 105/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.9812 - vae_r_loss: 216.8100 - vae_kl_loss: 5.1712\n",
      "Epoch 106/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.0677 - vae_r_loss: 216.0821 - vae_kl_loss: 4.9856\n",
      "Epoch 107/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.8206 - vae_r_loss: 215.5095 - vae_kl_loss: 5.3111\n",
      "Epoch 108/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.6212 - vae_r_loss: 216.5945 - vae_kl_loss: 5.0267\n",
      "Epoch 109/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 221.0856 - vae_r_loss: 216.0539 - vae_kl_loss: 5.0317\n",
      "Epoch 110/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.8396 - vae_r_loss: 215.7784 - vae_kl_loss: 5.0611\n",
      "Epoch 111/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.4993 - vae_r_loss: 215.2964 - vae_kl_loss: 5.2029\n",
      "Epoch 112/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.0852 - vae_r_loss: 215.0142 - vae_kl_loss: 5.0709\n",
      "Epoch 113/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.0526 - vae_r_loss: 214.8891 - vae_kl_loss: 5.1634\n",
      "Epoch 114/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.6082 - vae_r_loss: 214.5159 - vae_kl_loss: 5.0923\n",
      "Epoch 115/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.1550 - vae_r_loss: 214.9819 - vae_kl_loss: 5.1731\n",
      "Epoch 116/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.9728 - vae_r_loss: 214.9541 - vae_kl_loss: 5.0187\n",
      "Epoch 117/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.3192 - vae_r_loss: 215.2235 - vae_kl_loss: 5.0956\n",
      "Epoch 118/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.1293 - vae_r_loss: 214.9895 - vae_kl_loss: 5.1398\n",
      "Epoch 119/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.5672 - vae_r_loss: 214.4621 - vae_kl_loss: 5.1051\n",
      "Epoch 120/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.4733 - vae_r_loss: 214.3770 - vae_kl_loss: 5.0964\n",
      "Epoch 121/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.9756 - vae_r_loss: 214.8653 - vae_kl_loss: 5.1103\n",
      "Epoch 122/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.3319 - vae_r_loss: 214.0253 - vae_kl_loss: 5.3066\n",
      "Epoch 123/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.9827 - vae_r_loss: 215.7631 - vae_kl_loss: 5.2195\n",
      "Epoch 124/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 220.1869 - vae_r_loss: 214.8037 - vae_kl_loss: 5.3832\n",
      "Epoch 125/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 219.1977 - vae_r_loss: 214.0470 - vae_kl_loss: 5.1508\n",
      "Epoch 126/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.1676 - vae_r_loss: 214.0729 - vae_kl_loss: 5.0947\n",
      "Epoch 127/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.3608 - vae_r_loss: 214.0870 - vae_kl_loss: 5.2738\n",
      "Epoch 128/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.2264 - vae_r_loss: 213.9649 - vae_kl_loss: 5.2615\n",
      "Epoch 129/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.8452 - vae_r_loss: 214.6522 - vae_kl_loss: 5.1930\n",
      "Epoch 130/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.2096 - vae_r_loss: 213.8973 - vae_kl_loss: 5.3123\n",
      "Epoch 131/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 218.7887 - vae_r_loss: 213.5628 - vae_kl_loss: 5.2259\n",
      "Epoch 132/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.0831 - vae_r_loss: 213.9199 - vae_kl_loss: 5.1632\n",
      "Epoch 133/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.2626 - vae_r_loss: 214.1418 - vae_kl_loss: 5.1208\n",
      "Epoch 134/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 218.5299 - vae_r_loss: 213.1345 - vae_kl_loss: 5.3954\n",
      "Epoch 135/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.9522 - vae_r_loss: 212.6666 - vae_kl_loss: 5.2857\n",
      "Epoch 136/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.9652 - vae_r_loss: 212.5930 - vae_kl_loss: 5.3722\n",
      "Epoch 137/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 219.4708 - vae_r_loss: 214.2211 - vae_kl_loss: 5.2497\n",
      "Epoch 138/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 218.3205 - vae_r_loss: 213.0050 - vae_kl_loss: 5.3154\n",
      "Epoch 139/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.5293 - vae_r_loss: 212.3988 - vae_kl_loss: 5.1305\n",
      "Epoch 140/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.1482 - vae_r_loss: 211.6969 - vae_kl_loss: 5.4513\n",
      "Epoch 141/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.0206 - vae_r_loss: 211.8465 - vae_kl_loss: 5.1741\n",
      "Epoch 142/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.2424 - vae_r_loss: 211.7539 - vae_kl_loss: 5.4885\n",
      "Epoch 143/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.1607 - vae_r_loss: 211.9473 - vae_kl_loss: 5.2134\n",
      "Epoch 144/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 216.5057 - vae_r_loss: 211.0575 - vae_kl_loss: 5.4482\n",
      "Epoch 145/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.0506 - vae_r_loss: 211.8522 - vae_kl_loss: 5.1984: 0s - loss: 217.8805 - vae_r_loss: 212.\n",
      "Epoch 146/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.0483 - vae_r_loss: 211.5798 - vae_kl_loss: 5.4684\n",
      "Epoch 147/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 216.7911 - vae_r_loss: 211.4357 - vae_kl_loss: 5.3554\n",
      "Epoch 148/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 216.4980 - vae_r_loss: 211.1242 - vae_kl_loss: 5.3737\n",
      "Epoch 149/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 216.5443 - vae_r_loss: 211.1523 - vae_kl_loss: 5.3921\n",
      "Epoch 150/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 216.1108 - vae_r_loss: 210.7221 - vae_kl_loss: 5.3887\n",
      "Epoch 151/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 215.9887 - vae_r_loss: 210.6959 - vae_kl_loss: 5.2928\n",
      "Epoch 152/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 216.0808 - vae_r_loss: 210.4796 - vae_kl_loss: 5.6011\n",
      "Epoch 153/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 217.8195 - vae_r_loss: 212.4077 - vae_kl_loss: 5.4119\n",
      "Epoch 154/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 216.1934 - vae_r_loss: 210.7550 - vae_kl_loss: 5.4384\n",
      "Epoch 155/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 215.9851 - vae_r_loss: 210.6901 - vae_kl_loss: 5.2951\n",
      "Epoch 156/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 216.2942 - vae_r_loss: 210.8119 - vae_kl_loss: 5.4823\n",
      "Epoch 157/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 215.3523 - vae_r_loss: 209.8775 - vae_kl_loss: 5.4747\n",
      "Epoch 158/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 215.2613 - vae_r_loss: 209.8490 - vae_kl_loss: 5.4123\n",
      "Epoch 159/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 214.4738 - vae_r_loss: 208.9443 - vae_kl_loss: 5.5296\n",
      "Epoch 160/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 214.5503 - vae_r_loss: 209.0970 - vae_kl_loss: 5.4533\n",
      "Epoch 161/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 214.5285 - vae_r_loss: 209.0890 - vae_kl_loss: 5.4395\n",
      "Epoch 162/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 215.0816 - vae_r_loss: 209.2957 - vae_kl_loss: 5.7860\n",
      "Epoch 163/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 214.1351 - vae_r_loss: 208.5472 - vae_kl_loss: 5.5879\n",
      "Epoch 164/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 213.7095 - vae_r_loss: 208.1219 - vae_kl_loss: 5.5876\n",
      "Epoch 165/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 214.5136 - vae_r_loss: 209.0272 - vae_kl_loss: 5.4865\n",
      "Epoch 166/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 213.8849 - vae_r_loss: 208.3761 - vae_kl_loss: 5.5088\n",
      "Epoch 167/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 215.3383 - vae_r_loss: 209.7198 - vae_kl_loss: 5.6185\n",
      "Epoch 168/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 214.0975 - vae_r_loss: 208.6002 - vae_kl_loss: 5.4973\n",
      "Epoch 169/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 213.5971 - vae_r_loss: 208.1101 - vae_kl_loss: 5.4870\n",
      "Epoch 170/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 213.3802 - vae_r_loss: 207.9080 - vae_kl_loss: 5.4721\n",
      "Epoch 171/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 214.2112 - vae_r_loss: 208.5249 - vae_kl_loss: 5.6862\n",
      "Epoch 172/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 212.8386 - vae_r_loss: 207.0943 - vae_kl_loss: 5.7443\n",
      "Epoch 173/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 212.7118 - vae_r_loss: 207.2863 - vae_kl_loss: 5.4255\n",
      "Epoch 174/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 213.5613 - vae_r_loss: 207.8042 - vae_kl_loss: 5.7570\n",
      "Epoch 175/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 212.8826 - vae_r_loss: 207.4686 - vae_kl_loss: 5.4140\n",
      "Epoch 176/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 212.3681 - vae_r_loss: 206.5732 - vae_kl_loss: 5.7949\n",
      "Epoch 177/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 212.3447 - vae_r_loss: 206.6503 - vae_kl_loss: 5.6944\n",
      "Epoch 178/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.5851 - vae_r_loss: 205.7617 - vae_kl_loss: 5.8234\n",
      "Epoch 179/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.7595 - vae_r_loss: 206.3104 - vae_kl_loss: 5.4490\n",
      "Epoch 180/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.7443 - vae_r_loss: 206.1561 - vae_kl_loss: 5.5882\n",
      "Epoch 181/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.2133 - vae_r_loss: 205.5697 - vae_kl_loss: 5.6437\n",
      "Epoch 182/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.7803 - vae_r_loss: 206.0763 - vae_kl_loss: 5.7040\n",
      "Epoch 183/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.4283 - vae_r_loss: 205.7070 - vae_kl_loss: 5.7213\n",
      "Epoch 184/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.1376 - vae_r_loss: 205.4943 - vae_kl_loss: 5.6433\n",
      "Epoch 185/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.9826 - vae_r_loss: 206.1259 - vae_kl_loss: 5.8567\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 212.2603 - vae_r_loss: 206.3686 - vae_kl_loss: 5.8917\n",
      "Epoch 187/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.0264 - vae_r_loss: 205.4377 - vae_kl_loss: 5.5888\n",
      "Epoch 188/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.1559 - vae_r_loss: 205.4827 - vae_kl_loss: 5.6732\n",
      "Epoch 189/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.6618 - vae_r_loss: 205.9601 - vae_kl_loss: 5.7017\n",
      "Epoch 190/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.7362 - vae_r_loss: 204.8612 - vae_kl_loss: 5.8750\n",
      "Epoch 191/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.4700 - vae_r_loss: 204.6221 - vae_kl_loss: 5.8480\n",
      "Epoch 192/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.3314 - vae_r_loss: 205.6613 - vae_kl_loss: 5.6700\n",
      "Epoch 193/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.8715 - vae_r_loss: 204.9818 - vae_kl_loss: 5.8897\n",
      "Epoch 194/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.6771 - vae_r_loss: 205.0820 - vae_kl_loss: 5.5951\n",
      "Epoch 195/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 212.1796 - vae_r_loss: 206.3841 - vae_kl_loss: 5.7955\n",
      "Epoch 196/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.5266 - vae_r_loss: 204.4438 - vae_kl_loss: 6.0828\n",
      "Epoch 197/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.2621 - vae_r_loss: 204.5293 - vae_kl_loss: 5.7328\n",
      "Epoch 198/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.3696 - vae_r_loss: 204.5790 - vae_kl_loss: 5.7906\n",
      "Epoch 199/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 209.4875 - vae_r_loss: 203.6286 - vae_kl_loss: 5.8588\n",
      "Epoch 200/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 209.6996 - vae_r_loss: 203.7773 - vae_kl_loss: 5.9223\n",
      "Epoch 201/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.4741 - vae_r_loss: 204.5263 - vae_kl_loss: 5.9478: 0s - loss: 210.8977 - vae_r_loss: 204.9453 - vae_kl_loss: 5.95\n",
      "Epoch 202/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 209.7378 - vae_r_loss: 204.1062 - vae_kl_loss: 5.6317\n",
      "Epoch 203/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 209.6551 - vae_r_loss: 203.6628 - vae_kl_loss: 5.9923\n",
      "Epoch 204/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 209.7531 - vae_r_loss: 204.0711 - vae_kl_loss: 5.6819\n",
      "Epoch 205/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 208.9873 - vae_r_loss: 203.0537 - vae_kl_loss: 5.9335\n",
      "Epoch 206/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 209.5754 - vae_r_loss: 203.6622 - vae_kl_loss: 5.9132\n",
      "Epoch 207/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 209.5410 - vae_r_loss: 203.5577 - vae_kl_loss: 5.9834: 0s - loss: 210.3646 - vae_r_loss: 204.3428 - vae_kl_loss: \n",
      "Epoch 208/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.1323 - vae_r_loss: 204.3426 - vae_kl_loss: 5.7898\n",
      "Epoch 209/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.3383 - vae_r_loss: 205.4650 - vae_kl_loss: 5.8732\n",
      "Epoch 210/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 211.1230 - vae_r_loss: 205.1639 - vae_kl_loss: 5.9592\n",
      "Epoch 211/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 210.1174 - vae_r_loss: 204.2515 - vae_kl_loss: 5.8659\n",
      "Epoch 212/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 208.3471 - vae_r_loss: 202.6861 - vae_kl_loss: 5.6610\n",
      "Epoch 213/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 207.3811 - vae_r_loss: 201.4408 - vae_kl_loss: 5.9403\n",
      "Epoch 214/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 207.7898 - vae_r_loss: 201.8821 - vae_kl_loss: 5.9077\n",
      "Epoch 215/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 207.7429 - vae_r_loss: 201.7758 - vae_kl_loss: 5.9671\n",
      "Epoch 216/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 208.0699 - vae_r_loss: 202.0392 - vae_kl_loss: 6.0306\n",
      "Epoch 217/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 208.0316 - vae_r_loss: 202.0660 - vae_kl_loss: 5.9656\n",
      "Epoch 218/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 208.8298 - vae_r_loss: 202.6874 - vae_kl_loss: 6.1425\n",
      "Epoch 219/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 207.2929 - vae_r_loss: 201.3265 - vae_kl_loss: 5.9664\n",
      "Epoch 220/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 207.5028 - vae_r_loss: 201.6767 - vae_kl_loss: 5.8260\n",
      "Epoch 221/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 207.8063 - vae_r_loss: 201.6751 - vae_kl_loss: 6.1312\n",
      "Epoch 222/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 208.4946 - vae_r_loss: 202.5016 - vae_kl_loss: 5.9930\n",
      "Epoch 223/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 208.8760 - vae_r_loss: 202.8699 - vae_kl_loss: 6.0060\n",
      "Epoch 224/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 208.3914 - vae_r_loss: 202.5353 - vae_kl_loss: 5.8561\n",
      "Epoch 225/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.9726 - vae_r_loss: 200.8625 - vae_kl_loss: 6.1101\n",
      "Epoch 226/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 207.1325 - vae_r_loss: 201.1061 - vae_kl_loss: 6.0265\n",
      "Epoch 227/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.1853 - vae_r_loss: 200.1075 - vae_kl_loss: 6.0778\n",
      "Epoch 228/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.6713 - vae_r_loss: 200.6752 - vae_kl_loss: 5.9961\n",
      "Epoch 229/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.1245 - vae_r_loss: 200.0176 - vae_kl_loss: 6.1069\n",
      "Epoch 230/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.0559 - vae_r_loss: 199.9664 - vae_kl_loss: 6.0895\n",
      "Epoch 231/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.3452 - vae_r_loss: 200.2735 - vae_kl_loss: 6.0717\n",
      "Epoch 232/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.0916 - vae_r_loss: 200.0621 - vae_kl_loss: 6.0295\n",
      "Epoch 233/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.5356 - vae_r_loss: 199.3880 - vae_kl_loss: 6.1477\n",
      "Epoch 234/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.4440 - vae_r_loss: 199.3101 - vae_kl_loss: 6.1339\n",
      "Epoch 235/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 207.2362 - vae_r_loss: 201.0340 - vae_kl_loss: 6.2022\n",
      "Epoch 236/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.8419 - vae_r_loss: 199.9633 - vae_kl_loss: 5.8786\n",
      "Epoch 237/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.4295 - vae_r_loss: 199.2704 - vae_kl_loss: 6.1591\n",
      "Epoch 238/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.2116 - vae_r_loss: 200.1184 - vae_kl_loss: 6.0933\n",
      "Epoch 239/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.7455 - vae_r_loss: 199.6477 - vae_kl_loss: 6.0977\n",
      "Epoch 240/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.5298 - vae_r_loss: 199.3601 - vae_kl_loss: 6.1697\n",
      "Epoch 241/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 204.7784 - vae_r_loss: 198.6913 - vae_kl_loss: 6.0871\n",
      "Epoch 242/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.2902 - vae_r_loss: 199.1691 - vae_kl_loss: 6.1211\n",
      "Epoch 243/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.3152 - vae_r_loss: 199.0547 - vae_kl_loss: 6.2605\n",
      "Epoch 244/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.9574 - vae_r_loss: 199.8264 - vae_kl_loss: 6.1310: 0s - loss: 198.6975 - vae_r_loss: 192.5986 - vae\n",
      "Epoch 245/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.6497 - vae_r_loss: 199.7155 - vae_kl_loss: 5.9341\n",
      "Epoch 246/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 204.8529 - vae_r_loss: 198.4875 - vae_kl_loss: 6.3653\n",
      "Epoch 247/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 204.7062 - vae_r_loss: 198.5869 - vae_kl_loss: 6.1193\n",
      "Epoch 248/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.5770 - vae_r_loss: 200.3706 - vae_kl_loss: 6.2064\n",
      "Epoch 249/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 206.3600 - vae_r_loss: 200.2064 - vae_kl_loss: 6.1536\n",
      "Epoch 250/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.9253 - vae_r_loss: 199.7313 - vae_kl_loss: 6.1940\n",
      "Epoch 251/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 204.5398 - vae_r_loss: 198.4296 - vae_kl_loss: 6.1103\n",
      "Epoch 252/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.0155 - vae_r_loss: 198.8916 - vae_kl_loss: 6.1240\n",
      "Epoch 253/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 204.5779 - vae_r_loss: 198.3088 - vae_kl_loss: 6.2691\n",
      "Epoch 254/1000\n",
      "500/500 [==============================] - ETA: 0s - loss: 202.9198 - vae_r_loss: 196.6622 - vae_kl_loss: 6.25 - 1s 2ms/step - loss: 203.9858 - vae_r_loss: 197.7068 - vae_kl_loss: 6.2790\n",
      "Epoch 255/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.6982 - vae_r_loss: 197.4511 - vae_kl_loss: 6.2471\n",
      "Epoch 256/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.2823 - vae_r_loss: 196.8788 - vae_kl_loss: 6.4036\n",
      "Epoch 257/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.7094 - vae_r_loss: 197.4702 - vae_kl_loss: 6.2392\n",
      "Epoch 258/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 204.2950 - vae_r_loss: 198.3365 - vae_kl_loss: 5.9585\n",
      "Epoch 259/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 204.1008 - vae_r_loss: 197.6629 - vae_kl_loss: 6.4379: 0s - loss: 204.0370 - vae_r_loss: 197.5915 - vae_kl_loss: \n",
      "Epoch 260/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.4958 - vae_r_loss: 197.1328 - vae_kl_loss: 6.3630\n",
      "Epoch 261/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.2687 - vae_r_loss: 197.0497 - vae_kl_loss: 6.2190\n",
      "Epoch 262/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.8747 - vae_r_loss: 196.6039 - vae_kl_loss: 6.2708\n",
      "Epoch 263/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.7003 - vae_r_loss: 197.2885 - vae_kl_loss: 6.4118\n",
      "Epoch 264/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.5343 - vae_r_loss: 197.2084 - vae_kl_loss: 6.3259\n",
      "Epoch 265/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 204.6970 - vae_r_loss: 198.2275 - vae_kl_loss: 6.4694\n",
      "Epoch 266/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 204.9598 - vae_r_loss: 198.7512 - vae_kl_loss: 6.2086\n",
      "Epoch 267/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 205.1352 - vae_r_loss: 198.8908 - vae_kl_loss: 6.2444\n",
      "Epoch 268/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.4380 - vae_r_loss: 197.0227 - vae_kl_loss: 6.4154\n",
      "Epoch 269/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.3959 - vae_r_loss: 196.0547 - vae_kl_loss: 6.3413\n",
      "Epoch 270/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.9752 - vae_r_loss: 196.6240 - vae_kl_loss: 6.3512\n",
      "Epoch 271/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.8550 - vae_r_loss: 197.4623 - vae_kl_loss: 6.3927\n",
      "Epoch 272/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.1569 - vae_r_loss: 195.9675 - vae_kl_loss: 6.1894\n",
      "Epoch 273/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.5786 - vae_r_loss: 196.2809 - vae_kl_loss: 6.2976\n",
      "Epoch 274/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.4991 - vae_r_loss: 195.0571 - vae_kl_loss: 6.4420\n",
      "Epoch 275/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.8067 - vae_r_loss: 195.5189 - vae_kl_loss: 6.2878\n",
      "Epoch 276/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.5813 - vae_r_loss: 196.2295 - vae_kl_loss: 6.3518\n",
      "Epoch 277/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 203.0235 - vae_r_loss: 196.6182 - vae_kl_loss: 6.4053\n",
      "Epoch 278/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.3781 - vae_r_loss: 195.9956 - vae_kl_loss: 6.3825\n",
      "Epoch 279/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.4057 - vae_r_loss: 195.9623 - vae_kl_loss: 6.4434\n",
      "Epoch 280/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.3755 - vae_r_loss: 195.9640 - vae_kl_loss: 6.4115\n",
      "Epoch 281/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.1758 - vae_r_loss: 195.7206 - vae_kl_loss: 6.4552\n",
      "Epoch 282/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.7098 - vae_r_loss: 196.3335 - vae_kl_loss: 6.3762\n",
      "Epoch 283/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.8764 - vae_r_loss: 196.6848 - vae_kl_loss: 6.1916\n",
      "Epoch 284/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.0869 - vae_r_loss: 195.8452 - vae_kl_loss: 6.2417\n",
      "Epoch 285/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.2227 - vae_r_loss: 195.6720 - vae_kl_loss: 6.5507\n",
      "Epoch 286/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.6340 - vae_r_loss: 194.2439 - vae_kl_loss: 6.3901\n",
      "Epoch 287/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.6308 - vae_r_loss: 194.0036 - vae_kl_loss: 6.6272\n",
      "Epoch 288/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.3165 - vae_r_loss: 194.9205 - vae_kl_loss: 6.3960\n",
      "Epoch 289/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.2510 - vae_r_loss: 194.6560 - vae_kl_loss: 6.5950\n",
      "Epoch 290/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.1742 - vae_r_loss: 194.7279 - vae_kl_loss: 6.4463\n",
      "Epoch 291/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.9430 - vae_r_loss: 194.5761 - vae_kl_loss: 6.3669\n",
      "Epoch 292/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.5183 - vae_r_loss: 194.0313 - vae_kl_loss: 6.4870\n",
      "Epoch 293/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.4804 - vae_r_loss: 193.7308 - vae_kl_loss: 6.7496\n",
      "Epoch 294/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 202.1690 - vae_r_loss: 195.8220 - vae_kl_loss: 6.3470\n",
      "Epoch 295/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.8327 - vae_r_loss: 195.3947 - vae_kl_loss: 6.4380\n",
      "Epoch 296/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.2111 - vae_r_loss: 194.8267 - vae_kl_loss: 6.3845\n",
      "Epoch 297/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.6648 - vae_r_loss: 195.0544 - vae_kl_loss: 6.6104\n",
      "Epoch 298/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.4786 - vae_r_loss: 195.1209 - vae_kl_loss: 6.3576\n",
      "Epoch 299/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 201.0882 - vae_r_loss: 194.7279 - vae_kl_loss: 6.3603\n",
      "Epoch 300/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.5518 - vae_r_loss: 194.0117 - vae_kl_loss: 6.5401\n",
      "Epoch 301/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.2646 - vae_r_loss: 193.7707 - vae_kl_loss: 6.4939\n",
      "Epoch 302/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.0644 - vae_r_loss: 193.6412 - vae_kl_loss: 6.4232\n",
      "Epoch 303/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.3499 - vae_r_loss: 193.6501 - vae_kl_loss: 6.6998\n",
      "Epoch 304/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 199.1554 - vae_r_loss: 192.5285 - vae_kl_loss: 6.6269\n",
      "Epoch 305/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 199.0896 - vae_r_loss: 192.5981 - vae_kl_loss: 6.4915\n",
      "Epoch 306/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 199.4888 - vae_r_loss: 192.9187 - vae_kl_loss: 6.5701\n",
      "Epoch 307/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 200.8273 - vae_r_loss: 194.2134 - vae_kl_loss: 6.6140\n",
      "Epoch 308/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.4350 - vae_r_loss: 193.9864 - vae_kl_loss: 6.4486\n",
      "Epoch 309/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.1998 - vae_r_loss: 193.5292 - vae_kl_loss: 6.6706\n",
      "Epoch 310/1000\n",
      "416/500 [=======================>......] - ETA: 0s - loss: 199.1099 - vae_r_loss: 192.5713 - vae_kl_loss: 6.5386"
     ]
    }
   ],
   "source": [
    "# größeres modell\n",
    "#Wichtig: Beim Output Filteranzahl = 3 -> Dreidimensionales Ergebnis für RGB Interpretation\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [128,128,128,512]\n",
    "    , encoder_conv_kernel_size = [2,2,3,4]\n",
    "    , encoder_conv_strides = [1,1,2,2]\n",
    "    , decoder_conv_t_filters = [512,128,128,3]\n",
    "    , decoder_conv_t_kernel_size = [4,3,2,2]\n",
    "    , decoder_conv_t_strides = [2,2,1,1]\n",
    "    , z_dim = 2\n",
    ")\n",
    "vae.save(RUN_FOLDER)\n",
    "\n",
    "vae.encoder.summary()\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "R_LOSS_FACTOR = 5000\n",
    "vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "vae.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")\n",
    "vae.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Higher Z Dim and Lower R Loss Factor\n",
    "\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [128,128,128,512]\n",
    "    , encoder_conv_kernel_size = [2,2,3,4]\n",
    "    , encoder_conv_strides = [1,1,2,2]\n",
    "    , decoder_conv_t_filters = [512,128,128,3]\n",
    "    , decoder_conv_t_kernel_size = [4,3,2,2]\n",
    "    , decoder_conv_t_strides = [2,2,1,1]\n",
    "    , z_dim = 400\n",
    ")\n",
    "vae.save(RUN_FOLDER)\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "R_LOSS_FACTOR = 250\n",
    "vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 250\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "vae.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")\n",
    "vae.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0000005\n",
    "R_LOSS_FACTOR = 1000\n",
    "vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 250\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "vae.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")\n",
    "vae.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test again\n",
    "\n",
    "\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [32,64,64, 64]\n",
    "    , encoder_conv_kernel_size = [3,3,3,3]\n",
    "    , encoder_conv_strides = [1,2,2,1]\n",
    "    , decoder_conv_t_filters = [64,64,32,1]\n",
    "    , decoder_conv_t_kernel_size = [3,3,3,3]\n",
    "    , decoder_conv_t_strides = [1,2,2,1]\n",
    "    , z_dim = 2\n",
    ")\n",
    "\n",
    "#vae.load_weights(\"C:\\\\Users\\\\adoerr\\\\Desktop\\\\Machine Learning\\\\Aufgabe 2\\\\run\\\\weights\\\\weights-033-34.17.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testindex = 4999\n",
    "img = x_train[testindex]\n",
    "#print(\"Correct label =\",label_name(y_test_numerical[testindex]))\n",
    "print(plt.imshow(img))\n",
    "img = img.reshape((1,) + img.shape)\n",
    "\n",
    "encoding = vae.encoder.predict(img)\n",
    "reconst = vae.decoder.predict(encoding)[0].squeeze()\n",
    "\n",
    "print(encoding)\n",
    "\n",
    "#filepath = os.path.join(self.run_folder, 'images','img_' + str(self.epoch).zfill(3) + '_' + str(batch) + '.jpg')\n",
    "#print(\"Prediction:\",vae.encoder.predict(img))\n",
    "#print(\"Prediction:\",vae.decoder.predict(vae.encoder.predict(img)))\n",
    "#print(vae.decoder.predict(vae.encoder.predict(img)).type\n",
    "#print(vae.encoder.predict)\n",
    "#plt.imshow(vae.decoder.predict(vae.encoder.predict(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.imshow(reconst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "testindex = 51\n",
    "img = x_test[testindex]\n",
    "plt.imshow(img)\n",
    "print(label_name(y_test_numerical[testindex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
