{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#must be very first statement\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data prep\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.datasets import mnist\n",
    "import PIL\n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "cifar10.load_data()\n",
    "\n",
    "my_labels = [6]\n",
    "all_label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "label_names = []\n",
    "for label_index in my_labels:\n",
    "    label_names.append(all_label_names[label_index])  \n",
    "\n",
    "print(\"Defining new Labeling:\")\n",
    "print(dict(zip(range(len(my_labels)),label_names)))\n",
    "\n",
    "#if my_labels = [5,6,8] then 5 returns 0, 6 returns 1, 8 returns 2, ...\n",
    "def convert_label(label):\n",
    "    return dict(zip(my_labels,range(len(my_labels))))[label]\n",
    "\n",
    "def label_name(num):\n",
    "    return label_names[num]\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train_all, y_train_all), (x_test_all, y_test_all) = cifar10.load_data()\n",
    "    \n",
    "#temp lists\n",
    "x_train = []\n",
    "y_train_numerical = []\n",
    "\n",
    "#filter training data for my_labels\n",
    "for i in range(len(x_train_all)):\n",
    "    if y_train_all[i] in my_labels:\n",
    "        x_train.append(x_train_all[i])\n",
    "        y_train_numerical.append(convert_label(y_train_all[i][0]))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train_numerical = np.array(y_train_numerical)\n",
    "\n",
    "print(\"Training Data:\\n\")\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'samples,', y_train_numerical.shape[0], 'labels')\n",
    "print(\"\\nClass  |  Counts:\")\n",
    "(unique, counts) = np.unique(y_train_numerical, return_counts=True)\n",
    "for i, label in enumerate(unique):\n",
    "    print(label_name(label),\"\\t\", counts[i])\n",
    "\n",
    "\n",
    "x_test = []\n",
    "y_test_numerical = []\n",
    "\n",
    "#filter test data\n",
    "for i in range(len(x_test_all)):\n",
    "    if y_test_all[i] in my_labels:\n",
    "        x_test.append(x_test_all[i])\n",
    "        y_test_numerical.append(convert_label(y_test_all[i][0]))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test_numerical = np.array(y_test_numerical)\n",
    "\n",
    "print(\"\\n\\nTesting Data:\\n\")\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_test.shape[0], 'samples,', y_test_numerical.shape[0], 'labels')\n",
    "print(\"\\nClass  |  Counts:\")\n",
    "(unique, counts) = np.unique(y_test_numerical, return_counts=True)\n",
    "for i, label in enumerate(unique):\n",
    "    print(label_name(label),\"\\t\", counts[i])\n",
    "    \n",
    "x_train=x_train.reshape(x_train.shape[0],32,32,3)\n",
    "x_test=x_test.reshape(x_test.shape[0],32,32,3)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = (x_train-127.5)/127.5\n",
    "x_test = (x_test-127.5)/127.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/davidADSP/GDL_code/blob/master/models/GAN.py\n",
    "\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, \\\n",
    "    BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers.merge import _Merge\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self\n",
    "                 , input_dim\n",
    "                 , discriminator_conv_filters\n",
    "                 , discriminator_conv_kernel_size\n",
    "                 , discriminator_conv_strides\n",
    "                 , discriminator_batch_norm_momentum\n",
    "                 , discriminator_activation\n",
    "                 , discriminator_dropout_rate\n",
    "                 , discriminator_learning_rate\n",
    "                 , generator_initial_dense_layer_size\n",
    "                 , generator_upsample\n",
    "                 , generator_conv_filters\n",
    "                 , generator_conv_kernel_size\n",
    "                 , generator_conv_strides\n",
    "                 , generator_batch_norm_momentum\n",
    "                 , generator_activation\n",
    "                 , generator_dropout_rate\n",
    "                 , generator_learning_rate\n",
    "                 , optimiser\n",
    "                 , z_dim\n",
    "                 ):\n",
    "\n",
    "        self.name = 'gan'\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.discriminator_conv_filters = discriminator_conv_filters\n",
    "        self.discriminator_conv_kernel_size = discriminator_conv_kernel_size\n",
    "        self.discriminator_conv_strides = discriminator_conv_strides\n",
    "        self.discriminator_batch_norm_momentum = discriminator_batch_norm_momentum\n",
    "        self.discriminator_activation = discriminator_activation\n",
    "        self.discriminator_dropout_rate = discriminator_dropout_rate\n",
    "        self.discriminator_learning_rate = discriminator_learning_rate\n",
    "\n",
    "        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n",
    "        self.generator_upsample = generator_upsample\n",
    "        self.generator_conv_filters = generator_conv_filters\n",
    "        self.generator_conv_kernel_size = generator_conv_kernel_size\n",
    "        self.generator_conv_strides = generator_conv_strides\n",
    "        self.generator_batch_norm_momentum = generator_batch_norm_momentum\n",
    "        self.generator_activation = generator_activation\n",
    "        self.generator_dropout_rate = generator_dropout_rate\n",
    "        self.generator_learning_rate = generator_learning_rate\n",
    "\n",
    "        self.optimiser = optimiser\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.n_layers_discriminator = len(discriminator_conv_filters)\n",
    "        self.n_layers_generator = len(generator_conv_filters)\n",
    "\n",
    "        self.weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "\n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "\n",
    "        self.epoch = 0\n",
    "\n",
    "        self._build_discriminator()\n",
    "        self._build_generator()\n",
    "\n",
    "        self._build_adversarial()\n",
    "\n",
    "    def get_activation(self, activation):\n",
    "        if activation == 'leaky_relu':\n",
    "            layer = LeakyReLU(alpha=0.2)\n",
    "        else:\n",
    "            layer = Activation(activation)\n",
    "        return layer\n",
    "\n",
    "    def _build_discriminator(self):\n",
    "\n",
    "        ### THE discriminator\n",
    "        discriminator_input = Input(shape=self.input_dim, name='discriminator_input')\n",
    "\n",
    "        x = discriminator_input\n",
    "\n",
    "        for i in range(self.n_layers_discriminator):\n",
    "\n",
    "            x = Conv2D(\n",
    "                filters=self.discriminator_conv_filters[i]\n",
    "                , kernel_size=self.discriminator_conv_kernel_size[i]\n",
    "                , strides=self.discriminator_conv_strides[i]\n",
    "                , padding='same'\n",
    "                , name='discriminator_conv_' + str(i)\n",
    "                , kernel_initializer=self.weight_init\n",
    "            )(x)\n",
    "\n",
    "            if self.discriminator_batch_norm_momentum and i > 0:\n",
    "                x = BatchNormalization(momentum=self.discriminator_batch_norm_momentum)(x)\n",
    "\n",
    "            x = self.get_activation(self.discriminator_activation)(x)\n",
    "\n",
    "            #Removing dropout layers\n",
    "            #if self.discriminator_dropout_rate:\n",
    "            #    x = Dropout(rate=self.discriminator_dropout_rate)(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = Dropout(rate=self.discriminator_dropout_rate)(x)\n",
    "        \n",
    "        discriminator_output = Dense(1, activation='sigmoid', kernel_initializer=self.weight_init)(x)\n",
    "\n",
    "        self.discriminator = Model(discriminator_input, discriminator_output)\n",
    "\n",
    "    def _build_generator(self):\n",
    "\n",
    "        ### THE generator\n",
    "\n",
    "        generator_input = Input(shape=(self.z_dim,), name='generator_input')\n",
    "\n",
    "        x = generator_input\n",
    "\n",
    "        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer=self.weight_init)(x)\n",
    "\n",
    "        if self.generator_batch_norm_momentum:\n",
    "            x = BatchNormalization(momentum=self.generator_batch_norm_momentum)(x)\n",
    "\n",
    "        x = self.get_activation(self.generator_activation)(x)\n",
    "\n",
    "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
    "\n",
    "        if self.generator_dropout_rate:\n",
    "            x = Dropout(rate=self.generator_dropout_rate)(x)\n",
    "\n",
    "        for i in range(self.n_layers_generator):\n",
    "\n",
    "            if self.generator_upsample[i] == 2:\n",
    "                x = UpSampling2D()(x)\n",
    "                x = Conv2D(\n",
    "                    filters=self.generator_conv_filters[i]\n",
    "                    , kernel_size=self.generator_conv_kernel_size[i]\n",
    "                    , padding='same'\n",
    "                    , name='generator_conv_' + str(i)\n",
    "                    , kernel_initializer=self.weight_init\n",
    "                )(x)\n",
    "            else:\n",
    "\n",
    "                x = Conv2DTranspose(\n",
    "                    filters=self.generator_conv_filters[i]\n",
    "                    , kernel_size=self.generator_conv_kernel_size[i]\n",
    "                    , padding='same'\n",
    "                    , strides=self.generator_conv_strides[i]\n",
    "                    , name='generator_conv_' + str(i)\n",
    "                    , kernel_initializer=self.weight_init\n",
    "                )(x)\n",
    "\n",
    "            if i < self.n_layers_generator - 1:\n",
    "\n",
    "                if self.generator_batch_norm_momentum:\n",
    "                    x = BatchNormalization(momentum=self.generator_batch_norm_momentum)(x)\n",
    "\n",
    "                x = self.get_activation(self.generator_activation)(x)\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                x = Activation('tanh')(x)\n",
    "\n",
    "        generator_output = x\n",
    "\n",
    "        self.generator = Model(generator_input, generator_output)\n",
    "\n",
    "    def get_opti(self, lr):\n",
    "        if self.optimiser == 'adam':\n",
    "            opti = Adam(lr=lr, beta_1=0.5)\n",
    "        elif self.optimiser == 'rmsprop':\n",
    "            opti = RMSprop(lr=lr)\n",
    "        else:\n",
    "            opti = Adam(lr=lr)\n",
    "\n",
    "        return opti\n",
    "\n",
    "    def set_trainable(self, m, val):\n",
    "        m.trainable = val\n",
    "        for l in m.layers:\n",
    "            l.trainable = val\n",
    "\n",
    "    def _build_adversarial(self):\n",
    "\n",
    "        ### COMPILE DISCRIMINATOR\n",
    "\n",
    "        self.discriminator.compile(\n",
    "            optimizer=self.get_opti(self.discriminator_learning_rate)\n",
    "            , loss='binary_crossentropy'\n",
    "            , metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        ### COMPILE THE FULL GAN\n",
    "\n",
    "        self.set_trainable(self.discriminator, False)\n",
    "\n",
    "        model_input = Input(shape=(self.z_dim,), name='model_input')\n",
    "        model_output = self.discriminator(self.generator(model_input))\n",
    "        self.model = Model(model_input, model_output)\n",
    "\n",
    "        self.model.compile(optimizer=self.get_opti(self.generator_learning_rate), loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "        self.set_trainable(self.discriminator, True)\n",
    "\n",
    "\n",
    "    def train_discriminator(self, x_train, batch_size, using_generator):\n",
    "\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        if using_generator:\n",
    "            true_imgs = next(x_train)[0]     \n",
    "            if true_imgs.shape[0] != batch_size:\n",
    "                true_imgs = next(x_train)[0]\n",
    "        else:\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            true_imgs = x_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        d_loss_real, d_acc_real = self.discriminator.train_on_batch(true_imgs, valid)\n",
    "        d_loss_fake, d_acc_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "        d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
    "\n",
    "        return [d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake]\n",
    "    \n",
    "    def train_generator(self, batch_size):\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
    "        return self.model.train_on_batch(noise, valid)\n",
    "\n",
    "    def train(self, x_train, batch_size, epochs, run_folder\n",
    "              , print_every_n_batches=50\n",
    "              , using_generator=False):\n",
    "\n",
    "        for epoch in range(self.epoch, self.epoch + epochs):\n",
    "            \n",
    "            d = self.train_discriminator(x_train, batch_size, using_generator)\n",
    "            g = self.train_generator(batch_size)\n",
    "\n",
    "            print(\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (\n",
    "            epoch, d[0], d[1], d[2], d[3], d[4], d[5], g[0], g[1]))\n",
    "\n",
    "            self.d_losses.append(d)\n",
    "            self.g_losses.append(g)\n",
    "\n",
    "            if epoch % print_every_n_batches == 0:\n",
    "                self.sample_images(run_folder)\n",
    "                #self.model.save_weights(os.path.join(run_folder, 'weights/weights-%d.h5' % (epoch)))\n",
    "                self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n",
    "                self.save_model(run_folder)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def sample_images(self, run_folder):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.z_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "        gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
    "        cnt = 0\n",
    "\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(np.squeeze(gen_imgs[cnt, :, :, :]))\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(os.path.join(run_folder, \"images/sample_%d.png\" % self.epoch))\n",
    "        plt.close()\n",
    "\n",
    "    def plot_model(self, run_folder):\n",
    "        plot_model(self.model, to_file=os.path.join(run_folder, 'viz/model.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "        plot_model(self.discriminator, to_file=os.path.join(run_folder, 'viz/discriminator.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "        plot_model(self.generator, to_file=os.path.join(run_folder, 'viz/generator.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "\n",
    "    def save(self, folder):\n",
    "\n",
    "        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n",
    "            pkl.dump([\n",
    "                self.input_dim\n",
    "                , self.discriminator_conv_filters\n",
    "                , self.discriminator_conv_kernel_size\n",
    "                , self.discriminator_conv_strides\n",
    "                , self.discriminator_batch_norm_momentum\n",
    "                , self.discriminator_activation\n",
    "                , self.discriminator_dropout_rate\n",
    "                , self.discriminator_learning_rate\n",
    "                , self.generator_initial_dense_layer_size\n",
    "                , self.generator_upsample\n",
    "                , self.generator_conv_filters\n",
    "                , self.generator_conv_kernel_size\n",
    "                , self.generator_conv_strides\n",
    "                , self.generator_batch_norm_momentum\n",
    "                , self.generator_activation\n",
    "                , self.generator_dropout_rate\n",
    "                , self.generator_learning_rate\n",
    "                , self.optimiser\n",
    "                , self.z_dim\n",
    "            ], f)\n",
    "\n",
    "        self.plot_model(folder)\n",
    "\n",
    "    def save_model(self, run_folder):\n",
    "        self.model.save(os.path.join(run_folder, 'model.h5'))\n",
    "        self.discriminator.save(os.path.join(run_folder, 'discriminator.h5'))\n",
    "        self.generator.save(os.path.join(run_folder, 'generator.h5'))\n",
    "        pkl.dump(self, open(os.path.join(run_folder, \"obj.pkl\"), \"wb\"))\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applyGan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# run params\n",
    "SECTION = 'gan'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'cifar10_frogs'\n",
    "RUN_FOLDER = 'run_{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #\n",
    "#mode = 'load'\n",
    "\n",
    "#(x_train, y_train) = load_safari(DATA_NAME)\n",
    "\n",
    "x_train.shape\n",
    "\n",
    "plt.imshow(x_train[200,:,:,0])\n",
    "\n",
    "gan = GAN(input_dim = (32,32,3)\n",
    "          #Flatten>DO>Dense(1 sigmoid) nach convs\n",
    "          #padding=same\n",
    "          #leaky relu alpha 0.2\n",
    "          #opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "          #loss='binary_crossentropy'\n",
    "        , discriminator_conv_filters = [64,128,128,256]\n",
    "        , discriminator_conv_kernel_size = [3,3,3,3]\n",
    "        , discriminator_conv_strides = [1,2,2,2]\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'leaky_relu'\n",
    "        , discriminator_dropout_rate = 0.4\n",
    "        , discriminator_learning_rate = 0.0002 #0.0008\n",
    "          \n",
    "          \n",
    "        , generator_initial_dense_layer_size = (4,4,256)\n",
    "        , generator_upsample = [2,2,1,1]\n",
    "        , generator_conv_filters = [128,128,128,3]\n",
    "        , generator_conv_kernel_size = [4,4,4,4]\n",
    "        , generator_conv_strides = [2,2,2,1]\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'leaky_relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0002 #0.0004\n",
    "        , optimiser = 'adam'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "if mode == 'build':\n",
    "    gan.save(RUN_FOLDER)\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))\n",
    "\n",
    "gan.discriminator.summary()\n",
    "\n",
    "gan.generator.summary()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200 * BATCH_SIZE\n",
    "PRINT_EVERY_N_BATCHES = BATCH_SIZE * 5\n",
    "\n",
    "gan.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , using_generator = False\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "\n",
    "plt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[0] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(0, 2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([x[3] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "plt.plot([x[4] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[5] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('accuracy', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# run params\n",
    "SECTION = 'gan'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'cifar10_frogs'\n",
    "RUN_FOLDER = 'run_{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #\n",
    "mode = 'load'\n",
    "\n",
    "#(x_train, y_train) = load_safari(DATA_NAME)\n",
    "\n",
    "x_train.shape\n",
    "\n",
    "plt.imshow(x_train[200,:,:,0])\n",
    "\n",
    "gan = GAN(input_dim = (32,32,3)\n",
    "          #Flatten>DO>Dense(1 sigmoid) nach convs\n",
    "          #padding=same\n",
    "          #leaky relu alpha 0.2\n",
    "          #opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "          #loss='binary_crossentropy'\n",
    "        , discriminator_conv_filters = [64,128,128,256]\n",
    "        , discriminator_conv_kernel_size = [3,3,3,3]\n",
    "        , discriminator_conv_strides = [1,2,2,2]\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'leaky_relu'\n",
    "        , discriminator_dropout_rate = 0.4\n",
    "        , discriminator_learning_rate = 0.0001 #0.0008\n",
    "          \n",
    "          \n",
    "        , generator_initial_dense_layer_size = (4,4,256)\n",
    "        , generator_upsample = [2,2,1,1]\n",
    "        , generator_conv_filters = [128,128,128,3]\n",
    "        , generator_conv_kernel_size = [4,4,4,4]\n",
    "        , generator_conv_strides = [2,2,2,1]\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'leaky_relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0001 #0.0004\n",
    "        , optimiser = 'adam'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "if mode == 'build':\n",
    "    gan.save(RUN_FOLDER)\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))\n",
    "\n",
    "gan.discriminator.summary()\n",
    "\n",
    "gan.generator.summary()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200 * BATCH_SIZE\n",
    "PRINT_EVERY_N_BATCHES = BATCH_SIZE * 5\n",
    "\n",
    "gan.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , using_generator = False\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "\n",
    "plt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[0] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(0, 2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([x[3] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "plt.plot([x[4] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[5] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('accuracy', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# run params\n",
    "SECTION = 'gan'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'cifar10_frogs'\n",
    "RUN_FOLDER = 'run_{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #\n",
    "mode = 'load'\n",
    "\n",
    "#(x_train, y_train) = load_safari(DATA_NAME)\n",
    "\n",
    "x_train.shape\n",
    "\n",
    "plt.imshow(x_train[200,:,:,0])\n",
    "\n",
    "gan = GAN(input_dim = (32,32,3)\n",
    "          #Flatten>DO>Dense(1 sigmoid) nach convs\n",
    "          #padding=same\n",
    "          #leaky relu alpha 0.2\n",
    "          #opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "          #loss='binary_crossentropy'\n",
    "        , discriminator_conv_filters = [64,128,128,256]\n",
    "        , discriminator_conv_kernel_size = [3,3,3,3]\n",
    "        , discriminator_conv_strides = [1,2,2,2]\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'leaky_relu'\n",
    "        , discriminator_dropout_rate = 0.4\n",
    "        , discriminator_learning_rate = 0.00003 #0.0008\n",
    "          \n",
    "          \n",
    "        , generator_initial_dense_layer_size = (4,4,256)\n",
    "        , generator_upsample = [2,2,1,1]\n",
    "        , generator_conv_filters = [128,128,128,3]\n",
    "        , generator_conv_kernel_size = [4,4,4,4]\n",
    "        , generator_conv_strides = [2,2,2,1]\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'leaky_relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.00003 #0.0004\n",
    "        , optimiser = 'adam'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "if mode == 'build':\n",
    "    gan.save(RUN_FOLDER)\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))\n",
    "\n",
    "gan.discriminator.summary()\n",
    "\n",
    "gan.generator.summary()\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200 * BATCH_SIZE\n",
    "PRINT_EVERY_N_BATCHES = BATCH_SIZE * 5\n",
    "\n",
    "gan.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , using_generator = False\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "\n",
    "plt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[0] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(0, 2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([x[3] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "plt.plot([x[4] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[5] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('accuracy', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "noise = np.random.normal(0, 1, (1, z_dim))\n",
    "print(noise)\n",
    "\n",
    "r, c = 10, 10\n",
    "fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
    "cnt = 0\n",
    "imgcnt = 0\n",
    "\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        \n",
    "        imgcnt += 1\n",
    "\n",
    "        gen_imgs = gan.generator.predict(noise)\n",
    "        gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "        gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "        \n",
    "        axs[i, j].imshow(np.squeeze(gen_imgs[cnt, :, :, :]))\n",
    "        axs[i, j].axis('off')\n",
    "        \n",
    "        \n",
    "        #cnt += 1\n",
    "#fig.savefig(os.path.join(run_folder, \"images/sample_%d.png\" % self.epoch))\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "\n",
    "z_dim = 100\n",
    "initial_noise = np.random.normal(0, 1, (1, z_dim))\n",
    "#print(noise)\n",
    "\n",
    "def printimg(x):\n",
    "    r, c = 1, 1\n",
    "    fig, axs = plt.subplots(r, c, figsize=(3, 3))\n",
    "    cnt = 0\n",
    "    imgcnt = 0\n",
    "\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            \n",
    "            noise = []\n",
    "            for val in initial_noise:\n",
    "                noise.append(val*x)\n",
    "            \n",
    "            noise = np.array(noise)\n",
    "            \n",
    "            imgcnt += 1\n",
    "\n",
    "            gen_imgs = gan.generator.predict(noise)\n",
    "            gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "            gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "            axs.imshow(np.squeeze(gen_imgs[cnt, :, :, :]))\n",
    "            axs.axis('off')\n",
    "    return x\n",
    "\n",
    "interact(printimg, x=widgets.FloatSlider(min=-5, max=5, step=0.001, value=1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "z_dim = 100\n",
    "initial_noise = np.random.normal(0, 1, (1, z_dim))\n",
    "\n",
    "from ipywidgets import *\n",
    "\n",
    "z_dim = 100\n",
    "initial_noise = np.random.normal(0, 1, (1, z_dim))\n",
    "#print(noise)\n",
    "\n",
    "def printimg(**args):\n",
    "    r, c = 1, 1\n",
    "    fig, axs = plt.subplots(r, c, figsize=(3, 3))\n",
    "    cnt = 0\n",
    "    imgcnt = 0\n",
    "\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            \n",
    "            noise = []\n",
    "            for arg in args:\n",
    "                noise.append(args[arg])\n",
    "            \n",
    "            noise = np.array(noise).T\n",
    "            noise = np.reshape(noise, (100,1)).T\n",
    "            \n",
    "            imgcnt += 1\n",
    "\n",
    "            gen_imgs = gan.generator.predict(noise)\n",
    "            gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "            gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "            axs.imshow(np.squeeze(gen_imgs[cnt, :, :, :]))\n",
    "            axs.axis('off')\n",
    "    return None\n",
    "\n",
    "start_value=1\n",
    "\n",
    "interact(printimg, \n",
    "            x1=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x2=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x3=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x4=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x5=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x6=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x7=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x8=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x9=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x10=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x11=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x12=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x13=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x14=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x15=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x16=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x17=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x18=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x19=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x20=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x21=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x22=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x23=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x24=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x25=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x26=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x27=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x28=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x29=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x30=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x31=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x32=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x33=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x34=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x35=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x36=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x37=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x38=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x39=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x40=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x41=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x42=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x43=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x44=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x45=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x46=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x47=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x48=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x49=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x50=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x51=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x52=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x53=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x54=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x55=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x56=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x57=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x58=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x59=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x60=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x61=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x62=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x63=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x64=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x65=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x66=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x67=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x68=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x69=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x70=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x71=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x72=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x73=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x74=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x75=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x76=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x77=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x78=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x79=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x80=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x81=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x82=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x83=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x84=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x85=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x86=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x87=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x88=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x89=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x90=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x91=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x92=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x93=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x94=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x95=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x96=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x97=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x98=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x99=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value),\n",
    "            x100=widgets.FloatSlider(min=0, max=1, step=0.001, value=start_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
