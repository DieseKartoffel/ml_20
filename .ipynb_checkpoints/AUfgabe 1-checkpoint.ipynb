{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (Guckert) SS2020\n",
    "\n",
    "### Tensorboard Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "  \n",
    "  model = create_model()\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "  model.fit(x=x_train, \n",
    "            y=y_train, \n",
    "            epochs=5, \n",
    "            validation_data=(x_test, y_test), \n",
    "            callbacks=[tensorboard_callback])\n",
    "\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1 (MNIST):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def randomPoints(n,m=3,b=4,seed=None):\n",
    "    if seed!=None:\n",
    "        np.random.seed(seed)\n",
    "    # Create 100 random values for x between 0 and 2...\n",
    "    x = 2 * np.random.rand(n, 1)\n",
    "    # Create 100 values for y linearly dependent on x and add some noise...\n",
    "    y = b + m * x + np.random.randn(n, 1)\n",
    "    return x,y\n",
    "\n",
    "def randomPointsQuadratic(m):\n",
    "    x = 6 * np.random.rand(m, 1) - 3\n",
    "    y = 0.5 * x ** 2 + x + 2 + np.random.randn(m, 1)\n",
    "    return x,y\n",
    "\n",
    "\n",
    "def learningCurvePlot(model, x, y):\n",
    "    # Split data into trainig and validation, x and y values respectively\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    # Increase number of training records used\n",
    "    for m in range(1, len(x_train)):\n",
    "        model.fit(x_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(x_train[:m])\n",
    "        y_val_predict = model.predict(x_val)\n",
    "        train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
    "        val_errors.append(mean_squared_error(y_val_predict, y_val))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.ylim(0, 3)\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plotPoints(x,y,show=False):\n",
    "    plt.plot(x, y, 'ro')\n",
    "\n",
    "def plotLine(theta,show=False):\n",
    "    xx = np.linspace(0, 2, 1000)\n",
    "    plt.plot(xx, theta[1] * xx + theta[0], linestyle='solid')\n",
    "    if show==True:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "def getDigits(show=False):\n",
    "    (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "    print(x_train.shape)\n",
    "\n",
    "    if show:\n",
    "        for index, (image, label) in enumerate(zip(x_train[0:5], t_train[0:5])):\n",
    "            plt.subplot(1, 5, index + 1)\n",
    "            print(index,image,label)\n",
    "            plt.imshow(image, cmap=plt.cm.gray)\n",
    "            plt.title('Image: %i\\n' % label, fontsize = 12)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return x_train,t_train,x_test, t_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train original shape (60000, 28, 28)\n",
      "y_train original shape (60000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "from tensorflow.keras import Sequential, optimizers\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Dense, GlobalAveragePooling1D, AveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_digits\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import  plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: None\n",
      "(60000, 28, 28)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 13s 221us/sample - loss: 0.2816 - accuracy: 0.9062 - val_loss: 0.0833 - val_accuracy: 0.9735\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 11s 189us/sample - loss: 0.0532 - accuracy: 0.9840 - val_loss: 0.0414 - val_accuracy: 0.9870\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 9s 158us/sample - loss: 0.0358 - accuracy: 0.9894 - val_loss: 0.0349 - val_accuracy: 0.9886\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 9s 145us/sample - loss: 0.0268 - accuracy: 0.9924 - val_loss: 0.0354 - val_accuracy: 0.9886\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 10s 168us/sample - loss: 0.0217 - accuracy: 0.9938 - val_loss: 0.0322 - val_accuracy: 0.9893\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 10s 163us/sample - loss: 0.0183 - accuracy: 0.9948 - val_loss: 0.0303 - val_accuracy: 0.9897\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 10s 167us/sample - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0286 - val_accuracy: 0.9903\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 10s 165us/sample - loss: 0.0138 - accuracy: 0.9962 - val_loss: 0.0283 - val_accuracy: 0.9905\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0123 - accuracy: 0.9968 - val_loss: 0.0286 - val_accuracy: 0.9905\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0110 - accuracy: 0.9973 - val_loss: 0.0315 - val_accuracy: 0.9907\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 8s 141us/sample - loss: 0.0099 - accuracy: 0.9976 - val_loss: 0.0294 - val_accuracy: 0.9900\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 9s 147us/sample - loss: 0.0092 - accuracy: 0.9978 - val_loss: 0.0298 - val_accuracy: 0.9901\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 8s 139us/sample - loss: 0.0085 - accuracy: 0.9982 - val_loss: 0.0289 - val_accuracy: 0.9908\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 8s 139us/sample - loss: 0.0078 - accuracy: 0.9983 - val_loss: 0.0293 - val_accuracy: 0.9902\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0074 - accuracy: 0.9984 - val_loss: 0.0292 - val_accuracy: 0.9906\n",
      "10000/10000 [==============================] - 1s 79us/sample - loss: 0.0292 - accuracy: 0.9906\n",
      "Test accuracy: 0.9906\n",
      "(60000, 28, 28)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 11s 180us/sample - loss: 0.2932 - accuracy: 0.9033 - val_loss: 0.0688 - val_accuracy: 0.9796\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0537 - accuracy: 0.9834 - val_loss: 0.0421 - val_accuracy: 0.9866\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.0360 - accuracy: 0.9892 - val_loss: 0.0396 - val_accuracy: 0.9863\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 8s 141us/sample - loss: 0.0272 - accuracy: 0.9919 - val_loss: 0.0342 - val_accuracy: 0.9887\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 9s 150us/sample - loss: 0.0214 - accuracy: 0.9934 - val_loss: 0.0346 - val_accuracy: 0.9884\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0182 - accuracy: 0.9948 - val_loss: 0.0305 - val_accuracy: 0.9902\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 9s 142us/sample - loss: 0.0155 - accuracy: 0.9958 - val_loss: 0.0304 - val_accuracy: 0.9904\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0135 - accuracy: 0.9966 - val_loss: 0.0303 - val_accuracy: 0.9903\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 8s 139us/sample - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.0299 - val_accuracy: 0.9911\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 9s 153us/sample - loss: 0.0109 - accuracy: 0.9976 - val_loss: 0.0301 - val_accuracy: 0.9907\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 9s 147us/sample - loss: 0.0101 - accuracy: 0.9975 - val_loss: 0.0321 - val_accuracy: 0.9902\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 9s 144us/sample - loss: 0.0092 - accuracy: 0.9980 - val_loss: 0.0297 - val_accuracy: 0.9909\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.0298 - val_accuracy: 0.9911\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 9s 142us/sample - loss: 0.0081 - accuracy: 0.9984 - val_loss: 0.0295 - val_accuracy: 0.9912\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 9s 142us/sample - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0290 - val_accuracy: 0.9914\n",
      "10000/10000 [==============================] - 1s 79us/sample - loss: 0.0290 - accuracy: 0.9914\n",
      "Test accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "filtergrößen = [3, 6, 12]\n",
    "anzahl_filters = [32 ,64, 128]\n",
    "conv_layerss = [2, 4]\n",
    "neural_layerss = [1, 3] # before softmax layer\n",
    "neural_unitss = [16, 64, 128] # bestes ergebnis am ende noch mal kleiner werden lassen?\n",
    "activation_functionss = [\"relu\"]\n",
    "\n",
    "## after grid:\n",
    "filtergrößen = [6]\n",
    "anzahl_filters = [100]\n",
    "#conv_layerss = [2]\n",
    "neural_layerss = [3] # before softmax layer\n",
    "neural_unitss = [128] # bestes ergebnis am ende noch mal kleiner werden lassen?\n",
    "activation_functionss = [\"relu\"]\n",
    "\n",
    "for filtergröße in filtergrößen:\n",
    "    for anzahl_filter in anzahl_filters:\n",
    "        for conv_layers in conv_layerss:\n",
    "            for neural_layers in neural_layerss:\n",
    "                for neural_units in neural_unitss:\n",
    "                    for activation_function in activation_functionss:                      \n",
    "                        logdir = os.path.join(\"logs\",\"finetune\", \"{}_filtersize-{}_filters-{}_conv_nlayers-CCPCP_nunits{}_act{}\".format(i, filtergröße, anzahl_filter,neural_layers, neural_units, activation_function))\n",
    "\n",
    "                        # Load train and test data\n",
    "                        train_images, train_labels, test_images, test_labels = getDigits()\n",
    "\n",
    "                        # Normalize color values (here: grey-scales) 0-1\n",
    "                        train_images = train_images / 255.0\n",
    "                        test_images = test_images / 255.0\n",
    "\n",
    "                        # Do one-hot encoding / do categorical conversion\n",
    "                        train_labels = to_categorical(train_labels)\n",
    "                        test_labels = to_categorical(test_labels)\n",
    "\n",
    "                        num_epochs=15\n",
    "\n",
    "                        # Extract number of classes from data dimensions\n",
    "                        classes = np.shape(train_labels)[1]\n",
    "\n",
    "                        # Define model architecture\n",
    "                        model = Sequential()\n",
    "\n",
    "                        model.add(Conv1D(input_shape=(28, 28), filters=anzahl_filter, kernel_size=filtergröße, padding='valid', activation=activation_function))\n",
    "                        model.add(Conv1D(filters=anzahl_filter, kernel_size=filtergröße, padding='valid', activation=activation_function))\n",
    "                        #model.add(MaxPool1D(strides=1, pool_size=1))\n",
    "                        model.add(MaxPool1D(strides=2, pool_size=2))\n",
    "\n",
    "                        model.add(Conv1D(filters=anzahl_filter, kernel_size=filtergröße, padding='valid', activation=activation_function))\n",
    "                        model.add(GlobalAveragePooling1D())\n",
    "\n",
    "                        # FCC\n",
    "                        for i in range(neural_layers):\n",
    "                            model.add(Dense(units=neural_units, activation=activation_function))\n",
    "\n",
    "                        # Classifier\n",
    "                        model.add(Dense(units=classes, activation='softmax'))\n",
    "\n",
    "                        # Compile model\n",
    "                        opt = optimizers.SGD(lr=0.2, momentum=0.0, decay=0.001, nesterov=False)\n",
    "                        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']\n",
    "\n",
    "                        #Board Callback\n",
    "                        tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "                        #visualize model\n",
    "                        plot_model(model, to_file=logdir+\".png\")\n",
    "                                      \n",
    "                        # Train model\n",
    "                        model.fit(x=train_images, y=train_labels, epochs=num_epochs, validation_data=(test_images, test_labels), callbacks=[tensorboard_callback])\n",
    "\n",
    "                        # Evaluate model\n",
    "                        test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "                        print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 mal :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDigits():\n",
    "    (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "    print(x_train.shape)\n",
    "    plt.show()\n",
    "    return x_train,t_train,x_test, t_test\n",
    "\n",
    "# load data\n",
    "\n",
    "# split data into training and test\n",
    "x_train, t_train, x_test, t_test = getDigits()\n",
    "\n",
    "num_classes=10\n",
    "# One hot vectors\n",
    "t_train = tf.keras.utils.to_categorical(t_train, num_classes)\n",
    "t_test = tf.keras.utils.to_categorical(t_test, num_classes)\n",
    "\n",
    "\n",
    "train_size=x_train.shape[0]\n",
    "num_features=x_train.shape[1]\n",
    "test_size=x_test.shape[0]\n",
    "print(train_size, num_features)\n",
    "\n",
    "x_train = x_train.reshape(train_size, num_features*num_features)\n",
    "x_test = x_test.reshape(test_size, num_features*num_features)\n",
    "\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "num_features=x_train.shape[1]\n",
    "\n",
    "\n",
    "learning_rate=0.1\n",
    "#training_epochs = 10\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(name='A',units=num_classes, input_dim=num_features, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "\n",
    "#training_history = model.fit(\n",
    "#    x_train, # input\n",
    "#    t_train, # output\n",
    "#    batch_size=32,\n",
    "#    verbose=1, # Suppress chatty output; use Tensorboard instead\n",
    "#    epochs=training_epochs,\n",
    "#    validation_data=(x_test, t_test),\n",
    "#    callbacks=[tensorboard_callback],\n",
    "#)\n",
    "\n",
    "# Extract output of first layer\n",
    "#layer_name = 'A'\n",
    "#intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "#intermediate_output = intermediate_layer_model.predict(x_test[0].reshape(1,-1))\n",
    "#print(\"Output Layer A:\",intermediate_output)\n",
    "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "#print(model.metrics_names, model.evaluate(x_test,t_test))\n",
    "#print(x_test[0])\n",
    "#print(model.predict(x_test[0].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "#train_images = X_train\n",
    "#train_labels = y_train\n",
    "#test_images = X_test\n",
    "#test_labels = y_test\n",
    "train_images, train_labels, test_images, test_labels = getDigits()\n",
    "\n",
    "# Normalize color values (here: grey-scales) 0-1\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Do one-hot encoding / do categorical conversion\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "num_epochs=10\n",
    "\n",
    "# Extract number of classes from data dimensions\n",
    "classes = np.shape(train_labels)[1]\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional and pooling layer\n",
    "model.add(Conv1D(input_shape=(28, 28), filters=64, kernel_size=3, padding='valid', activation='relu'))\n",
    "model.add(MaxPool1D(strides=2, pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='valid', activation='relu'))\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "# FCC\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "# Classifier\n",
    "model.add(Dense(units=classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Board Callback\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"second_try_%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "# Train model\n",
    "model.fit(x=train_images, y=train_labels, epochs=num_epochs, validation_data=(test_images, test_labels), callbacks=[tensorboard_callback])\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
