{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 448s 3us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([[[[ 59,  62,  63],\n",
       "           [ 43,  46,  45],\n",
       "           [ 50,  48,  43],\n",
       "           ...,\n",
       "           [158, 132, 108],\n",
       "           [152, 125, 102],\n",
       "           [148, 124, 103]],\n",
       "  \n",
       "          [[ 16,  20,  20],\n",
       "           [  0,   0,   0],\n",
       "           [ 18,   8,   0],\n",
       "           ...,\n",
       "           [123,  88,  55],\n",
       "           [119,  83,  50],\n",
       "           [122,  87,  57]],\n",
       "  \n",
       "          [[ 25,  24,  21],\n",
       "           [ 16,   7,   0],\n",
       "           [ 49,  27,   8],\n",
       "           ...,\n",
       "           [118,  84,  50],\n",
       "           [120,  84,  50],\n",
       "           [109,  73,  42]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[208, 170,  96],\n",
       "           [201, 153,  34],\n",
       "           [198, 161,  26],\n",
       "           ...,\n",
       "           [160, 133,  70],\n",
       "           [ 56,  31,   7],\n",
       "           [ 53,  34,  20]],\n",
       "  \n",
       "          [[180, 139,  96],\n",
       "           [173, 123,  42],\n",
       "           [186, 144,  30],\n",
       "           ...,\n",
       "           [184, 148,  94],\n",
       "           [ 97,  62,  34],\n",
       "           [ 83,  53,  34]],\n",
       "  \n",
       "          [[177, 144, 116],\n",
       "           [168, 129,  94],\n",
       "           [179, 142,  87],\n",
       "           ...,\n",
       "           [216, 184, 140],\n",
       "           [151, 118,  84],\n",
       "           [123,  92,  72]]],\n",
       "  \n",
       "  \n",
       "         [[[154, 177, 187],\n",
       "           [126, 137, 136],\n",
       "           [105, 104,  95],\n",
       "           ...,\n",
       "           [ 91,  95,  71],\n",
       "           [ 87,  90,  71],\n",
       "           [ 79,  81,  70]],\n",
       "  \n",
       "          [[140, 160, 169],\n",
       "           [145, 153, 154],\n",
       "           [125, 125, 118],\n",
       "           ...,\n",
       "           [ 96,  99,  78],\n",
       "           [ 77,  80,  62],\n",
       "           [ 71,  73,  61]],\n",
       "  \n",
       "          [[140, 155, 164],\n",
       "           [139, 146, 149],\n",
       "           [115, 115, 112],\n",
       "           ...,\n",
       "           [ 79,  82,  64],\n",
       "           [ 68,  70,  55],\n",
       "           [ 67,  69,  55]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[175, 167, 166],\n",
       "           [156, 154, 160],\n",
       "           [154, 160, 170],\n",
       "           ...,\n",
       "           [ 42,  34,  36],\n",
       "           [ 61,  53,  57],\n",
       "           [ 93,  83,  91]],\n",
       "  \n",
       "          [[165, 154, 128],\n",
       "           [156, 152, 130],\n",
       "           [159, 161, 142],\n",
       "           ...,\n",
       "           [103,  93,  96],\n",
       "           [123, 114, 120],\n",
       "           [131, 121, 131]],\n",
       "  \n",
       "          [[163, 148, 120],\n",
       "           [158, 148, 122],\n",
       "           [163, 156, 133],\n",
       "           ...,\n",
       "           [143, 133, 139],\n",
       "           [143, 134, 142],\n",
       "           [143, 133, 144]]],\n",
       "  \n",
       "  \n",
       "         [[[255, 255, 255],\n",
       "           [253, 253, 253],\n",
       "           [253, 253, 253],\n",
       "           ...,\n",
       "           [253, 253, 253],\n",
       "           [253, 253, 253],\n",
       "           [253, 253, 253]],\n",
       "  \n",
       "          [[255, 255, 255],\n",
       "           [255, 255, 255],\n",
       "           [255, 255, 255],\n",
       "           ...,\n",
       "           [255, 255, 255],\n",
       "           [255, 255, 255],\n",
       "           [255, 255, 255]],\n",
       "  \n",
       "          [[255, 255, 255],\n",
       "           [254, 254, 254],\n",
       "           [254, 254, 254],\n",
       "           ...,\n",
       "           [254, 254, 254],\n",
       "           [254, 254, 254],\n",
       "           [254, 254, 254]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[113, 120, 112],\n",
       "           [111, 118, 111],\n",
       "           [105, 112, 106],\n",
       "           ...,\n",
       "           [ 72,  81,  80],\n",
       "           [ 72,  80,  79],\n",
       "           [ 72,  80,  79]],\n",
       "  \n",
       "          [[111, 118, 110],\n",
       "           [104, 111, 104],\n",
       "           [ 99, 106,  98],\n",
       "           ...,\n",
       "           [ 68,  75,  73],\n",
       "           [ 70,  76,  75],\n",
       "           [ 78,  84,  82]],\n",
       "  \n",
       "          [[106, 113, 105],\n",
       "           [ 99, 106,  98],\n",
       "           [ 95, 102,  94],\n",
       "           ...,\n",
       "           [ 78,  85,  83],\n",
       "           [ 79,  85,  83],\n",
       "           [ 80,  86,  84]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[ 35, 178, 235],\n",
       "           [ 40, 176, 239],\n",
       "           [ 42, 176, 241],\n",
       "           ...,\n",
       "           [ 99, 177, 219],\n",
       "           [ 79, 147, 197],\n",
       "           [ 89, 148, 189]],\n",
       "  \n",
       "          [[ 57, 182, 234],\n",
       "           [ 44, 184, 250],\n",
       "           [ 50, 183, 240],\n",
       "           ...,\n",
       "           [156, 182, 200],\n",
       "           [141, 177, 206],\n",
       "           [116, 149, 175]],\n",
       "  \n",
       "          [[ 98, 197, 237],\n",
       "           [ 64, 189, 252],\n",
       "           [ 69, 192, 245],\n",
       "           ...,\n",
       "           [188, 195, 206],\n",
       "           [119, 135, 147],\n",
       "           [ 61,  79,  90]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 73,  79,  77],\n",
       "           [ 53,  63,  68],\n",
       "           [ 54,  68,  80],\n",
       "           ...,\n",
       "           [ 17,  40,  64],\n",
       "           [ 21,  36,  51],\n",
       "           [ 33,  48,  49]],\n",
       "  \n",
       "          [[ 61,  68,  75],\n",
       "           [ 55,  70,  86],\n",
       "           [ 57,  79, 103],\n",
       "           ...,\n",
       "           [ 24,  48,  72],\n",
       "           [ 17,  35,  53],\n",
       "           [  7,  23,  32]],\n",
       "  \n",
       "          [[ 44,  56,  73],\n",
       "           [ 46,  66,  88],\n",
       "           [ 49,  77, 105],\n",
       "           ...,\n",
       "           [ 27,  52,  77],\n",
       "           [ 21,  43,  66],\n",
       "           [ 12,  31,  50]]],\n",
       "  \n",
       "  \n",
       "         [[[189, 211, 240],\n",
       "           [186, 208, 236],\n",
       "           [185, 207, 235],\n",
       "           ...,\n",
       "           [175, 195, 224],\n",
       "           [172, 194, 222],\n",
       "           [169, 194, 220]],\n",
       "  \n",
       "          [[194, 210, 239],\n",
       "           [191, 207, 236],\n",
       "           [190, 206, 235],\n",
       "           ...,\n",
       "           [173, 192, 220],\n",
       "           [171, 191, 218],\n",
       "           [167, 190, 216]],\n",
       "  \n",
       "          [[208, 219, 244],\n",
       "           [205, 216, 240],\n",
       "           [204, 215, 239],\n",
       "           ...,\n",
       "           [175, 191, 217],\n",
       "           [172, 190, 216],\n",
       "           [169, 191, 215]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[207, 199, 181],\n",
       "           [203, 195, 175],\n",
       "           [203, 196, 173],\n",
       "           ...,\n",
       "           [135, 132, 127],\n",
       "           [162, 158, 150],\n",
       "           [168, 163, 151]],\n",
       "  \n",
       "          [[198, 190, 170],\n",
       "           [189, 181, 159],\n",
       "           [180, 172, 147],\n",
       "           ...,\n",
       "           [178, 171, 160],\n",
       "           [175, 169, 156],\n",
       "           [175, 169, 154]],\n",
       "  \n",
       "          [[198, 189, 173],\n",
       "           [189, 181, 162],\n",
       "           [178, 170, 149],\n",
       "           ...,\n",
       "           [195, 184, 169],\n",
       "           [196, 189, 171],\n",
       "           [195, 190, 171]]],\n",
       "  \n",
       "  \n",
       "         [[[229, 229, 239],\n",
       "           [236, 237, 247],\n",
       "           [234, 236, 247],\n",
       "           ...,\n",
       "           [217, 219, 233],\n",
       "           [221, 223, 234],\n",
       "           [222, 223, 233]],\n",
       "  \n",
       "          [[222, 221, 229],\n",
       "           [239, 239, 249],\n",
       "           [233, 234, 246],\n",
       "           ...,\n",
       "           [223, 223, 236],\n",
       "           [227, 228, 238],\n",
       "           [210, 211, 220]],\n",
       "  \n",
       "          [[213, 206, 211],\n",
       "           [234, 232, 239],\n",
       "           [231, 233, 244],\n",
       "           ...,\n",
       "           [220, 220, 232],\n",
       "           [220, 219, 232],\n",
       "           [202, 203, 215]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[150, 143, 135],\n",
       "           [140, 135, 127],\n",
       "           [132, 127, 120],\n",
       "           ...,\n",
       "           [224, 222, 218],\n",
       "           [230, 228, 225],\n",
       "           [241, 241, 238]],\n",
       "  \n",
       "          [[137, 132, 126],\n",
       "           [130, 127, 120],\n",
       "           [125, 121, 115],\n",
       "           ...,\n",
       "           [181, 180, 178],\n",
       "           [202, 201, 198],\n",
       "           [212, 211, 207]],\n",
       "  \n",
       "          [[122, 119, 114],\n",
       "           [118, 116, 110],\n",
       "           [120, 116, 111],\n",
       "           ...,\n",
       "           [179, 177, 173],\n",
       "           [164, 164, 162],\n",
       "           [163, 163, 161]]]], dtype=uint8),\n",
       "  array([[6],\n",
       "         [9],\n",
       "         [9],\n",
       "         ...,\n",
       "         [9],\n",
       "         [1],\n",
       "         [1]], dtype=uint8)),\n",
       " (array([[[[158, 112,  49],\n",
       "           [159, 111,  47],\n",
       "           [165, 116,  51],\n",
       "           ...,\n",
       "           [137,  95,  36],\n",
       "           [126,  91,  36],\n",
       "           [116,  85,  33]],\n",
       "  \n",
       "          [[152, 112,  51],\n",
       "           [151, 110,  40],\n",
       "           [159, 114,  45],\n",
       "           ...,\n",
       "           [136,  95,  31],\n",
       "           [125,  91,  32],\n",
       "           [119,  88,  34]],\n",
       "  \n",
       "          [[151, 110,  47],\n",
       "           [151, 109,  33],\n",
       "           [158, 111,  36],\n",
       "           ...,\n",
       "           [139,  98,  34],\n",
       "           [130,  95,  34],\n",
       "           [120,  89,  33]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 68, 124, 177],\n",
       "           [ 42, 100, 148],\n",
       "           [ 31,  88, 137],\n",
       "           ...,\n",
       "           [ 38,  97, 146],\n",
       "           [ 13,  64, 108],\n",
       "           [ 40,  85, 127]],\n",
       "  \n",
       "          [[ 61, 116, 168],\n",
       "           [ 49, 102, 148],\n",
       "           [ 35,  85, 132],\n",
       "           ...,\n",
       "           [ 26,  82, 130],\n",
       "           [ 29,  82, 126],\n",
       "           [ 20,  64, 107]],\n",
       "  \n",
       "          [[ 54, 107, 160],\n",
       "           [ 56, 105, 149],\n",
       "           [ 45,  89, 132],\n",
       "           ...,\n",
       "           [ 24,  77, 124],\n",
       "           [ 34,  84, 129],\n",
       "           [ 21,  67, 110]]],\n",
       "  \n",
       "  \n",
       "         [[[235, 235, 235],\n",
       "           [231, 231, 231],\n",
       "           [232, 232, 232],\n",
       "           ...,\n",
       "           [233, 233, 233],\n",
       "           [233, 233, 233],\n",
       "           [232, 232, 232]],\n",
       "  \n",
       "          [[238, 238, 238],\n",
       "           [235, 235, 235],\n",
       "           [235, 235, 235],\n",
       "           ...,\n",
       "           [236, 236, 236],\n",
       "           [236, 236, 236],\n",
       "           [235, 235, 235]],\n",
       "  \n",
       "          [[237, 237, 237],\n",
       "           [234, 234, 234],\n",
       "           [234, 234, 234],\n",
       "           ...,\n",
       "           [235, 235, 235],\n",
       "           [235, 235, 235],\n",
       "           [234, 234, 234]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 87,  99,  89],\n",
       "           [ 43,  51,  37],\n",
       "           [ 19,  23,  11],\n",
       "           ...,\n",
       "           [169, 184, 179],\n",
       "           [182, 197, 193],\n",
       "           [188, 202, 201]],\n",
       "  \n",
       "          [[ 82,  96,  82],\n",
       "           [ 46,  57,  36],\n",
       "           [ 36,  44,  22],\n",
       "           ...,\n",
       "           [174, 189, 183],\n",
       "           [185, 200, 196],\n",
       "           [187, 202, 200]],\n",
       "  \n",
       "          [[ 85, 101,  83],\n",
       "           [ 62,  75,  48],\n",
       "           [ 58,  67,  38],\n",
       "           ...,\n",
       "           [168, 183, 178],\n",
       "           [180, 195, 191],\n",
       "           [186, 200, 199]]],\n",
       "  \n",
       "  \n",
       "         [[[158, 190, 222],\n",
       "           [158, 187, 218],\n",
       "           [139, 166, 194],\n",
       "           ...,\n",
       "           [228, 231, 234],\n",
       "           [237, 239, 243],\n",
       "           [238, 241, 246]],\n",
       "  \n",
       "          [[170, 200, 229],\n",
       "           [172, 199, 226],\n",
       "           [151, 176, 201],\n",
       "           ...,\n",
       "           [232, 232, 236],\n",
       "           [246, 246, 250],\n",
       "           [246, 247, 251]],\n",
       "  \n",
       "          [[174, 201, 225],\n",
       "           [176, 200, 222],\n",
       "           [157, 179, 199],\n",
       "           ...,\n",
       "           [230, 229, 232],\n",
       "           [250, 249, 251],\n",
       "           [245, 244, 247]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 31,  40,  45],\n",
       "           [ 30,  39,  44],\n",
       "           [ 26,  35,  40],\n",
       "           ...,\n",
       "           [ 37,  40,  46],\n",
       "           [  9,  13,  14],\n",
       "           [  4,   7,   5]],\n",
       "  \n",
       "          [[ 23,  34,  39],\n",
       "           [ 27,  38,  43],\n",
       "           [ 25,  36,  41],\n",
       "           ...,\n",
       "           [ 19,  20,  24],\n",
       "           [  4,   6,   3],\n",
       "           [  5,   7,   3]],\n",
       "  \n",
       "          [[ 28,  41,  47],\n",
       "           [ 30,  43,  50],\n",
       "           [ 32,  45,  52],\n",
       "           ...,\n",
       "           [  5,   6,   8],\n",
       "           [  4,   5,   3],\n",
       "           [  7,   8,   7]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[ 20,  15,  12],\n",
       "           [ 19,  14,  11],\n",
       "           [ 15,  14,  11],\n",
       "           ...,\n",
       "           [ 10,   9,   7],\n",
       "           [ 12,  11,   9],\n",
       "           [ 13,  12,  10]],\n",
       "  \n",
       "          [[ 21,  16,  13],\n",
       "           [ 20,  16,  13],\n",
       "           [ 18,  17,  12],\n",
       "           ...,\n",
       "           [ 10,   9,   7],\n",
       "           [ 10,   9,   7],\n",
       "           [ 12,  11,   9]],\n",
       "  \n",
       "          [[ 21,  16,  13],\n",
       "           [ 21,  17,  12],\n",
       "           [ 20,  18,  11],\n",
       "           ...,\n",
       "           [ 12,  11,   9],\n",
       "           [ 12,  11,   9],\n",
       "           [ 13,  12,  10]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 33,  25,  13],\n",
       "           [ 34,  26,  15],\n",
       "           [ 34,  26,  15],\n",
       "           ...,\n",
       "           [ 28,  25,  52],\n",
       "           [ 29,  25,  58],\n",
       "           [ 23,  20,  42]],\n",
       "  \n",
       "          [[ 33,  25,  14],\n",
       "           [ 34,  26,  15],\n",
       "           [ 34,  26,  15],\n",
       "           ...,\n",
       "           [ 27,  24,  52],\n",
       "           [ 27,  24,  56],\n",
       "           [ 25,  22,  47]],\n",
       "  \n",
       "          [[ 31,  23,  12],\n",
       "           [ 32,  24,  13],\n",
       "           [ 33,  25,  14],\n",
       "           ...,\n",
       "           [ 24,  23,  50],\n",
       "           [ 26,  23,  53],\n",
       "           [ 25,  20,  47]]],\n",
       "  \n",
       "  \n",
       "         [[[ 25,  40,  12],\n",
       "           [ 15,  36,   3],\n",
       "           [ 23,  41,  18],\n",
       "           ...,\n",
       "           [ 61,  82,  78],\n",
       "           [ 92, 113, 112],\n",
       "           [ 75,  89,  92]],\n",
       "  \n",
       "          [[ 12,  25,   6],\n",
       "           [ 20,  37,   7],\n",
       "           [ 24,  36,  15],\n",
       "           ...,\n",
       "           [115, 134, 138],\n",
       "           [149, 168, 177],\n",
       "           [104, 117, 131]],\n",
       "  \n",
       "          [[ 12,  25,  11],\n",
       "           [ 15,  29,   6],\n",
       "           [ 34,  40,  24],\n",
       "           ...,\n",
       "           [154, 172, 182],\n",
       "           [157, 175, 192],\n",
       "           [116, 129, 151]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[100, 129,  81],\n",
       "           [103, 132,  84],\n",
       "           [104, 134,  86],\n",
       "           ...,\n",
       "           [ 97, 128,  84],\n",
       "           [ 98, 126,  84],\n",
       "           [ 91, 121,  79]],\n",
       "  \n",
       "          [[103, 132,  83],\n",
       "           [104, 131,  83],\n",
       "           [107, 135,  87],\n",
       "           ...,\n",
       "           [101, 132,  87],\n",
       "           [ 99, 127,  84],\n",
       "           [ 92, 121,  79]],\n",
       "  \n",
       "          [[ 95, 126,  78],\n",
       "           [ 95, 123,  76],\n",
       "           [101, 128,  81],\n",
       "           ...,\n",
       "           [ 93, 124,  80],\n",
       "           [ 95, 123,  81],\n",
       "           [ 92, 120,  80]]],\n",
       "  \n",
       "  \n",
       "         [[[ 73,  78,  75],\n",
       "           [ 98, 103, 113],\n",
       "           [ 99, 106, 114],\n",
       "           ...,\n",
       "           [135, 150, 152],\n",
       "           [135, 149, 154],\n",
       "           [203, 215, 223]],\n",
       "  \n",
       "          [[ 69,  73,  70],\n",
       "           [ 84,  89,  97],\n",
       "           [ 68,  75,  81],\n",
       "           ...,\n",
       "           [ 85,  95,  89],\n",
       "           [ 71,  82,  80],\n",
       "           [120, 133, 135]],\n",
       "  \n",
       "          [[ 69,  73,  70],\n",
       "           [ 90,  95, 100],\n",
       "           [ 62,  71,  74],\n",
       "           ...,\n",
       "           [ 74,  81,  70],\n",
       "           [ 53,  62,  54],\n",
       "           [ 62,  74,  69]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[123, 128,  96],\n",
       "           [132, 132, 102],\n",
       "           [129, 128, 100],\n",
       "           ...,\n",
       "           [108, 107,  88],\n",
       "           [ 62,  60,  55],\n",
       "           [ 27,  27,  28]],\n",
       "  \n",
       "          [[115, 121,  91],\n",
       "           [123, 124,  95],\n",
       "           [129, 126,  99],\n",
       "           ...,\n",
       "           [115, 116,  94],\n",
       "           [ 66,  65,  59],\n",
       "           [ 27,  27,  27]],\n",
       "  \n",
       "          [[116, 120,  90],\n",
       "           [121, 122,  94],\n",
       "           [129, 128, 101],\n",
       "           ...,\n",
       "           [116, 115,  94],\n",
       "           [ 68,  65,  58],\n",
       "           [ 27,  26,  26]]]], dtype=uint8),\n",
       "  array([[3],\n",
       "         [8],\n",
       "         [8],\n",
       "         ...,\n",
       "         [5],\n",
       "         [1],\n",
       "         [7]])))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "x_train shape: (25000, 32, 32, 3)\n",
      "25000 train samples\n",
      "5000 test samples\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "data_augmentation = False #self generate additional training data\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train_all, y_train_all), (x_test_all, y_test_all) = cifar10.load_data()\n",
    "\n",
    "print(type(x_train_all))\n",
    "\n",
    "my_labels = [0,1,2,3,4]\n",
    "\n",
    "#temp lists\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "#filter training data\n",
    "for i in range(len(x_train_all)):\n",
    "    if y_train_all[i] in my_labels:\n",
    "        x_train.append(x_train_all[i])\n",
    "        y_train.append(y_train_all[i][0])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "#filter test data\n",
    "for i in range(len(x_test_all)):\n",
    "    if y_test_all[i] in my_labels:\n",
    "        x_test.append(x_test_all[i])\n",
    "        y_test.append(y_test_all[i][0])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Train Classes and counts for each:\n",
      "[0 1 2 3 4] [5000 5000 5000 5000 5000]\n",
      "Unique Test Classes and counts for each:\n",
      "[0 1 2 3 4] [1000 1000 1000 1000 1000]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Train Classes and counts for each:\")\n",
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "print(unique, counts)\n",
    "\n",
    "print(\"Unique Test Classes and counts for each:\")\n",
    "(unique, counts) = np.unique(y_test, return_counts=True)\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def label_name(num):\n",
    "    #TODO: make list only hold \"my_labels\" index (last index small dataset not = last index full dataset)\n",
    "    return['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'][num]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "testindex = 4999\n",
    "img = x_test[testindex]\n",
    "plt.imshow(img)\n",
    "print(label_name(y_test[testindex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels to one hot vectors (for classification softmax loss)\n",
    "y_train = keras.utils.to_categorical(y_train, len(my_labels))\n",
    "y_test = keras.utils.to_categorical(y_test, len(my_labels))\n",
    "y_test[4999] #automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#values between 0 and 1 instead of 0 and 255\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#previous multidim. values => single vector 512\n",
    "#ohne flatten würde jeder filter einzeln interpretiert, so wird menge an filterergebnissen zusammengeführt\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(my_labels)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 7s 267us/sample - loss: 1.2347 - accuracy: 0.4757 - val_loss: 1.0732 - val_accuracy: 0.5448\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 6s 255us/sample - loss: 0.9905 - accuracy: 0.5944 - val_loss: 0.9072 - val_accuracy: 0.6396\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 6s 249us/sample - loss: 0.9106 - accuracy: 0.6368 - val_loss: 0.8738 - val_accuracy: 0.6602\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 6s 231us/sample - loss: 0.8564 - accuracy: 0.6602 - val_loss: 0.8070 - val_accuracy: 0.6758\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 6s 240us/sample - loss: 0.8137 - accuracy: 0.6792 - val_loss: 0.7675 - val_accuracy: 0.7048\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 6s 255us/sample - loss: 0.7720 - accuracy: 0.6975 - val_loss: 0.7125 - val_accuracy: 0.7244\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 7s 261us/sample - loss: 0.7330 - accuracy: 0.7142 - val_loss: 0.6865 - val_accuracy: 0.7314\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 6s 243us/sample - loss: 0.7015 - accuracy: 0.7306 - val_loss: 0.6614 - val_accuracy: 0.7440\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 6s 230us/sample - loss: 0.6710 - accuracy: 0.7419 - val_loss: 0.6621 - val_accuracy: 0.7392\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 5s 206us/sample - loss: 0.6464 - accuracy: 0.7537 - val_loss: 0.6588 - val_accuracy: 0.7460\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 6s 232us/sample - loss: 0.6204 - accuracy: 0.7620 - val_loss: 0.6053 - val_accuracy: 0.7736\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 6s 227us/sample - loss: 0.5994 - accuracy: 0.7706 - val_loss: 0.5817 - val_accuracy: 0.7818\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 5s 217us/sample - loss: 0.5777 - accuracy: 0.7828 - val_loss: 0.6244 - val_accuracy: 0.7676\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 6s 236us/sample - loss: 0.5590 - accuracy: 0.7880 - val_loss: 0.5844 - val_accuracy: 0.7834\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 5s 207us/sample - loss: 0.5374 - accuracy: 0.7975 - val_loss: 0.5572 - val_accuracy: 0.7932\n",
      "Epoch 16/100\n",
      "25000/25000 [==============================] - 6s 223us/sample - loss: 0.5209 - accuracy: 0.8058 - val_loss: 0.5559 - val_accuracy: 0.7928\n",
      "Epoch 17/100\n",
      "25000/25000 [==============================] - 6s 250us/sample - loss: 0.5001 - accuracy: 0.8104 - val_loss: 0.5428 - val_accuracy: 0.7968\n",
      "Epoch 18/100\n",
      "25000/25000 [==============================] - 6s 255us/sample - loss: 0.4874 - accuracy: 0.8190 - val_loss: 0.5527 - val_accuracy: 0.7928\n",
      "Epoch 19/100\n",
      "25000/25000 [==============================] - 6s 252us/sample - loss: 0.4770 - accuracy: 0.8207 - val_loss: 0.5341 - val_accuracy: 0.8042\n",
      "Epoch 20/100\n",
      "25000/25000 [==============================] - 6s 256us/sample - loss: 0.4555 - accuracy: 0.8299 - val_loss: 0.5168 - val_accuracy: 0.8090\n",
      "Epoch 21/100\n",
      "25000/25000 [==============================] - 5s 210us/sample - loss: 0.4414 - accuracy: 0.8355 - val_loss: 0.5049 - val_accuracy: 0.8120\n",
      "Epoch 22/100\n",
      "25000/25000 [==============================] - 5s 216us/sample - loss: 0.4267 - accuracy: 0.8421 - val_loss: 0.5196 - val_accuracy: 0.8074\n",
      "Epoch 23/100\n",
      "25000/25000 [==============================] - 5s 218us/sample - loss: 0.4186 - accuracy: 0.8454 - val_loss: 0.4971 - val_accuracy: 0.8170\n",
      "Epoch 24/100\n",
      "25000/25000 [==============================] - 6s 221us/sample - loss: 0.4014 - accuracy: 0.8502 - val_loss: 0.4918 - val_accuracy: 0.8188\n",
      "Epoch 25/100\n",
      "25000/25000 [==============================] - 5s 215us/sample - loss: 0.3944 - accuracy: 0.8520 - val_loss: 0.5062 - val_accuracy: 0.8168\n",
      "Epoch 26/100\n",
      "25000/25000 [==============================] - 6s 231us/sample - loss: 0.3791 - accuracy: 0.8609 - val_loss: 0.4736 - val_accuracy: 0.8260\n",
      "Epoch 27/100\n",
      "25000/25000 [==============================] - 6s 222us/sample - loss: 0.3676 - accuracy: 0.8624 - val_loss: 0.4786 - val_accuracy: 0.8266\n",
      "Epoch 28/100\n",
      "25000/25000 [==============================] - 6s 224us/sample - loss: 0.3522 - accuracy: 0.8693 - val_loss: 0.4794 - val_accuracy: 0.8274\n",
      "Epoch 29/100\n",
      "25000/25000 [==============================] - 6s 234us/sample - loss: 0.3422 - accuracy: 0.8732 - val_loss: 0.4617 - val_accuracy: 0.8350\n",
      "Epoch 30/100\n",
      "25000/25000 [==============================] - 6s 237us/sample - loss: 0.3267 - accuracy: 0.8772 - val_loss: 0.4795 - val_accuracy: 0.8288\n",
      "Epoch 31/100\n",
      "25000/25000 [==============================] - 6s 241us/sample - loss: 0.3205 - accuracy: 0.8812 - val_loss: 0.4964 - val_accuracy: 0.8238\n",
      "Epoch 32/100\n",
      "25000/25000 [==============================] - 5s 216us/sample - loss: 0.3121 - accuracy: 0.8855 - val_loss: 0.4764 - val_accuracy: 0.8310\n",
      "Epoch 33/100\n",
      "25000/25000 [==============================] - 5s 197us/sample - loss: 0.2992 - accuracy: 0.8887 - val_loss: 0.4930 - val_accuracy: 0.8292\n",
      "Epoch 34/100\n",
      "25000/25000 [==============================] - 5s 197us/sample - loss: 0.2914 - accuracy: 0.8926 - val_loss: 0.4886 - val_accuracy: 0.8366\n",
      "Epoch 35/100\n",
      "25000/25000 [==============================] - 6s 229us/sample - loss: 0.2807 - accuracy: 0.8966 - val_loss: 0.4618 - val_accuracy: 0.8406\n",
      "Epoch 36/100\n",
      "25000/25000 [==============================] - 6s 231us/sample - loss: 0.2717 - accuracy: 0.8984 - val_loss: 0.4730 - val_accuracy: 0.8356\n",
      "Epoch 37/100\n",
      "25000/25000 [==============================] - 5s 198us/sample - loss: 0.2633 - accuracy: 0.9036 - val_loss: 0.4889 - val_accuracy: 0.8334\n",
      "Epoch 38/100\n",
      "25000/25000 [==============================] - 5s 194us/sample - loss: 0.2549 - accuracy: 0.9049 - val_loss: 0.4677 - val_accuracy: 0.8378\n",
      "Epoch 39/100\n",
      "25000/25000 [==============================] - 5s 195us/sample - loss: 0.2459 - accuracy: 0.9082 - val_loss: 0.4952 - val_accuracy: 0.8348\n",
      "Epoch 40/100\n",
      "25000/25000 [==============================] - 5s 195us/sample - loss: 0.2384 - accuracy: 0.9113 - val_loss: 0.4752 - val_accuracy: 0.8370\n",
      "Epoch 41/100\n",
      "25000/25000 [==============================] - 5s 194us/sample - loss: 0.2297 - accuracy: 0.9154 - val_loss: 0.4915 - val_accuracy: 0.8376\n",
      "Epoch 42/100\n",
      "25000/25000 [==============================] - 5s 196us/sample - loss: 0.2210 - accuracy: 0.9195 - val_loss: 0.4850 - val_accuracy: 0.8382\n",
      "Epoch 43/100\n",
      "25000/25000 [==============================] - 5s 203us/sample - loss: 0.2119 - accuracy: 0.9218 - val_loss: 0.4769 - val_accuracy: 0.8454\n",
      "Epoch 44/100\n",
      "25000/25000 [==============================] - 5s 212us/sample - loss: 0.2070 - accuracy: 0.9226 - val_loss: 0.4936 - val_accuracy: 0.8388\n",
      "Epoch 45/100\n",
      "25000/25000 [==============================] - 5s 195us/sample - loss: 0.2039 - accuracy: 0.9240 - val_loss: 0.4824 - val_accuracy: 0.8412\n",
      "Epoch 46/100\n",
      "25000/25000 [==============================] - 5s 195us/sample - loss: 0.1926 - accuracy: 0.9287 - val_loss: 0.4814 - val_accuracy: 0.8444\n",
      "Epoch 47/100\n",
      "25000/25000 [==============================] - 6s 251us/sample - loss: 0.1837 - accuracy: 0.9334 - val_loss: 0.5005 - val_accuracy: 0.8386\n",
      "Epoch 48/100\n",
      "25000/25000 [==============================] - 5s 196us/sample - loss: 0.1828 - accuracy: 0.9340 - val_loss: 0.4925 - val_accuracy: 0.8458\n",
      "Epoch 49/100\n",
      "25000/25000 [==============================] - 5s 202us/sample - loss: 0.1768 - accuracy: 0.9349 - val_loss: 0.5135 - val_accuracy: 0.8376\n",
      "Epoch 50/100\n",
      "25000/25000 [==============================] - 5s 199us/sample - loss: 0.1685 - accuracy: 0.9394 - val_loss: 0.4849 - val_accuracy: 0.8478\n",
      "Epoch 51/100\n",
      "25000/25000 [==============================] - 5s 195us/sample - loss: 0.1658 - accuracy: 0.9396 - val_loss: 0.4962 - val_accuracy: 0.8462\n",
      "Epoch 52/100\n",
      "25000/25000 [==============================] - 5s 207us/sample - loss: 0.1552 - accuracy: 0.9434 - val_loss: 0.5357 - val_accuracy: 0.8418\n",
      "Epoch 53/100\n",
      "25000/25000 [==============================] - 6s 220us/sample - loss: 0.1565 - accuracy: 0.9438 - val_loss: 0.5447 - val_accuracy: 0.8348\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 5s 217us/sample - loss: 0.1475 - accuracy: 0.9471 - val_loss: 0.5273 - val_accuracy: 0.8378\n",
      "Epoch 55/100\n",
      "25000/25000 [==============================] - 6s 228us/sample - loss: 0.1454 - accuracy: 0.9498 - val_loss: 0.5166 - val_accuracy: 0.8486\n",
      "Epoch 56/100\n",
      "25000/25000 [==============================] - 6s 256us/sample - loss: 0.1418 - accuracy: 0.9479 - val_loss: 0.5236 - val_accuracy: 0.8434\n",
      "Epoch 57/100\n",
      "25000/25000 [==============================] - 5s 204us/sample - loss: 0.1388 - accuracy: 0.9499 - val_loss: 0.5239 - val_accuracy: 0.8444\n",
      "Epoch 58/100\n",
      "25000/25000 [==============================] - 5s 196us/sample - loss: 0.1337 - accuracy: 0.9524 - val_loss: 0.5100 - val_accuracy: 0.8518\n",
      "Epoch 59/100\n",
      "25000/25000 [==============================] - 5s 195us/sample - loss: 0.1337 - accuracy: 0.9524 - val_loss: 0.5223 - val_accuracy: 0.8444\n",
      "Epoch 60/100\n",
      "25000/25000 [==============================] - 5s 202us/sample - loss: 0.1266 - accuracy: 0.9532 - val_loss: 0.5251 - val_accuracy: 0.8492\n",
      "Epoch 61/100\n",
      "25000/25000 [==============================] - 6s 241us/sample - loss: 0.1198 - accuracy: 0.9567 - val_loss: 0.5277 - val_accuracy: 0.8464\n",
      "Epoch 62/100\n",
      "25000/25000 [==============================] - 5s 210us/sample - loss: 0.1258 - accuracy: 0.9555 - val_loss: 0.5165 - val_accuracy: 0.8432\n",
      "Epoch 63/100\n",
      "25000/25000 [==============================] - 5s 209us/sample - loss: 0.1109 - accuracy: 0.9592 - val_loss: 0.5573 - val_accuracy: 0.8436\n",
      "Epoch 64/100\n",
      "25000/25000 [==============================] - 5s 216us/sample - loss: 0.1146 - accuracy: 0.9587 - val_loss: 0.5560 - val_accuracy: 0.8392\n",
      "Epoch 65/100\n",
      "25000/25000 [==============================] - 5s 197us/sample - loss: 0.1083 - accuracy: 0.9622 - val_loss: 0.5404 - val_accuracy: 0.8488\n",
      "Epoch 66/100\n",
      "25000/25000 [==============================] - 5s 196us/sample - loss: 0.1088 - accuracy: 0.9602 - val_loss: 0.5416 - val_accuracy: 0.8480\n",
      "Epoch 67/100\n",
      "25000/25000 [==============================] - 5s 206us/sample - loss: 0.1041 - accuracy: 0.9646 - val_loss: 0.5579 - val_accuracy: 0.8474\n",
      "Epoch 68/100\n",
      "25000/25000 [==============================] - 5s 207us/sample - loss: 0.1051 - accuracy: 0.9631 - val_loss: 0.5811 - val_accuracy: 0.8384\n",
      "Epoch 69/100\n",
      "25000/25000 [==============================] - 5s 196us/sample - loss: 0.1011 - accuracy: 0.9636 - val_loss: 0.5574 - val_accuracy: 0.8464\n",
      "Epoch 70/100\n",
      "25000/25000 [==============================] - 5s 211us/sample - loss: 0.1039 - accuracy: 0.9638 - val_loss: 0.5742 - val_accuracy: 0.8466\n",
      "Epoch 71/100\n",
      "25000/25000 [==============================] - 6s 228us/sample - loss: 0.0962 - accuracy: 0.9650 - val_loss: 0.5657 - val_accuracy: 0.8456\n",
      "Epoch 72/100\n",
      "25000/25000 [==============================] - 5s 208us/sample - loss: 0.0950 - accuracy: 0.9645 - val_loss: 0.5513 - val_accuracy: 0.8440\n",
      "Epoch 73/100\n",
      "25000/25000 [==============================] - 5s 210us/sample - loss: 0.0960 - accuracy: 0.9658 - val_loss: 0.5746 - val_accuracy: 0.8438\n",
      "Epoch 74/100\n",
      "25000/25000 [==============================] - 5s 206us/sample - loss: 0.0891 - accuracy: 0.9690 - val_loss: 0.5835 - val_accuracy: 0.8478\n",
      "Epoch 75/100\n",
      "25000/25000 [==============================] - 5s 212us/sample - loss: 0.0844 - accuracy: 0.9699 - val_loss: 0.5785 - val_accuracy: 0.8512\n",
      "Epoch 76/100\n",
      "25000/25000 [==============================] - 6s 221us/sample - loss: 0.0846 - accuracy: 0.9703 - val_loss: 0.5774 - val_accuracy: 0.8526\n",
      "Epoch 77/100\n",
      "25000/25000 [==============================] - 5s 217us/sample - loss: 0.0855 - accuracy: 0.9696 - val_loss: 0.5708 - val_accuracy: 0.8512\n",
      "Epoch 78/100\n",
      "25000/25000 [==============================] - 6s 237us/sample - loss: 0.0854 - accuracy: 0.9690 - val_loss: 0.5708 - val_accuracy: 0.8514\n",
      "Epoch 79/100\n",
      "25000/25000 [==============================] - 7s 269us/sample - loss: 0.0838 - accuracy: 0.9711 - val_loss: 0.5841 - val_accuracy: 0.8506\n",
      "Epoch 80/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0833 - accuracy: 0.9689 - val_loss: 0.6302 - val_accuracy: 0.8442\n",
      "Epoch 81/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0828 - accuracy: 0.9713 - val_loss: 0.5815 - val_accuracy: 0.8482\n",
      "Epoch 82/100\n",
      "25000/25000 [==============================] - 5s 194us/sample - loss: 0.0797 - accuracy: 0.9721 - val_loss: 0.5941 - val_accuracy: 0.8480\n",
      "Epoch 83/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0809 - accuracy: 0.9710 - val_loss: 0.6342 - val_accuracy: 0.8368\n",
      "Epoch 84/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0743 - accuracy: 0.9733 - val_loss: 0.6049 - val_accuracy: 0.8496\n",
      "Epoch 85/100\n",
      "25000/25000 [==============================] - 5s 186us/sample - loss: 0.0749 - accuracy: 0.9729 - val_loss: 0.5916 - val_accuracy: 0.8538\n",
      "Epoch 86/100\n",
      "25000/25000 [==============================] - 5s 194us/sample - loss: 0.0710 - accuracy: 0.9753 - val_loss: 0.5832 - val_accuracy: 0.8448\n",
      "Epoch 87/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0735 - accuracy: 0.9742 - val_loss: 0.6139 - val_accuracy: 0.8454\n",
      "Epoch 88/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0688 - accuracy: 0.9761 - val_loss: 0.6242 - val_accuracy: 0.8492\n",
      "Epoch 89/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0699 - accuracy: 0.9752 - val_loss: 0.6155 - val_accuracy: 0.8508\n",
      "Epoch 90/100\n",
      "25000/25000 [==============================] - 5s 184us/sample - loss: 0.0689 - accuracy: 0.9753 - val_loss: 0.6467 - val_accuracy: 0.8452\n",
      "Epoch 91/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0673 - accuracy: 0.9773 - val_loss: 0.6572 - val_accuracy: 0.8496\n",
      "Epoch 92/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0684 - accuracy: 0.9768 - val_loss: 0.6406 - val_accuracy: 0.8508\n",
      "Epoch 93/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0685 - accuracy: 0.9756 - val_loss: 0.6062 - val_accuracy: 0.8514\n",
      "Epoch 94/100\n",
      "25000/25000 [==============================] - 5s 185us/sample - loss: 0.0620 - accuracy: 0.9781 - val_loss: 0.6055 - val_accuracy: 0.8514\n",
      "Epoch 95/100\n",
      "25000/25000 [==============================] - 5s 186us/sample - loss: 0.0670 - accuracy: 0.9761 - val_loss: 0.6188 - val_accuracy: 0.8512\n",
      "Epoch 96/100\n",
      "25000/25000 [==============================] - 5s 186us/sample - loss: 0.0631 - accuracy: 0.9788 - val_loss: 0.6312 - val_accuracy: 0.8468\n",
      "Epoch 97/100\n",
      "25000/25000 [==============================] - 5s 184us/sample - loss: 0.0584 - accuracy: 0.9799 - val_loss: 0.6125 - val_accuracy: 0.8500\n",
      "Epoch 98/100\n",
      "25000/25000 [==============================] - 5s 186us/sample - loss: 0.0595 - accuracy: 0.9802 - val_loss: 0.6654 - val_accuracy: 0.8472\n",
      "Epoch 99/100\n",
      "25000/25000 [==============================] - 5s 186us/sample - loss: 0.0616 - accuracy: 0.9778 - val_loss: 0.6469 - val_accuracy: 0.8454\n",
      "Epoch 100/100\n",
      "25000/25000 [==============================] - 5s 186us/sample - loss: 0.0609 - accuracy: 0.9788 - val_loss: 0.6531 - val_accuracy: 0.8470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f787f41b48>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "# open tb: tensorboard --logdir logs/\n",
    "logdir = os.path.join(os.getcwd(), 'logs\\\\'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
