{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLOSS Factor:\n",
    "\n",
    "R_LOSS = Input Image vs Output Image\n",
    "KL_LOSS = Distribution concluded from Input Image vs. Standard Distribution\n",
    "\n",
    "LOSS = R_LOSS_FACTOR * R_LOSS + KL_LOSS\n",
    "\n",
    "Hence, \n",
    "Low R_LOSS_FACTOR -> High KL_LOSS weighing = output images try to be similar to all training images (blurry)\n",
    "High R_LOSS_FACTOR -> Low KL_LOSS weighing = output images are similar to input images and have less variation leeway\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#must be very first statement\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#utils'\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.datasets import mnist\n",
    "import PIL\n",
    "\n",
    "\n",
    "def getDigits(show=False):\n",
    "    (x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "    #print(x_train.shape)\n",
    "\n",
    "    if show:\n",
    "        showImages(x_train, t_train,5)\n",
    "\n",
    "    return x_train,t_train,x_test, t_test\n",
    "\n",
    "\n",
    "#### CALLBACKS (https://github.com/davidADSP/GDL_code/blob/master/utils/callbacks.py)\n",
    "class CustomCallback(Callback):\n",
    "\n",
    "    def __init__(self, run_folder, print_every_n_batches, initial_epoch, vae):\n",
    "        self.epoch = initial_epoch\n",
    "        self.run_folder = run_folder\n",
    "        self.print_every_n_batches = print_every_n_batches\n",
    "        self.vae = vae\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if batch % self.print_every_n_batches == 0:\n",
    "            z_new = np.random.normal(size=(1, self.vae.z_dim))\n",
    "            reconst = self.vae.decoder.predict(np.array(z_new))[0].squeeze()\n",
    "\n",
    "            filepath = os.path.join(self.run_folder, 'images',\n",
    "                                    'img_' + str(self.epoch).zfill(3) + '_' + str(batch) + '.jpg')\n",
    "            if len(reconst.shape) == 2:\n",
    "                plt.imsave(filepath, reconst, cmap='gray_r')\n",
    "            else:\n",
    "                plt.imsave(filepath, reconst)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch += 1\n",
    "\n",
    "#### CALLBACKS (https://github.com/davidADSP/GDL_code/blob/master/utils/callbacks.py)\n",
    "def step_decay_schedule(initial_lr, decay_factor=0.5, step_size=1):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "\n",
    "    def schedule(epoch):\n",
    "        new_lr = initial_lr * (decay_factor ** np.floor(epoch / step_size))\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "    return LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining new Labeling:\n",
      "{0: 'automobile'}\n",
      "Training Data:\n",
      "\n",
      "x_train shape: (5000, 32, 32, 3)\n",
      "5000 samples, 5000 labels\n",
      "\n",
      "Class  |  Counts:\n",
      "automobile \t 5000\n",
      "\n",
      "\n",
      "Testing Data:\n",
      "\n",
      "x_test shape: (1000, 32, 32, 3)\n",
      "1000 samples, 1000 labels\n",
      "\n",
      "Class  |  Counts:\n",
      "automobile \t 1000\n"
     ]
    }
   ],
   "source": [
    "#data prep\n",
    "\n",
    "# changes:\n",
    "#\n",
    "# - keras load cifar 10 inst. getDigits (MNIST) from utils\n",
    "#\n",
    "# - allow selection of training subset based on label\n",
    "# - for softmax classification new label order: 2,3,8 -> 0,1,2\n",
    "#\n",
    "# - reshape:\n",
    "#   x_train=x_train.reshape(x_train.shape[0],32,32,3)\n",
    "#   x_test=x_test.reshape(x_test.shape[0],32,32,3)\n",
    "#   (instead of 28,28,1)\n",
    "#\n",
    "# Aktivierung:\n",
    "# My data was not best approximated by a Bernoulli but a Gaussian. \n",
    "# So, using a GaussianReconstructionDistribution with a TANH gave better results.\n",
    "# The DL4J JavaDocs state: \n",
    "# \"For activation functions, identity and perhaps tanh are typical - though tanh (unlike identity) \n",
    "# implies a minimum/maximum possible value for mean and log variance. Asymmetric activation functions \n",
    "# such as sigmoid or relu should be avoided\". \n",
    "\n",
    "from keras.datasets import cifar10\n",
    "cifar10.load_data()\n",
    "\n",
    "my_labels = [1]\n",
    "all_label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "label_names = []\n",
    "for label_index in my_labels:\n",
    "    label_names.append(all_label_names[label_index])  \n",
    "\n",
    "print(\"Defining new Labeling:\")\n",
    "print(dict(zip(range(len(my_labels)),label_names)))\n",
    "\n",
    "#if my_labels = [5,6,8] then 5 returns 0, 6 returns 1, 8 returns 2, ...\n",
    "def convert_label(label):\n",
    "    return dict(zip(my_labels,range(len(my_labels))))[label]\n",
    "\n",
    "def label_name(num):\n",
    "    return label_names[num]\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train_all, y_train_all), (x_test_all, y_test_all) = cifar10.load_data()\n",
    "    \n",
    "#temp lists\n",
    "x_train = []\n",
    "y_train_numerical = []\n",
    "\n",
    "#filter training data for my_labels\n",
    "for i in range(len(x_train_all)):\n",
    "    if y_train_all[i] in my_labels:\n",
    "        x_train.append(x_train_all[i])\n",
    "        y_train_numerical.append(convert_label(y_train_all[i][0]))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train_numerical = np.array(y_train_numerical)\n",
    "\n",
    "print(\"Training Data:\\n\")\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'samples,', y_train_numerical.shape[0], 'labels')\n",
    "print(\"\\nClass  |  Counts:\")\n",
    "(unique, counts) = np.unique(y_train_numerical, return_counts=True)\n",
    "for i, label in enumerate(unique):\n",
    "    print(label_name(label),\"\\t\", counts[i])\n",
    "\n",
    "\n",
    "x_test = []\n",
    "y_test_numerical = []\n",
    "\n",
    "#filter test data\n",
    "for i in range(len(x_test_all)):\n",
    "    if y_test_all[i] in my_labels:\n",
    "        x_test.append(x_test_all[i])\n",
    "        y_test_numerical.append(convert_label(y_test_all[i][0]))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test_numerical = np.array(y_test_numerical)\n",
    "\n",
    "print(\"\\n\\nTesting Data:\\n\")\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_test.shape[0], 'samples,', y_test_numerical.shape[0], 'labels')\n",
    "print(\"\\nClass  |  Counts:\")\n",
    "(unique, counts) = np.unique(y_test_numerical, return_counts=True)\n",
    "for i, label in enumerate(unique):\n",
    "    print(label_name(label),\"\\t\", counts[i])\n",
    "    \n",
    "x_train=x_train.reshape(x_train.shape[0],32,32,3)\n",
    "x_test=x_test.reshape(x_test.shape[0],32,32,3)\n",
    "\n",
    "x_train = x_train.astype('float32')[:500]\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model class\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras import callbacks\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Verteilung + KL:\n",
    "# https://stats.stackexchange.com/questions/402569/why-do-we-use-gaussian-distributions-in-variational-autoencoder\n",
    "#\n",
    "# TODO:\n",
    "# sigmoid vs tanh in decoder activation\n",
    "# batchnormalization: ON\n",
    "# (multivariate distribution instead of gaussian normal)\n",
    "\n",
    "class VariationalAutoencoder():\n",
    "    def __init__(self\n",
    "                 , input_dim\n",
    "                 , encoder_conv_filters\n",
    "                 , encoder_conv_kernel_size\n",
    "                 , encoder_conv_strides\n",
    "                 , decoder_conv_t_filters\n",
    "                 , decoder_conv_t_kernel_size\n",
    "                 , decoder_conv_t_strides\n",
    "                 , z_dim\n",
    "                 , use_batch_norm=False\n",
    "                 , use_dropout=False\n",
    "                 ):\n",
    "\n",
    "        self.name = 'variational_autoencoder'\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder_conv_filters = encoder_conv_filters\n",
    "        self.encoder_conv_kernel_size = encoder_conv_kernel_size\n",
    "        self.encoder_conv_strides = encoder_conv_strides\n",
    "        self.decoder_conv_t_filters = decoder_conv_t_filters\n",
    "        self.decoder_conv_t_kernel_size = decoder_conv_t_kernel_size\n",
    "        self.decoder_conv_t_strides = decoder_conv_t_strides\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        self.n_layers_encoder = len(encoder_conv_filters)\n",
    "        self.n_layers_decoder = len(decoder_conv_t_filters)\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        ### THE ENCODER\n",
    "        encoder_input = Input(shape=self.input_dim, name='encoder_input')\n",
    "\n",
    "        x = encoder_input\n",
    "\n",
    "        for i in range(self.n_layers_encoder):\n",
    "            conv_layer = Conv2D(\n",
    "                filters=self.encoder_conv_filters[i]\n",
    "                , kernel_size=self.encoder_conv_kernel_size[i]\n",
    "                , strides=self.encoder_conv_strides[i]\n",
    "                , padding='same'\n",
    "                , name='encoder_conv_' + str(i)\n",
    "            )\n",
    "\n",
    "            x = conv_layer(x)\n",
    "\n",
    "            if self.use_batch_norm:\n",
    "                x = BatchNormalization()(x)\n",
    "\n",
    "            x = LeakyReLU()(x) # for encoding\n",
    "\n",
    "            if self.use_dropout:\n",
    "                x = Dropout(rate=0.25)(x)\n",
    "\n",
    "        shape_before_flattening = K.int_shape(x)[1:]\n",
    "#-----------------------------\n",
    "        print(\"shape_bef_flat\",shape_before_flattening)\n",
    "        x = Flatten()(x)\n",
    "        print(\"shape_aft_flat\",x)\n",
    "        self.mu = Dense(self.z_dim, name='mu')(x)\n",
    "        self.log_var = Dense(self.z_dim, name='log_var')(x)\n",
    "\n",
    "        self.encoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\n",
    "\n",
    "        def sampling(args):\n",
    "            mu, log_var = args\n",
    "            epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1) #gaussian standardnormalvert.\n",
    "            return mu + K.exp(log_var / 2) * epsilon #try without /2\n",
    "    \n",
    "        encoder_output = Lambda(sampling, name='encoder_output')([self.mu, self.log_var])\n",
    "        print(\"encoder_output: \",encoder_output)\n",
    "        self.encoder = Model(encoder_input, encoder_output)\n",
    "\n",
    "        ### THE DECODER\n",
    "\n",
    "        decoder_input = Input(shape=(self.z_dim,), name='decoder_input')\n",
    "        print(\"dec_input\",decoder_input)\n",
    "        x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "        x = Reshape(shape_before_flattening)(x)\n",
    "\n",
    "        for i in range(self.n_layers_decoder):\n",
    "            conv_t_layer = Conv2DTranspose(\n",
    "                filters=self.decoder_conv_t_filters[i]\n",
    "                , kernel_size=self.decoder_conv_t_kernel_size[i]\n",
    "                , strides=self.decoder_conv_t_strides[i]\n",
    "                , padding='same'\n",
    "                , name='decoder_conv_t_' + str(i)\n",
    "            )\n",
    "\n",
    "            x = conv_t_layer(x)\n",
    "\n",
    "            if i < self.n_layers_decoder - 1:\n",
    "                if self.use_batch_norm:\n",
    "                    x = BatchNormalization()(x)\n",
    "                x = LeakyReLU()(x)\n",
    "                if self.use_dropout:\n",
    "                    x = Dropout(rate=0.25)(x)\n",
    "            else:\n",
    "                x = Activation('sigmoid')(x)\n",
    "\n",
    "        decoder_output = x\n",
    "        self.decoder = Model(decoder_input, decoder_output)\n",
    "\n",
    "        ### THE FULL VAE\n",
    "        model_input = encoder_input\n",
    "        model_output = self.decoder(encoder_output)\n",
    "        \n",
    "        self.model = Model(model_input, model_output)\n",
    "\n",
    "    def compile(self, learning_rate, r_loss_factor):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        ### COMPILATION\n",
    "        def vae_r_loss(y_true, y_pred):\n",
    "            r_loss = K.mean(K.square(y_true - y_pred), axis=[1, 2, 3])\n",
    "            return r_loss * r_loss_factor\n",
    "\n",
    "        def vae_kl_loss(y_true, y_pred):\n",
    "            kl_loss = -0.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis=1)\n",
    "            return kl_loss\n",
    "\n",
    "        def vae_loss(y_true, y_pred):\n",
    "            r_loss = vae_r_loss(y_true, y_pred)\n",
    "            kl_loss = vae_kl_loss(y_true, y_pred)\n",
    "            return r_loss + kl_loss\n",
    "\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss=vae_loss, metrics=[vae_r_loss, vae_kl_loss])\n",
    "\n",
    "    def save(self, folder=\"run\"):\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            os.makedirs(os.path.join(folder, 'viz'))\n",
    "            os.makedirs(os.path.join(folder, 'weights'))\n",
    "            os.makedirs(os.path.join(folder, 'images'))\n",
    "\n",
    "        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n",
    "            pickle.dump([\n",
    "                self.input_dim\n",
    "                , self.encoder_conv_filters\n",
    "                , self.encoder_conv_kernel_size\n",
    "                , self.encoder_conv_strides\n",
    "                , self.decoder_conv_t_filters\n",
    "                , self.decoder_conv_t_kernel_size\n",
    "                , self.decoder_conv_t_strides\n",
    "                , self.z_dim\n",
    "                , self.use_batch_norm\n",
    "                , self.use_dropout\n",
    "            ], f)\n",
    "\n",
    "        self.plot_model(folder)\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "\n",
    "    def train(self, x_train, batch_size, epochs, run_folder, print_every_n_batches=100, initial_epoch=0, lr_decay=1):\n",
    "\n",
    "        custom_callback = CustomCallback(run_folder, print_every_n_batches, initial_epoch, self)\n",
    "        lr_sched = step_decay_schedule(initial_lr=self.learning_rate, decay_factor=lr_decay, step_size=1)\n",
    "\n",
    "        checkpoint_filepath = os.path.join(run_folder, \"weights/weights.h5\")\n",
    "        \n",
    "        model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_acc',\n",
    "            mode='max')\n",
    "\n",
    "        \n",
    "        #checkpoint1 = ModelCheckpoint(checkpoint_filepath, save_weights_only=True, verbose=1)\n",
    "        #checkpoint2 = ModelCheckpoint(os.path.join(run_folder, 'weights/weights.h5'), save_weights_only=True, verbose=1)\n",
    "\n",
    "        callbacks_list = [custom_callback, lr_sched, model_checkpoint_callback]\n",
    "\n",
    "        self.model.fit(\n",
    "            x_train\n",
    "            , x_train\n",
    "            , batch_size=batch_size\n",
    "            , shuffle=True\n",
    "            , epochs=epochs\n",
    "            , initial_epoch=initial_epoch\n",
    "            , callbacks=callbacks_list\n",
    "        )\n",
    "\n",
    "    def plot_model(self, run_folder):\n",
    "        plot_model(self.model, to_file=os.path.join(run_folder, 'viz/model.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "        plot_model(self.encoder, to_file=os.path.join(run_folder, 'viz/encoder.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "        plot_model(self.decoder, to_file=os.path.join(run_folder, 'viz/decoder.png'), show_shapes=True,\n",
    "                   show_layer_names=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:\\Hub\\ML_SS_2020\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "SECTION = 'vae'\n",
    "RUN_ID = '0002'\n",
    "DATA_NAME = 'digits'\n",
    "RUN_FOLDER = 'run_losscompare'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# größeres modell\n",
    "#Wichtig: Beim Output Filteranzahl = 3 -> Dreidimensionales Ergebnis für RGB Interpretation\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [128,128,128,512]\n",
    "    , encoder_conv_kernel_size = [2,2,3,4]\n",
    "    , encoder_conv_strides = [1,1,2,2]\n",
    "    , decoder_conv_t_filters = [512,128,128,3]\n",
    "    , decoder_conv_t_kernel_size = [4,3,2,2]\n",
    "    , decoder_conv_t_strides = [2,2,1,1]\n",
    "    , z_dim = 128\n",
    ")\n",
    "vae.save(RUN_FOLDER)\n",
    "\n",
    "vae.encoder.summary()\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "R_LOSS_FACTOR = 100\n",
    "vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "vae.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")\n",
    "vae.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "shape_bef_flat (8, 8, 512)\n",
      "shape_aft_flat Tensor(\"flatten_1/Reshape:0\", shape=(None, None), dtype=float32)\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "encoder_output:  Tensor(\"encoder_output/add:0\", shape=(None, 128), dtype=float32)\n",
      "dec_input Tensor(\"decoder_input:0\", shape=(None, 128), dtype=float32)\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)         (None, 32, 32, 128)  1664        encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 128)  0           encoder_conv_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)         (None, 32, 32, 128)  65664       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 128)  0           encoder_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)         (None, 16, 16, 128)  147584      leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 16, 16, 128)  0           encoder_conv_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)         (None, 8, 8, 512)    1049088     leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 8, 8, 512)    0           encoder_conv_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32768)        0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mu (Dense)                      (None, 128)          4194432     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "log_var (Dense)                 (None, 128)          4194432     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_output (Lambda)         (None, 128)          0           mu[0][0]                         \n",
      "                                                                 log_var[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,652,864\n",
      "Trainable params: 9,652,864\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Epoch 1/1000\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_keras_scratch_graph_3141 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      " 32/500 [>.............................] - ETA: 2:23 - loss: 344.6944 - vae_r_loss: 344.6060 - vae_kl_loss: 0.0885Executing op __inference_keras_scratch_graph_3278 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "128/500 [======>.......................] - ETA: 32s - loss: 371.7214 - vae_r_loss: 371.6729 - vae_kl_loss: 0.0485"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\ml20\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.600560). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 12s 25ms/step - loss: 376.1353 - vae_r_loss: 376.1075 - vae_kl_loss: 0.0278\n",
      "Epoch 2/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 367.4291 - vae_r_loss: 366.1761 - vae_kl_loss: 1.2530\n",
      "Epoch 3/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 323.3033 - vae_r_loss: 313.1949 - vae_kl_loss: 10.1085\n",
      "Epoch 4/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 280.7666 - vae_r_loss: 267.4770 - vae_kl_loss: 13.2896\n",
      "Epoch 5/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 252.8971 - vae_r_loss: 235.8314 - vae_kl_loss: 17.0657\n",
      "Epoch 6/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 231.4185 - vae_r_loss: 213.9075 - vae_kl_loss: 17.5110\n",
      "Epoch 7/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 216.2904 - vae_r_loss: 193.5601 - vae_kl_loss: 22.7303\n",
      "Epoch 8/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 207.6018 - vae_r_loss: 184.9980 - vae_kl_loss: 22.6038A: 0s - loss: 208.3639 - vae_r_loss: 186.1063 - vae_kl_loss: 22.\n",
      "Epoch 9/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 200.9948 - vae_r_loss: 176.6549 - vae_kl_loss: 24.3399\n",
      "Epoch 10/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 197.3803 - vae_r_loss: 171.3670 - vae_kl_loss: 26.0133\n",
      "Epoch 11/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 193.9035 - vae_r_loss: 168.0475 - vae_kl_loss: 25.8560\n",
      "Epoch 12/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 191.1504 - vae_r_loss: 164.0535 - vae_kl_loss: 27.0969\n",
      "Epoch 13/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 186.7725 - vae_r_loss: 159.1732 - vae_kl_loss: 27.5994\n",
      "Epoch 14/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 186.0350 - vae_r_loss: 157.1033 - vae_kl_loss: 28.9317\n",
      "Epoch 15/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 183.8717 - vae_r_loss: 154.9788 - vae_kl_loss: 28.8929\n",
      "Epoch 16/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 183.1625 - vae_r_loss: 153.9781 - vae_kl_loss: 29.1845\n",
      "Epoch 17/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 182.6105 - vae_r_loss: 153.1824 - vae_kl_loss: 29.4281\n",
      "Epoch 18/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 179.2103 - vae_r_loss: 148.8064 - vae_kl_loss: 30.4039\n",
      "Epoch 19/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 178.8692 - vae_r_loss: 149.6077 - vae_kl_loss: 29.2615\n",
      "Epoch 20/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 178.7834 - vae_r_loss: 147.0841 - vae_kl_loss: 31.6992\n",
      "Epoch 21/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 177.6611 - vae_r_loss: 147.3473 - vae_kl_loss: 30.3138\n",
      "Epoch 22/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 176.8768 - vae_r_loss: 145.4359 - vae_kl_loss: 31.4408\n",
      "Epoch 23/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 176.6308 - vae_r_loss: 144.9869 - vae_kl_loss: 31.6439\n",
      "Epoch 24/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 174.0979 - vae_r_loss: 142.5996 - vae_kl_loss: 31.4983\n",
      "Epoch 25/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 172.8989 - vae_r_loss: 139.5488 - vae_kl_loss: 33.3501\n",
      "Epoch 26/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 171.5165 - vae_r_loss: 138.8336 - vae_kl_loss: 32.6829A: 0s - loss: 171.4738 - vae_r_loss: 140.0750 - vae_kl_loss\n",
      "Epoch 27/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 169.1579 - vae_r_loss: 136.0804 - vae_kl_loss: 33.0775\n",
      "Epoch 28/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 164.7577 - vae_r_loss: 132.2054 - vae_kl_loss: 32.5523\n",
      "Epoch 29/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 165.3718 - vae_r_loss: 131.7011 - vae_kl_loss: 33.6707\n",
      "Epoch 30/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 163.4105 - vae_r_loss: 128.8924 - vae_kl_loss: 34.5182\n",
      "Epoch 31/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 162.7626 - vae_r_loss: 127.8794 - vae_kl_loss: 34.8832\n",
      "Epoch 32/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 160.3879 - vae_r_loss: 125.5437 - vae_kl_loss: 34.8441\n",
      "Epoch 33/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 159.6192 - vae_r_loss: 125.1050 - vae_kl_loss: 34.5143\n",
      "Epoch 34/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 159.2602 - vae_r_loss: 124.5830 - vae_kl_loss: 34.6773\n",
      "Epoch 35/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 157.1375 - vae_r_loss: 122.4716 - vae_kl_loss: 34.6659\n",
      "Epoch 36/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 156.7711 - vae_r_loss: 121.9607 - vae_kl_loss: 34.8104\n",
      "Epoch 37/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 155.1289 - vae_r_loss: 120.0570 - vae_kl_loss: 35.0720\n",
      "Epoch 38/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 153.4873 - vae_r_loss: 118.7509 - vae_kl_loss: 34.7364\n",
      "Epoch 39/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 152.9337 - vae_r_loss: 118.0542 - vae_kl_loss: 34.8795\n",
      "Epoch 40/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 151.6400 - vae_r_loss: 116.4156 - vae_kl_loss: 35.2244\n",
      "Epoch 41/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 149.9220 - vae_r_loss: 114.8928 - vae_kl_loss: 35.0291\n",
      "Epoch 42/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 149.5219 - vae_r_loss: 114.7370 - vae_kl_loss: 34.7849\n",
      "Epoch 43/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 147.6916 - vae_r_loss: 112.3638 - vae_kl_loss: 35.3278\n",
      "Epoch 44/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 146.5861 - vae_r_loss: 111.3380 - vae_kl_loss: 35.2481\n",
      "Epoch 45/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 145.7077 - vae_r_loss: 110.5119 - vae_kl_loss: 35.1958\n",
      "Epoch 46/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 145.7454 - vae_r_loss: 110.4191 - vae_kl_loss: 35.3263\n",
      "Epoch 47/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 144.0298 - vae_r_loss: 108.1526 - vae_kl_loss: 35.8771\n",
      "Epoch 48/1000\n",
      "500/500 [==============================] - ETA: 0s - loss: 143.4926 - vae_r_loss: 108.4317 - vae_kl_loss: 35.060 - 1s 2ms/step - loss: 143.4761 - vae_r_loss: 108.2238 - vae_kl_loss: 35.2523\n",
      "Epoch 49/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 142.4577 - vae_r_loss: 106.7816 - vae_kl_loss: 35.6761\n",
      "Epoch 50/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 141.1079 - vae_r_loss: 105.8681 - vae_kl_loss: 35.2398\n",
      "Epoch 51/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 141.4565 - vae_r_loss: 106.5921 - vae_kl_loss: 34.8644\n",
      "Epoch 52/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 141.3548 - vae_r_loss: 105.6909 - vae_kl_loss: 35.6639\n",
      "Epoch 53/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 139.3802 - vae_r_loss: 104.1937 - vae_kl_loss: 35.1865\n",
      "Epoch 54/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 138.6048 - vae_r_loss: 103.3087 - vae_kl_loss: 35.2961: 0s - loss: 135.9559 - vae_r_loss: 99.5773 - vae_kl\n",
      "Epoch 55/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 137.6249 - vae_r_loss: 102.1443 - vae_kl_loss: 35.4806\n",
      "Epoch 56/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 136.8007 - vae_r_loss: 101.7279 - vae_kl_loss: 35.0728A: 0s - loss: 134.7483 - vae_r_loss: 100.1654 - vae_kl_los\n",
      "Epoch 57/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 138.5264 - vae_r_loss: 102.7268 - vae_kl_loss: 35.7997A: 0s - loss: 139.9934 - vae_r_loss: 103.8332 - vae_kl_loss: 36\n",
      "Epoch 58/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 136.4269 - vae_r_loss: 100.7309 - vae_kl_loss: 35.6960\n",
      "Epoch 59/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 136.6363 - vae_r_loss: 101.2988 - vae_kl_loss: 35.3375\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 135.6826 - vae_r_loss: 99.9059 - vae_kl_loss: 35.7767\n",
      "Epoch 61/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 134.6850 - vae_r_loss: 99.7961 - vae_kl_loss: 34.8890\n",
      "Epoch 62/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 134.8523 - vae_r_loss: 99.2363 - vae_kl_loss: 35.6160\n",
      "Epoch 63/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 134.1896 - vae_r_loss: 98.6745 - vae_kl_loss: 35.5151\n",
      "Epoch 64/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 133.1239 - vae_r_loss: 97.3342 - vae_kl_loss: 35.7897\n",
      "Epoch 65/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 132.5968 - vae_r_loss: 97.0924 - vae_kl_loss: 35.5044\n",
      "Epoch 66/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 131.3237 - vae_r_loss: 95.4895 - vae_kl_loss: 35.8342\n",
      "Epoch 67/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 131.2952 - vae_r_loss: 95.6549 - vae_kl_loss: 35.6403\n",
      "Epoch 68/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 131.0891 - vae_r_loss: 95.7844 - vae_kl_loss: 35.3047\n",
      "Epoch 69/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 131.1464 - vae_r_loss: 95.4558 - vae_kl_loss: 35.6906\n",
      "Epoch 70/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 130.1372 - vae_r_loss: 94.2437 - vae_kl_loss: 35.8935\n",
      "Epoch 71/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 129.6698 - vae_r_loss: 93.9167 - vae_kl_loss: 35.7531\n",
      "Epoch 72/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 129.5808 - vae_r_loss: 93.4526 - vae_kl_loss: 36.1282\n",
      "Epoch 73/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 129.3677 - vae_r_loss: 93.8062 - vae_kl_loss: 35.5616\n",
      "Epoch 74/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 128.5341 - vae_r_loss: 92.5384 - vae_kl_loss: 35.9957\n",
      "Epoch 75/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 129.1720 - vae_r_loss: 93.0983 - vae_kl_loss: 36.0738\n",
      "Epoch 76/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 128.7554 - vae_r_loss: 92.6217 - vae_kl_loss: 36.1337\n",
      "Epoch 77/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 128.2464 - vae_r_loss: 91.7037 - vae_kl_loss: 36.5427\n",
      "Epoch 78/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 127.7468 - vae_r_loss: 91.7881 - vae_kl_loss: 35.9587: 0s - loss: 127.7552 - vae_r_loss: 91.7907 - vae_kl_loss: 3\n",
      "Epoch 79/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 126.7866 - vae_r_loss: 90.9960 - vae_kl_loss: 35.7906\n",
      "Epoch 80/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 126.4375 - vae_r_loss: 90.3272 - vae_kl_loss: 36.1103\n",
      "Epoch 81/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 125.7357 - vae_r_loss: 89.4136 - vae_kl_loss: 36.3221: 0s - loss: 129.2259 - vae_r_loss: 91.6367 \n",
      "Epoch 82/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 126.4971 - vae_r_loss: 89.6801 - vae_kl_loss: 36.8170: 0s - loss: 127.0596 - vae_r_loss: 90.4223 - vae_kl_loss: 36.63 - ETA: 0s - loss: 126.4211 - vae_r_loss: 89.7540 - vae_kl\n",
      "Epoch 83/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 125.5375 - vae_r_loss: 89.1467 - vae_kl_loss: 36.3908\n",
      "Epoch 84/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 125.4566 - vae_r_loss: 89.0569 - vae_kl_loss: 36.3998\n",
      "Epoch 85/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 125.3502 - vae_r_loss: 88.4457 - vae_kl_loss: 36.9046\n",
      "Epoch 86/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 123.5224 - vae_r_loss: 87.4667 - vae_kl_loss: 36.0557\n",
      "Epoch 87/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 124.0352 - vae_r_loss: 86.9950 - vae_kl_loss: 37.0402\n",
      "Epoch 88/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 124.1091 - vae_r_loss: 87.3457 - vae_kl_loss: 36.7634\n",
      "Epoch 89/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 123.3101 - vae_r_loss: 86.2533 - vae_kl_loss: 37.0568\n",
      "Epoch 90/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 123.9557 - vae_r_loss: 86.7863 - vae_kl_loss: 37.1694\n",
      "Epoch 91/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 123.6073 - vae_r_loss: 86.9988 - vae_kl_loss: 36.6085\n",
      "Epoch 92/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 123.0609 - vae_r_loss: 86.1431 - vae_kl_loss: 36.9177\n",
      "Epoch 93/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 122.5359 - vae_r_loss: 85.4299 - vae_kl_loss: 37.1059\n",
      "Epoch 94/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 121.9476 - vae_r_loss: 84.4917 - vae_kl_loss: 37.4559\n",
      "Epoch 95/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 121.8664 - vae_r_loss: 84.0916 - vae_kl_loss: 37.7748\n",
      "Epoch 96/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 120.9590 - vae_r_loss: 84.1270 - vae_kl_loss: 36.8320\n",
      "Epoch 97/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 121.7355 - vae_r_loss: 83.9955 - vae_kl_loss: 37.7399\n",
      "Epoch 98/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 120.5781 - vae_r_loss: 83.3318 - vae_kl_loss: 37.2463\n",
      "Epoch 99/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 120.3019 - vae_r_loss: 82.6271 - vae_kl_loss: 37.6748\n",
      "Epoch 100/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 120.7395 - vae_r_loss: 83.2588 - vae_kl_loss: 37.4807\n",
      "Epoch 101/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 120.0206 - vae_r_loss: 82.4172 - vae_kl_loss: 37.6034\n",
      "Epoch 102/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 119.5215 - vae_r_loss: 81.8291 - vae_kl_loss: 37.6924\n",
      "Epoch 103/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 118.7277 - vae_r_loss: 81.4025 - vae_kl_loss: 37.3252\n",
      "Epoch 104/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 118.7489 - vae_r_loss: 81.2096 - vae_kl_loss: 37.5393\n",
      "Epoch 105/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 118.7225 - vae_r_loss: 80.9789 - vae_kl_loss: 37.7436\n",
      "Epoch 106/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 118.5620 - vae_r_loss: 80.4124 - vae_kl_loss: 38.1496\n",
      "Epoch 107/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 119.1006 - vae_r_loss: 80.8978 - vae_kl_loss: 38.2029\n",
      "Epoch 108/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 118.6274 - vae_r_loss: 80.5764 - vae_kl_loss: 38.0511\n",
      "Epoch 109/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 118.2506 - vae_r_loss: 79.8042 - vae_kl_loss: 38.4464\n",
      "Epoch 110/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 117.5525 - vae_r_loss: 79.2903 - vae_kl_loss: 38.2622\n",
      "Epoch 111/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 117.7466 - vae_r_loss: 79.2975 - vae_kl_loss: 38.4491\n",
      "Epoch 112/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 117.1938 - vae_r_loss: 78.5657 - vae_kl_loss: 38.6282\n",
      "Epoch 113/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 116.3730 - vae_r_loss: 77.8839 - vae_kl_loss: 38.4891\n",
      "Epoch 114/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 116.4110 - vae_r_loss: 77.8206 - vae_kl_loss: 38.5905\n",
      "Epoch 115/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 116.0277 - vae_r_loss: 77.3942 - vae_kl_loss: 38.6336\n",
      "Epoch 116/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 116.0632 - vae_r_loss: 77.3031 - vae_kl_loss: 38.7601\n",
      "Epoch 117/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 115.3315 - vae_r_loss: 76.6663 - vae_kl_loss: 38.6652\n",
      "Epoch 118/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 116.0292 - vae_r_loss: 76.7011 - vae_kl_loss: 39.3281\n",
      "Epoch 119/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 114.7557 - vae_r_loss: 75.9834 - vae_kl_loss: 38.7723\n",
      "Epoch 120/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 114.5235 - vae_r_loss: 75.3840 - vae_kl_loss: 39.1395\n",
      "Epoch 121/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 114.4958 - vae_r_loss: 75.8883 - vae_kl_loss: 38.6075\n",
      "Epoch 122/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 114.0021 - vae_r_loss: 75.2185 - vae_kl_loss: 38.7836\n",
      "Epoch 123/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 113.5986 - vae_r_loss: 74.8627 - vae_kl_loss: 38.7358\n",
      "Epoch 124/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 113.8679 - vae_r_loss: 74.0956 - vae_kl_loss: 39.7723\n",
      "Epoch 125/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 113.5289 - vae_r_loss: 74.1978 - vae_kl_loss: 39.3311\n",
      "Epoch 126/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 113.7620 - vae_r_loss: 74.6842 - vae_kl_loss: 39.0778\n",
      "Epoch 127/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 113.2546 - vae_r_loss: 73.6181 - vae_kl_loss: 39.6365\n",
      "Epoch 128/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 111.8910 - vae_r_loss: 73.3683 - vae_kl_loss: 38.5227: 0s - loss: 111.3963 - vae_r_loss: 73.1549 - vae_kl_l\n",
      "Epoch 129/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 112.2427 - vae_r_loss: 72.7831 - vae_kl_loss: 39.4595: 0s - loss: 110.2671 - vae_r_loss: 71.0224 \n",
      "Epoch 130/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 112.2425 - vae_r_loss: 72.6814 - vae_kl_loss: 39.5610\n",
      "Epoch 131/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 111.2257 - vae_r_loss: 71.9564 - vae_kl_loss: 39.2693\n",
      "Epoch 132/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 111.3705 - vae_r_loss: 72.2407 - vae_kl_loss: 39.1298\n",
      "Epoch 133/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 111.3557 - vae_r_loss: 72.0115 - vae_kl_loss: 39.3443\n",
      "Epoch 134/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 111.9408 - vae_r_loss: 72.4568 - vae_kl_loss: 39.4840\n",
      "Epoch 135/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 111.1927 - vae_r_loss: 71.6864 - vae_kl_loss: 39.5063: 0s - loss: 111.4065 - vae_r_loss: 71.9823 - vae_kl_los\n",
      "Epoch 136/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 110.6665 - vae_r_loss: 71.0058 - vae_kl_loss: 39.6607\n",
      "Epoch 137/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 109.8885 - vae_r_loss: 70.6141 - vae_kl_loss: 39.2744\n",
      "Epoch 138/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 109.9200 - vae_r_loss: 70.1612 - vae_kl_loss: 39.7587\n",
      "Epoch 139/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 110.1494 - vae_r_loss: 70.6386 - vae_kl_loss: 39.5107\n",
      "Epoch 140/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 109.4087 - vae_r_loss: 69.9468 - vae_kl_loss: 39.4619\n",
      "Epoch 141/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 109.5811 - vae_r_loss: 70.0078 - vae_kl_loss: 39.5733\n",
      "Epoch 142/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 108.7143 - vae_r_loss: 69.2163 - vae_kl_loss: 39.4980\n",
      "Epoch 143/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 108.4181 - vae_r_loss: 68.7271 - vae_kl_loss: 39.6910\n",
      "Epoch 144/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 108.9515 - vae_r_loss: 68.8004 - vae_kl_loss: 40.1512\n",
      "Epoch 145/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 107.6545 - vae_r_loss: 68.0000 - vae_kl_loss: 39.6544\n",
      "Epoch 146/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 108.0271 - vae_r_loss: 67.4182 - vae_kl_loss: 40.6089\n",
      "Epoch 147/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 107.9177 - vae_r_loss: 67.7734 - vae_kl_loss: 40.1443\n",
      "Epoch 148/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 106.9249 - vae_r_loss: 66.9954 - vae_kl_loss: 39.9294\n",
      "Epoch 149/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 106.7528 - vae_r_loss: 67.0832 - vae_kl_loss: 39.6696\n",
      "Epoch 150/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 107.6466 - vae_r_loss: 67.3743 - vae_kl_loss: 40.2723\n",
      "Epoch 151/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 106.5873 - vae_r_loss: 66.5722 - vae_kl_loss: 40.0151\n",
      "Epoch 152/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 105.9961 - vae_r_loss: 66.2328 - vae_kl_loss: 39.7633\n",
      "Epoch 153/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 105.4807 - vae_r_loss: 65.3885 - vae_kl_loss: 40.0922\n",
      "Epoch 154/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 106.5559 - vae_r_loss: 66.0693 - vae_kl_loss: 40.4866\n",
      "Epoch 155/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 105.6240 - vae_r_loss: 65.6361 - vae_kl_loss: 39.9879\n",
      "Epoch 156/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 105.4081 - vae_r_loss: 65.3005 - vae_kl_loss: 40.1076\n",
      "Epoch 157/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 104.7931 - vae_r_loss: 64.7182 - vae_kl_loss: 40.0749\n",
      "Epoch 158/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 104.4166 - vae_r_loss: 64.2272 - vae_kl_loss: 40.1893\n",
      "Epoch 159/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 104.1419 - vae_r_loss: 63.8538 - vae_kl_loss: 40.2881\n",
      "Epoch 160/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 104.0258 - vae_r_loss: 63.9416 - vae_kl_loss: 40.0841\n",
      "Epoch 161/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 104.2006 - vae_r_loss: 64.6346 - vae_kl_loss: 39.5660\n",
      "Epoch 162/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 103.9175 - vae_r_loss: 63.6545 - vae_kl_loss: 40.2629\n",
      "Epoch 163/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 103.8382 - vae_r_loss: 63.5157 - vae_kl_loss: 40.3225\n",
      "Epoch 164/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 103.1993 - vae_r_loss: 62.7388 - vae_kl_loss: 40.4605\n",
      "Epoch 165/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 103.0646 - vae_r_loss: 62.4929 - vae_kl_loss: 40.5717: 0s - loss: 102.0559 - vae_r_loss: 60.3\n",
      "Epoch 166/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 102.8009 - vae_r_loss: 62.2794 - vae_kl_loss: 40.5215\n",
      "Epoch 167/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 102.4814 - vae_r_loss: 62.1343 - vae_kl_loss: 40.3471\n",
      "Epoch 168/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 101.6850 - vae_r_loss: 61.9147 - vae_kl_loss: 39.7702\n",
      "Epoch 169/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 101.6560 - vae_r_loss: 61.0390 - vae_kl_loss: 40.6169\n",
      "Epoch 170/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 101.7652 - vae_r_loss: 61.1585 - vae_kl_loss: 40.6067\n",
      "Epoch 171/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 101.1603 - vae_r_loss: 60.9062 - vae_kl_loss: 40.2541\n",
      "Epoch 172/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 101.3865 - vae_r_loss: 60.5915 - vae_kl_loss: 40.7950\n",
      "Epoch 173/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 100.8101 - vae_r_loss: 59.8297 - vae_kl_loss: 40.9804: 0s - loss: 99.8422 - vae_r_loss: 60.2464 - v\n",
      "Epoch 174/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 100.5250 - vae_r_loss: 60.5047 - vae_kl_loss: 40.0202\n",
      "Epoch 175/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 99.7971 - vae_r_loss: 59.6571 - vae_kl_loss: 40.1400\n",
      "Epoch 176/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 99.9656 - vae_r_loss: 59.3635 - vae_kl_loss: 40.6021\n",
      "Epoch 177/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 99.8584 - vae_r_loss: 58.8334 - vae_kl_loss: 41.0251\n",
      "Epoch 178/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 99.0126 - vae_r_loss: 58.9178 - vae_kl_loss: 40.0948\n",
      "Epoch 179/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 99.4520 - vae_r_loss: 59.1623 - vae_kl_loss: 40.2897\n",
      "Epoch 180/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 99.1497 - vae_r_loss: 58.7161 - vae_kl_loss: 40.4335\n",
      "Epoch 181/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 99.0860 - vae_r_loss: 58.4344 - vae_kl_loss: 40.6516\n",
      "Epoch 182/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 99.2062 - vae_r_loss: 59.2376 - vae_kl_loss: 39.9686\n",
      "Epoch 183/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 99.5077 - vae_r_loss: 57.9650 - vae_kl_loss: 41.5426\n",
      "Epoch 184/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 98.6359 - vae_r_loss: 58.3252 - vae_kl_loss: 40.3107\n",
      "Epoch 185/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 98.1186 - vae_r_loss: 57.3346 - vae_kl_loss: 40.7840\n",
      "Epoch 186/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 98.1099 - vae_r_loss: 57.5427 - vae_kl_loss: 40.5672\n",
      "Epoch 187/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 98.6918 - vae_r_loss: 58.1329 - vae_kl_loss: 40.5589\n",
      "Epoch 188/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 98.4144 - vae_r_loss: 57.8038 - vae_kl_loss: 40.6106\n",
      "Epoch 189/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 98.1208 - vae_r_loss: 57.0688 - vae_kl_loss: 41.0520\n",
      "Epoch 190/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 98.0625 - vae_r_loss: 57.0304 - vae_kl_loss: 41.0321\n",
      "Epoch 191/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 97.2774 - vae_r_loss: 56.3697 - vae_kl_loss: 40.9077\n",
      "Epoch 192/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 97.2174 - vae_r_loss: 56.4614 - vae_kl_loss: 40.7560\n",
      "Epoch 193/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 96.4226 - vae_r_loss: 55.7472 - vae_kl_loss: 40.6754\n",
      "Epoch 194/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 96.8611 - vae_r_loss: 55.6898 - vae_kl_loss: 41.1713\n",
      "Epoch 195/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 96.1244 - vae_r_loss: 55.0804 - vae_kl_loss: 41.0440\n",
      "Epoch 196/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 95.1706 - vae_r_loss: 54.6030 - vae_kl_loss: 40.5676\n",
      "Epoch 197/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 95.2684 - vae_r_loss: 54.5759 - vae_kl_loss: 40.6924\n",
      "Epoch 198/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 95.5251 - vae_r_loss: 54.3681 - vae_kl_loss: 41.1571\n",
      "Epoch 199/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 94.8381 - vae_r_loss: 54.2698 - vae_kl_loss: 40.5683\n",
      "Epoch 200/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 95.2093 - vae_r_loss: 54.2442 - vae_kl_loss: 40.9652\n",
      "Epoch 201/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 94.9765 - vae_r_loss: 54.5109 - vae_kl_loss: 40.4657\n",
      "Epoch 202/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 94.9247 - vae_r_loss: 53.7320 - vae_kl_loss: 41.1927\n",
      "Epoch 203/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 95.0144 - vae_r_loss: 54.0074 - vae_kl_loss: 41.0069\n",
      "Epoch 204/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 94.6697 - vae_r_loss: 54.0175 - vae_kl_loss: 40.6522\n",
      "Epoch 205/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 93.6143 - vae_r_loss: 53.0156 - vae_kl_loss: 40.5987A: 0s - loss: 94.2641 - vae_r_loss: 53.1996 - vae_kl_loss: \n",
      "Epoch 206/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 94.1684 - vae_r_loss: 53.0567 - vae_kl_loss: 41.1117\n",
      "Epoch 207/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 93.9130 - vae_r_loss: 53.3327 - vae_kl_loss: 40.5803\n",
      "Epoch 208/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 93.8521 - vae_r_loss: 52.7044 - vae_kl_loss: 41.1476\n",
      "Epoch 209/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 93.7401 - vae_r_loss: 52.6173 - vae_kl_loss: 41.1228\n",
      "Epoch 210/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 93.8253 - vae_r_loss: 52.6212 - vae_kl_loss: 41.2041\n",
      "Epoch 211/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 93.0480 - vae_r_loss: 52.4089 - vae_kl_loss: 40.6391\n",
      "Epoch 212/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 93.0758 - vae_r_loss: 51.7078 - vae_kl_loss: 41.3679\n",
      "Epoch 213/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 92.0258 - vae_r_loss: 51.2207 - vae_kl_loss: 40.8052A: 0s - loss: 92.2575 - vae_r_loss: 51.4648 - vae_kl_loss: 40.7\n",
      "Epoch 214/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 92.2025 - vae_r_loss: 51.5067 - vae_kl_loss: 40.6958A: 0s - loss: 90.2073 - vae_r_loss: 49.2596 - vae_kl_loss\n",
      "Epoch 215/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 92.0123 - vae_r_loss: 51.2472 - vae_kl_loss: 40.7652\n",
      "Epoch 216/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 92.0704 - vae_r_loss: 50.7894 - vae_kl_loss: 41.2811A: 0s - loss: 91.6961 - vae_r_loss: 50.5011 - vae_kl_loss: 4\n",
      "Epoch 217/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 91.0017 - vae_r_loss: 50.3338 - vae_kl_loss: 40.6679\n",
      "Epoch 218/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 91.4059 - vae_r_loss: 50.3686 - vae_kl_loss: 41.0373\n",
      "Epoch 219/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 90.7287 - vae_r_loss: 50.0743 - vae_kl_loss: 40.6544\n",
      "Epoch 220/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 90.8816 - vae_r_loss: 49.8705 - vae_kl_loss: 41.0111\n",
      "Epoch 221/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 90.6991 - vae_r_loss: 50.1879 - vae_kl_loss: 40.5112\n",
      "Epoch 222/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 90.8848 - vae_r_loss: 50.1167 - vae_kl_loss: 40.7681\n",
      "Epoch 223/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 90.5417 - vae_r_loss: 49.8298 - vae_kl_loss: 40.7119\n",
      "Epoch 224/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 89.9979 - vae_r_loss: 49.0277 - vae_kl_loss: 40.9701\n",
      "Epoch 225/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 90.4781 - vae_r_loss: 49.9624 - vae_kl_loss: 40.5157\n",
      "Epoch 226/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 90.2809 - vae_r_loss: 49.3841 - vae_kl_loss: 40.8968\n",
      "Epoch 227/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 89.9492 - vae_r_loss: 49.5433 - vae_kl_loss: 40.4059\n",
      "Epoch 228/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 90.1693 - vae_r_loss: 49.0804 - vae_kl_loss: 41.0889\n",
      "Epoch 229/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 89.4263 - vae_r_loss: 48.5493 - vae_kl_loss: 40.8770\n",
      "Epoch 230/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 89.5960 - vae_r_loss: 48.4827 - vae_kl_loss: 41.1132\n",
      "Epoch 231/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 89.3129 - vae_r_loss: 48.6972 - vae_kl_loss: 40.6157\n",
      "Epoch 232/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 89.3646 - vae_r_loss: 48.5839 - vae_kl_loss: 40.7807\n",
      "Epoch 233/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 90.5530 - vae_r_loss: 49.1144 - vae_kl_loss: 41.4385\n",
      "Epoch 234/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 88.4688 - vae_r_loss: 48.1811 - vae_kl_loss: 40.2877\n",
      "Epoch 235/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 88.2236 - vae_r_loss: 47.6396 - vae_kl_loss: 40.5840\n",
      "Epoch 236/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 88.3692 - vae_r_loss: 47.9278 - vae_kl_loss: 40.4413\n",
      "Epoch 237/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 88.0513 - vae_r_loss: 47.1098 - vae_kl_loss: 40.9415\n",
      "Epoch 238/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 87.7486 - vae_r_loss: 47.3916 - vae_kl_loss: 40.3570A: 0s - loss: 87.6347 - vae_r_loss: 47.7614 - vae_kl_loss\n",
      "Epoch 239/1000\n",
      "500/500 [==============================] - 1s 3ms/step - loss: 87.4065 - vae_r_loss: 46.6224 - vae_kl_loss: 40.7841\n",
      "Epoch 240/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 87.9968 - vae_r_loss: 46.6992 - vae_kl_loss: 41.2975\n",
      "Epoch 241/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 87.7414 - vae_r_loss: 47.2240 - vae_kl_loss: 40.5174\n",
      "Epoch 242/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 87.4266 - vae_r_loss: 46.5242 - vae_kl_loss: 40.9024\n",
      "Epoch 243/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 87.0879 - vae_r_loss: 46.2705 - vae_kl_loss: 40.8173\n",
      "Epoch 244/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 86.8869 - vae_r_loss: 46.2846 - vae_kl_loss: 40.6023\n",
      "Epoch 245/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 87.3339 - vae_r_loss: 46.4970 - vae_kl_loss: 40.8369\n",
      "Epoch 246/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 86.7677 - vae_r_loss: 46.0568 - vae_kl_loss: 40.7109\n",
      "Epoch 247/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 87.0203 - vae_r_loss: 46.1629 - vae_kl_loss: 40.8574\n",
      "Epoch 248/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 86.4441 - vae_r_loss: 45.8694 - vae_kl_loss: 40.5747\n",
      "Epoch 249/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 86.9917 - vae_r_loss: 45.6653 - vae_kl_loss: 41.3264\n",
      "Epoch 250/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 87.0599 - vae_r_loss: 46.1871 - vae_kl_loss: 40.8728\n",
      "Epoch 251/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 86.2047 - vae_r_loss: 45.6912 - vae_kl_loss: 40.5135\n",
      "Epoch 252/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 85.9070 - vae_r_loss: 45.2362 - vae_kl_loss: 40.6708\n",
      "Epoch 253/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 85.7670 - vae_r_loss: 44.9881 - vae_kl_loss: 40.7789\n",
      "Epoch 254/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 85.3634 - vae_r_loss: 44.9521 - vae_kl_loss: 40.4113\n",
      "Epoch 255/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 85.4005 - vae_r_loss: 44.7828 - vae_kl_loss: 40.6177\n",
      "Epoch 256/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 85.3999 - vae_r_loss: 45.1844 - vae_kl_loss: 40.2156\n",
      "Epoch 257/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 85.0251 - vae_r_loss: 44.3754 - vae_kl_loss: 40.6497\n",
      "Epoch 258/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 84.8997 - vae_r_loss: 44.4623 - vae_kl_loss: 40.4373\n",
      "Epoch 259/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 84.4568 - vae_r_loss: 43.9930 - vae_kl_loss: 40.4638\n",
      "Epoch 260/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 84.4303 - vae_r_loss: 44.4725 - vae_kl_loss: 39.9578\n",
      "Epoch 261/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 84.8311 - vae_r_loss: 44.4444 - vae_kl_loss: 40.3867\n",
      "Epoch 262/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 85.1175 - vae_r_loss: 44.0402 - vae_kl_loss: 41.0773\n",
      "Epoch 263/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 84.3506 - vae_r_loss: 43.8588 - vae_kl_loss: 40.4919\n",
      "Epoch 264/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 84.8314 - vae_r_loss: 43.9218 - vae_kl_loss: 40.9097\n",
      "Epoch 265/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 84.3655 - vae_r_loss: 43.9425 - vae_kl_loss: 40.4231A: 0s - loss: 84.2118 - vae_r_loss: 43.8429 - vae_kl_loss: 40.36\n",
      "Epoch 266/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 84.4320 - vae_r_loss: 43.5294 - vae_kl_loss: 40.9026\n",
      "Epoch 267/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 83.7026 - vae_r_loss: 43.4482 - vae_kl_loss: 40.2544\n",
      "Epoch 268/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 83.8556 - vae_r_loss: 43.7303 - vae_kl_loss: 40.1253\n",
      "Epoch 269/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 83.6257 - vae_r_loss: 42.9820 - vae_kl_loss: 40.6437\n",
      "Epoch 270/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 83.0635 - vae_r_loss: 42.6903 - vae_kl_loss: 40.3732\n",
      "Epoch 271/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 82.8697 - vae_r_loss: 42.4632 - vae_kl_loss: 40.4065\n",
      "Epoch 272/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 82.6206 - vae_r_loss: 42.7297 - vae_kl_loss: 39.8909\n",
      "Epoch 273/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 83.3785 - vae_r_loss: 42.7751 - vae_kl_loss: 40.6034\n",
      "Epoch 274/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 83.5397 - vae_r_loss: 42.9718 - vae_kl_loss: 40.5679\n",
      "Epoch 275/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 82.6557 - vae_r_loss: 42.4428 - vae_kl_loss: 40.2130\n",
      "Epoch 276/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 82.1842 - vae_r_loss: 42.0780 - vae_kl_loss: 40.1061\n",
      "Epoch 277/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 82.2380 - vae_r_loss: 41.9659 - vae_kl_loss: 40.2721\n",
      "Epoch 278/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 82.7614 - vae_r_loss: 42.0528 - vae_kl_loss: 40.7086A: 0s - loss: 82.5571 - vae_r_loss: 41.6639 - vae_kl_loss\n",
      "Epoch 279/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 82.0070 - vae_r_loss: 41.6046 - vae_kl_loss: 40.4024\n",
      "Epoch 280/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 81.9435 - vae_r_loss: 41.7562 - vae_kl_loss: 40.1873\n",
      "Epoch 281/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 81.8497 - vae_r_loss: 41.7081 - vae_kl_loss: 40.1416\n",
      "Epoch 282/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 81.7568 - vae_r_loss: 41.6216 - vae_kl_loss: 40.1352\n",
      "Epoch 283/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 81.6399 - vae_r_loss: 41.0908 - vae_kl_loss: 40.5491\n",
      "Epoch 284/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 81.1307 - vae_r_loss: 41.1599 - vae_kl_loss: 39.9708A: 0s - loss: 81.1784 - vae_r_loss: 41.1296 - vae_kl_loss: 40.04\n",
      "Epoch 285/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 81.8236 - vae_r_loss: 41.6388 - vae_kl_loss: 40.1848\n",
      "Epoch 286/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 81.7483 - vae_r_loss: 41.4855 - vae_kl_loss: 40.2628\n",
      "Epoch 287/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 80.4432 - vae_r_loss: 40.6748 - vae_kl_loss: 39.7685A: 0s - loss: 80.6137 - vae_r_loss: 40.8086 - vae_kl_loss: 39\n",
      "Epoch 288/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 81.9173 - vae_r_loss: 41.3136 - vae_kl_loss: 40.6037A: 0s - loss: 80.9802 - vae_r_loss: 41.4597 - vae_kl_l\n",
      "Epoch 289/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 80.6630 - vae_r_loss: 40.5319 - vae_kl_loss: 40.1311\n",
      "Epoch 290/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 81.0019 - vae_r_loss: 40.9581 - vae_kl_loss: 40.0438\n",
      "Epoch 291/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 80.3170 - vae_r_loss: 40.1865 - vae_kl_loss: 40.1305\n",
      "Epoch 292/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 79.9731 - vae_r_loss: 39.8569 - vae_kl_loss: 40.1161\n",
      "Epoch 293/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 80.7650 - vae_r_loss: 40.6529 - vae_kl_loss: 40.1122\n",
      "Epoch 294/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 80.4452 - vae_r_loss: 40.6763 - vae_kl_loss: 39.7689\n",
      "Epoch 295/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 80.7397 - vae_r_loss: 40.4432 - vae_kl_loss: 40.2964\n",
      "Epoch 296/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 80.4793 - vae_r_loss: 40.0829 - vae_kl_loss: 40.3963\n",
      "Epoch 297/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 79.7965 - vae_r_loss: 39.9328 - vae_kl_loss: 39.8638A: 0s - loss: 80.0083 - vae_r_loss: 40.1413 - vae_kl_loss: 39.\n",
      "Epoch 298/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 79.9290 - vae_r_loss: 39.9131 - vae_kl_loss: 40.0159\n",
      "Epoch 299/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 80.0821 - vae_r_loss: 40.0182 - vae_kl_loss: 40.0639\n",
      "Epoch 300/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 79.8464 - vae_r_loss: 39.6887 - vae_kl_loss: 40.1577\n",
      "Epoch 301/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 79.5384 - vae_r_loss: 39.4833 - vae_kl_loss: 40.0551\n",
      "Epoch 302/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 79.9137 - vae_r_loss: 40.0491 - vae_kl_loss: 39.8646\n",
      "Epoch 303/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 79.8808 - vae_r_loss: 39.7680 - vae_kl_loss: 40.1127\n",
      "Epoch 304/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 79.0084 - vae_r_loss: 39.1784 - vae_kl_loss: 39.8300\n",
      "Epoch 305/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.8828 - vae_r_loss: 39.2875 - vae_kl_loss: 39.5953\n",
      "Epoch 306/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.6474 - vae_r_loss: 38.7383 - vae_kl_loss: 39.9091\n",
      "Epoch 307/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.4061 - vae_r_loss: 38.8674 - vae_kl_loss: 39.5387\n",
      "Epoch 308/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.6421 - vae_r_loss: 38.3976 - vae_kl_loss: 40.2446\n",
      "Epoch 309/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.9678 - vae_r_loss: 38.9942 - vae_kl_loss: 39.9736\n",
      "Epoch 310/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.9618 - vae_r_loss: 38.5494 - vae_kl_loss: 40.4124\n",
      "Epoch 311/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.1591 - vae_r_loss: 38.3318 - vae_kl_loss: 39.8274\n",
      "Epoch 312/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.6546 - vae_r_loss: 38.3512 - vae_kl_loss: 40.3034\n",
      "Epoch 313/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.1936 - vae_r_loss: 38.5013 - vae_kl_loss: 39.6923\n",
      "Epoch 314/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.6770 - vae_r_loss: 38.8392 - vae_kl_loss: 39.8378\n",
      "Epoch 315/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.0967 - vae_r_loss: 38.1563 - vae_kl_loss: 39.9405\n",
      "Epoch 316/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.7784 - vae_r_loss: 38.4480 - vae_kl_loss: 40.3304A: 0s - loss: 78.9360 - vae_r_loss: 38.2046 - vae_kl_loss:\n",
      "Epoch 317/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 78.3751 - vae_r_loss: 38.6360 - vae_kl_loss: 39.7391\n",
      "Epoch 318/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 77.2764 - vae_r_loss: 37.6342 - vae_kl_loss: 39.6422\n",
      "Epoch 319/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 77.6871 - vae_r_loss: 38.2478 - vae_kl_loss: 39.4393\n",
      "Epoch 320/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 77.6124 - vae_r_loss: 37.8714 - vae_kl_loss: 39.7410\n",
      "Epoch 321/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 77.6341 - vae_r_loss: 38.0326 - vae_kl_loss: 39.6015\n",
      "Epoch 322/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 77.5520 - vae_r_loss: 37.7131 - vae_kl_loss: 39.8389\n",
      "Epoch 323/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 77.4309 - vae_r_loss: 37.7022 - vae_kl_loss: 39.7287\n",
      "Epoch 324/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 77.3102 - vae_r_loss: 37.7049 - vae_kl_loss: 39.6053\n",
      "Epoch 325/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.6289 - vae_r_loss: 36.6714 - vae_kl_loss: 39.9575\n",
      "Epoch 326/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.5866 - vae_r_loss: 37.2927 - vae_kl_loss: 39.2939\n",
      "Epoch 327/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.5480 - vae_r_loss: 36.6160 - vae_kl_loss: 39.9320\n",
      "Epoch 328/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.5069 - vae_r_loss: 36.6037 - vae_kl_loss: 39.9031\n",
      "Epoch 329/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.3812 - vae_r_loss: 36.8036 - vae_kl_loss: 39.5777\n",
      "Epoch 330/1000\n",
      "500/500 [==============================] - ETA: 0s - loss: 76.6139 - vae_r_loss: 37.0329 - vae_kl_loss: 39.581 - 1s 2ms/step - loss: 76.7065 - vae_r_loss: 37.1290 - vae_kl_loss: 39.5774\n",
      "Epoch 331/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.6816 - vae_r_loss: 37.5167 - vae_kl_loss: 39.1649\n",
      "Epoch 332/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.3682 - vae_r_loss: 36.8321 - vae_kl_loss: 39.5361\n",
      "Epoch 333/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.6219 - vae_r_loss: 36.9629 - vae_kl_loss: 39.6591\n",
      "Epoch 334/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.2327 - vae_r_loss: 36.2371 - vae_kl_loss: 39.9956\n",
      "Epoch 335/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 75.6775 - vae_r_loss: 36.2121 - vae_kl_loss: 39.4654\n",
      "Epoch 336/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.1188 - vae_r_loss: 36.6150 - vae_kl_loss: 39.5038\n",
      "Epoch 337/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.4706 - vae_r_loss: 36.5697 - vae_kl_loss: 39.9009\n",
      "Epoch 338/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 75.5349 - vae_r_loss: 36.1702 - vae_kl_loss: 39.3647\n",
      "Epoch 339/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 75.8296 - vae_r_loss: 36.4727 - vae_kl_loss: 39.3569\n",
      "Epoch 340/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 76.0819 - vae_r_loss: 36.6280 - vae_kl_loss: 39.4539\n",
      "Epoch 341/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 75.9763 - vae_r_loss: 36.7300 - vae_kl_loss: 39.2463\n",
      "Epoch 342/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 75.3471 - vae_r_loss: 35.9106 - vae_kl_loss: 39.4364\n",
      "Epoch 343/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.3188 - vae_r_loss: 35.4278 - vae_kl_loss: 38.8910\n",
      "Epoch 344/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 75.4143 - vae_r_loss: 36.0425 - vae_kl_loss: 39.3717\n",
      "Epoch 345/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.9340 - vae_r_loss: 35.6883 - vae_kl_loss: 39.2456\n",
      "Epoch 346/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 75.4946 - vae_r_loss: 36.0115 - vae_kl_loss: 39.4830\n",
      "Epoch 347/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 75.4416 - vae_r_loss: 36.0304 - vae_kl_loss: 39.4112\n",
      "Epoch 348/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.5894 - vae_r_loss: 35.9726 - vae_kl_loss: 38.6168\n",
      "Epoch 349/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 75.1057 - vae_r_loss: 35.7417 - vae_kl_loss: 39.3640\n",
      "Epoch 350/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.7208 - vae_r_loss: 35.3268 - vae_kl_loss: 39.3940\n",
      "Epoch 351/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.2440 - vae_r_loss: 35.2915 - vae_kl_loss: 38.9526\n",
      "Epoch 352/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.2604 - vae_r_loss: 35.1506 - vae_kl_loss: 39.1098\n",
      "Epoch 353/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.2744 - vae_r_loss: 35.3009 - vae_kl_loss: 38.9734\n",
      "Epoch 354/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.1164 - vae_r_loss: 35.1793 - vae_kl_loss: 38.9371\n",
      "Epoch 355/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.9584 - vae_r_loss: 34.7419 - vae_kl_loss: 39.2165\n",
      "Epoch 356/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.7193 - vae_r_loss: 34.6359 - vae_kl_loss: 39.0835\n",
      "Epoch 357/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.3898 - vae_r_loss: 34.9651 - vae_kl_loss: 39.4247\n",
      "Epoch 358/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.4695 - vae_r_loss: 35.3943 - vae_kl_loss: 39.0751\n",
      "Epoch 359/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 74.0493 - vae_r_loss: 34.9189 - vae_kl_loss: 39.1303\n",
      "Epoch 360/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 74.0266 - vae_r_loss: 34.9104 - vae_kl_loss: 39.1163\n",
      "Epoch 361/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.3408 - vae_r_loss: 34.2857 - vae_kl_loss: 39.0551\n",
      "Epoch 362/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.9640 - vae_r_loss: 34.6775 - vae_kl_loss: 39.2865\n",
      "Epoch 363/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.3185 - vae_r_loss: 34.5536 - vae_kl_loss: 38.7649\n",
      "Epoch 364/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.2017 - vae_r_loss: 34.1100 - vae_kl_loss: 39.0917\n",
      "Epoch 365/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.0970 - vae_r_loss: 33.5734 - vae_kl_loss: 39.5236\n",
      "Epoch 366/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.3774 - vae_r_loss: 34.8022 - vae_kl_loss: 38.5752\n",
      "Epoch 367/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.6520 - vae_r_loss: 34.4057 - vae_kl_loss: 39.2462\n",
      "Epoch 368/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.4025 - vae_r_loss: 34.4924 - vae_kl_loss: 38.9101\n",
      "Epoch 369/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.4924 - vae_r_loss: 34.6205 - vae_kl_loss: 38.8719\n",
      "Epoch 370/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.2906 - vae_r_loss: 34.4025 - vae_kl_loss: 38.8881\n",
      "Epoch 371/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.3728 - vae_r_loss: 34.1291 - vae_kl_loss: 39.2437\n",
      "Epoch 372/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.0780 - vae_r_loss: 34.1712 - vae_kl_loss: 38.9068\n",
      "Epoch 373/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.8279 - vae_r_loss: 33.6705 - vae_kl_loss: 39.1573\n",
      "Epoch 374/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.7442 - vae_r_loss: 33.6905 - vae_kl_loss: 39.0537A: 0s - loss: 72.7478 - vae_r_loss: 33.3889 - vae_kl_loss: 39\n",
      "Epoch 375/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.5884 - vae_r_loss: 33.3555 - vae_kl_loss: 39.2329\n",
      "Epoch 376/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.0248 - vae_r_loss: 33.4938 - vae_kl_loss: 38.5310\n",
      "Epoch 377/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.1554 - vae_r_loss: 33.3838 - vae_kl_loss: 38.7716A: 0s - loss: 71.9980 - vae_r_loss: 33.5507 - vae_kl_loss: 38\n",
      "Epoch 378/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.4810 - vae_r_loss: 34.1791 - vae_kl_loss: 38.3019\n",
      "Epoch 379/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.1353 - vae_r_loss: 33.8073 - vae_kl_loss: 39.3280\n",
      "Epoch 380/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 73.1076 - vae_r_loss: 33.8986 - vae_kl_loss: 39.2090\n",
      "Epoch 381/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.4355 - vae_r_loss: 33.5299 - vae_kl_loss: 38.9056\n",
      "Epoch 382/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 71.9807 - vae_r_loss: 33.3530 - vae_kl_loss: 38.6277\n",
      "Epoch 383/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 71.9787 - vae_r_loss: 33.1355 - vae_kl_loss: 38.8433\n",
      "Epoch 384/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 71.5115 - vae_r_loss: 32.9017 - vae_kl_loss: 38.6098\n",
      "Epoch 385/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 71.4685 - vae_r_loss: 32.4577 - vae_kl_loss: 39.0108\n",
      "Epoch 386/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.2428 - vae_r_loss: 33.2448 - vae_kl_loss: 38.9980\n",
      "Epoch 387/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.1803 - vae_r_loss: 33.3791 - vae_kl_loss: 38.8013\n",
      "Epoch 388/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.5386 - vae_r_loss: 34.3947 - vae_kl_loss: 38.1439\n",
      "Epoch 389/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 71.7142 - vae_r_loss: 32.9380 - vae_kl_loss: 38.7762\n",
      "Epoch 390/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 72.6897 - vae_r_loss: 33.9480 - vae_kl_loss: 38.7416\n",
      "Epoch 391/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 71.4987 - vae_r_loss: 32.8691 - vae_kl_loss: 38.6296\n",
      "Epoch 392/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 71.2152 - vae_r_loss: 32.5264 - vae_kl_loss: 38.6888\n",
      "Epoch 393/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.9264 - vae_r_loss: 32.7194 - vae_kl_loss: 38.2070\n",
      "Epoch 394/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.6964 - vae_r_loss: 32.2118 - vae_kl_loss: 38.4846\n",
      "Epoch 395/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.6478 - vae_r_loss: 32.0675 - vae_kl_loss: 38.5803\n",
      "Epoch 396/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.2526 - vae_r_loss: 32.0610 - vae_kl_loss: 38.1916\n",
      "Epoch 397/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.8528 - vae_r_loss: 32.2408 - vae_kl_loss: 38.6120\n",
      "Epoch 398/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 71.4363 - vae_r_loss: 32.5852 - vae_kl_loss: 38.8511A: 0s - loss: 71.5520 - vae_r_loss: 32.7868 - vae_kl_l\n",
      "Epoch 399/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.3357 - vae_r_loss: 31.8830 - vae_kl_loss: 38.4526\n",
      "Epoch 400/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.3307 - vae_r_loss: 32.2307 - vae_kl_loss: 38.1000\n",
      "Epoch 401/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.2382 - vae_r_loss: 31.9199 - vae_kl_loss: 38.3183\n",
      "Epoch 402/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.5352 - vae_r_loss: 31.3473 - vae_kl_loss: 38.1879\n",
      "Epoch 403/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.2803 - vae_r_loss: 31.9318 - vae_kl_loss: 38.3485\n",
      "Epoch 404/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.9515 - vae_r_loss: 31.6598 - vae_kl_loss: 38.2917\n",
      "Epoch 405/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.4921 - vae_r_loss: 31.9450 - vae_kl_loss: 38.5471\n",
      "Epoch 406/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.6552 - vae_r_loss: 32.0734 - vae_kl_loss: 38.5818\n",
      "Epoch 407/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.2674 - vae_r_loss: 31.8939 - vae_kl_loss: 38.3735A: 0s - loss: 70.3436 - vae_r_loss: 32.1146 - vae_kl_loss: 38\n",
      "Epoch 408/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.5242 - vae_r_loss: 31.1869 - vae_kl_loss: 38.3373\n",
      "Epoch 409/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.4222 - vae_r_loss: 31.5148 - vae_kl_loss: 37.9074\n",
      "Epoch 410/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.8219 - vae_r_loss: 31.6397 - vae_kl_loss: 38.1822\n",
      "Epoch 411/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 70.0185 - vae_r_loss: 31.6228 - vae_kl_loss: 38.3957\n",
      "Epoch 412/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.7722 - vae_r_loss: 31.7692 - vae_kl_loss: 38.0030\n",
      "Epoch 413/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.9330 - vae_r_loss: 31.4050 - vae_kl_loss: 38.5280\n",
      "Epoch 414/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.5968 - vae_r_loss: 31.2764 - vae_kl_loss: 38.3204\n",
      "Epoch 415/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.1111 - vae_r_loss: 31.0275 - vae_kl_loss: 38.0836\n",
      "Epoch 416/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.6470 - vae_r_loss: 31.1396 - vae_kl_loss: 38.5074A: 0s - loss: 68.7621 - vae_r_loss: 30.3454 - vae_kl_loss\n",
      "Epoch 417/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.8455 - vae_r_loss: 31.8545 - vae_kl_loss: 37.9910\n",
      "Epoch 418/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.8657 - vae_r_loss: 31.3016 - vae_kl_loss: 38.5641\n",
      "Epoch 419/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.7850 - vae_r_loss: 31.6374 - vae_kl_loss: 38.1477\n",
      "Epoch 420/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 69.6258 - vae_r_loss: 31.6943 - vae_kl_loss: 37.9315\n",
      "Epoch 421/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.4402 - vae_r_loss: 31.1132 - vae_kl_loss: 37.3270\n",
      "Epoch 422/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.9183 - vae_r_loss: 30.7093 - vae_kl_loss: 38.2091\n",
      "Epoch 423/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.7941 - vae_r_loss: 30.5156 - vae_kl_loss: 38.2786A: 0s - loss: 68.4020 - vae_r_loss: 29.7798 - vae_kl_loss: 3 - ETA: 0s - loss: 69.0648 - vae_r_loss: 30.8148 - vae_kl_loss: 38.25 - ETA: 0s - loss: 68.7878 - vae_r_loss: 30.5665 - vae_kl_loss: 38.2\n",
      "Epoch 424/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 69.0101 - vae_r_loss: 30.4349 - vae_kl_loss: 38.5752\n",
      "Epoch 425/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.7699 - vae_r_loss: 30.7702 - vae_kl_loss: 37.9996\n",
      "Epoch 426/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.8904 - vae_r_loss: 30.5672 - vae_kl_loss: 38.3232\n",
      "Epoch 427/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.8967 - vae_r_loss: 31.3495 - vae_kl_loss: 37.5472\n",
      "Epoch 428/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.7758 - vae_r_loss: 30.5507 - vae_kl_loss: 38.2252\n",
      "Epoch 429/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.2039 - vae_r_loss: 30.4528 - vae_kl_loss: 37.7510\n",
      "Epoch 430/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.7485 - vae_r_loss: 30.4359 - vae_kl_loss: 38.3126\n",
      "Epoch 431/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.7116 - vae_r_loss: 30.5935 - vae_kl_loss: 38.1180\n",
      "Epoch 432/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.6983 - vae_r_loss: 29.9594 - vae_kl_loss: 37.7388A: 0s - loss: 67.8740 - vae_r_loss: 29.8466 - vae_kl_loss\n",
      "Epoch 433/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.2639 - vae_r_loss: 30.0773 - vae_kl_loss: 38.1865\n",
      "Epoch 434/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.0314 - vae_r_loss: 30.3656 - vae_kl_loss: 37.6658\n",
      "Epoch 435/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.9246 - vae_r_loss: 29.9519 - vae_kl_loss: 37.9727\n",
      "Epoch 436/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.8037 - vae_r_loss: 29.8762 - vae_kl_loss: 37.9275\n",
      "Epoch 437/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.2376 - vae_r_loss: 30.5271 - vae_kl_loss: 37.7105\n",
      "Epoch 438/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.8070 - vae_r_loss: 30.2473 - vae_kl_loss: 37.5597\n",
      "Epoch 439/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.9741 - vae_r_loss: 30.2607 - vae_kl_loss: 37.7134\n",
      "Epoch 440/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.8924 - vae_r_loss: 29.7907 - vae_kl_loss: 38.1017\n",
      "Epoch 441/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.3174 - vae_r_loss: 29.7699 - vae_kl_loss: 37.5475\n",
      "Epoch 442/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.2469 - vae_r_loss: 30.3695 - vae_kl_loss: 37.8774A: 0s - loss: 67.9918 - vae_r_loss: 30.5292 - vae_kl_loss: 37\n",
      "Epoch 443/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.8694 - vae_r_loss: 29.9674 - vae_kl_loss: 37.9020\n",
      "Epoch 444/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.5637 - vae_r_loss: 29.9106 - vae_kl_loss: 37.6530\n",
      "Epoch 445/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.5350 - vae_r_loss: 30.4464 - vae_kl_loss: 37.0886\n",
      "Epoch 446/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.7233 - vae_r_loss: 30.2961 - vae_kl_loss: 37.4272\n",
      "Epoch 447/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.2033 - vae_r_loss: 29.7112 - vae_kl_loss: 37.4921\n",
      "Epoch 448/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.2467 - vae_r_loss: 29.5200 - vae_kl_loss: 37.7267\n",
      "Epoch 449/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 68.0321 - vae_r_loss: 29.9744 - vae_kl_loss: 38.0577\n",
      "Epoch 450/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.4683 - vae_r_loss: 29.5598 - vae_kl_loss: 37.9084\n",
      "Epoch 451/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.7650 - vae_r_loss: 29.8538 - vae_kl_loss: 37.9112\n",
      "Epoch 452/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.2053 - vae_r_loss: 29.5072 - vae_kl_loss: 37.6981\n",
      "Epoch 453/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.3854 - vae_r_loss: 29.5901 - vae_kl_loss: 37.7953\n",
      "Epoch 454/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.3757 - vae_r_loss: 29.9462 - vae_kl_loss: 37.4295\n",
      "Epoch 455/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.3095 - vae_r_loss: 30.0683 - vae_kl_loss: 37.2413\n",
      "Epoch 456/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.7231 - vae_r_loss: 28.9933 - vae_kl_loss: 37.7298\n",
      "Epoch 457/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.1425 - vae_r_loss: 29.4192 - vae_kl_loss: 37.7234\n",
      "Epoch 458/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.3206 - vae_r_loss: 28.9231 - vae_kl_loss: 37.3974\n",
      "Epoch 459/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.0602 - vae_r_loss: 28.9845 - vae_kl_loss: 38.0757\n",
      "Epoch 460/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.3268 - vae_r_loss: 29.1477 - vae_kl_loss: 37.1791\n",
      "Epoch 461/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.2944 - vae_r_loss: 28.7565 - vae_kl_loss: 37.5379\n",
      "Epoch 462/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 67.0527 - vae_r_loss: 29.6685 - vae_kl_loss: 37.3842\n",
      "Epoch 463/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.9170 - vae_r_loss: 29.1261 - vae_kl_loss: 37.7909\n",
      "Epoch 464/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.1853 - vae_r_loss: 28.7567 - vae_kl_loss: 37.4286\n",
      "Epoch 465/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.1698 - vae_r_loss: 29.0692 - vae_kl_loss: 37.1006\n",
      "Epoch 466/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.5654 - vae_r_loss: 28.7419 - vae_kl_loss: 37.8236\n",
      "Epoch 467/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.7044 - vae_r_loss: 28.5779 - vae_kl_loss: 37.1265\n",
      "Epoch 468/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.6589 - vae_r_loss: 29.0076 - vae_kl_loss: 37.6514\n",
      "Epoch 469/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 66.8018 - vae_r_loss: 29.3652 - vae_kl_loss: 37.4366\n",
      "Epoch 470/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.9620 - vae_r_loss: 28.7492 - vae_kl_loss: 37.2128A: 0s - loss: 64.9829 - vae_r_loss: 27.7972 - vae_kl_los\n",
      "Epoch 471/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.9898 - vae_r_loss: 28.4286 - vae_kl_loss: 37.5611\n",
      "Epoch 472/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.4887 - vae_r_loss: 28.6869 - vae_kl_loss: 36.8018\n",
      "Epoch 473/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.5302 - vae_r_loss: 28.4200 - vae_kl_loss: 37.1102\n",
      "Epoch 474/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.7208 - vae_r_loss: 28.2913 - vae_kl_loss: 37.4295A: 0s - loss: 65.5559 - vae_r_loss: 28.2751 - vae_kl_loss: 37.2\n",
      "Epoch 475/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.8666 - vae_r_loss: 28.3649 - vae_kl_loss: 37.5017\n",
      "Epoch 476/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.2889 - vae_r_loss: 28.1783 - vae_kl_loss: 37.1106\n",
      "Epoch 477/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.5953 - vae_r_loss: 28.6572 - vae_kl_loss: 36.9381\n",
      "Epoch 478/1000\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 65.7469 - vae_r_loss: 28.6024 - vae_kl_loss: 37.1445\n",
      "Epoch 479/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/step - loss: 65.8140 - vae_r_loss: 29.0023 - vae_kl_loss: 36.8117\n",
      "Epoch 480/1000\n",
      "320/500 [==================>...........] - ETA: 0s - loss: 65.0843 - vae_r_loss: 27.9427 - vae_kl_loss: 37.1417"
     ]
    }
   ],
   "source": [
    "# größeres modell\n",
    "#Wichtig: Beim Output Filteranzahl = 3 -> Dreidimensionales Ergebnis für RGB Interpretation\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [128,128,128,512]\n",
    "    , encoder_conv_kernel_size = [2,2,3,4]\n",
    "    , encoder_conv_strides = [1,1,2,2]\n",
    "    , decoder_conv_t_filters = [512,128,128,3]\n",
    "    , decoder_conv_t_kernel_size = [4,3,2,2]\n",
    "    , decoder_conv_t_strides = [2,2,1,1]\n",
    "    , z_dim = 128\n",
    ")\n",
    "vae.save(RUN_FOLDER)\n",
    "\n",
    "vae.encoder.summary()\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "R_LOSS_FACTOR = 5000\n",
    "vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "vae.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")\n",
    "vae.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Higher Z Dim and Lower R Loss Factor\n",
    "\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [128,128,128,512]\n",
    "    , encoder_conv_kernel_size = [2,2,3,4]\n",
    "    , encoder_conv_strides = [1,1,2,2]\n",
    "    , decoder_conv_t_filters = [512,128,128,3]\n",
    "    , decoder_conv_t_kernel_size = [4,3,2,2]\n",
    "    , decoder_conv_t_strides = [2,2,1,1]\n",
    "    , z_dim = 400\n",
    ")\n",
    "vae.save(RUN_FOLDER)\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "R_LOSS_FACTOR = 250\n",
    "vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 250\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "vae.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")\n",
    "vae.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0000005\n",
    "R_LOSS_FACTOR = 1000\n",
    "vae.compile(LEARNING_RATE, R_LOSS_FACTOR)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 250\n",
    "PRINT_EVERY_N_BATCHES = 100\n",
    "INITIAL_EPOCH = 0\n",
    "\n",
    "vae.train(\n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    ")\n",
    "vae.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test again\n",
    "\n",
    "\n",
    "vae = VariationalAutoencoder(\n",
    "    input_dim = (32,32,3)\n",
    "    , encoder_conv_filters = [32,64,64, 64]\n",
    "    , encoder_conv_kernel_size = [3,3,3,3]\n",
    "    , encoder_conv_strides = [1,2,2,1]\n",
    "    , decoder_conv_t_filters = [64,64,32,1]\n",
    "    , decoder_conv_t_kernel_size = [3,3,3,3]\n",
    "    , decoder_conv_t_strides = [1,2,2,1]\n",
    "    , z_dim = 2\n",
    ")\n",
    "\n",
    "#vae.load_weights(\"C:\\\\Users\\\\adoerr\\\\Desktop\\\\Machine Learning\\\\Aufgabe 2\\\\run\\\\weights\\\\weights-033-34.17.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testindex = 4999\n",
    "img = x_train[testindex]\n",
    "#print(\"Correct label =\",label_name(y_test_numerical[testindex]))\n",
    "print(plt.imshow(img))\n",
    "img = img.reshape((1,) + img.shape)\n",
    "\n",
    "encoding = vae.encoder.predict(img)\n",
    "reconst = vae.decoder.predict(encoding)[0].squeeze()\n",
    "\n",
    "print(encoding)\n",
    "\n",
    "#filepath = os.path.join(self.run_folder, 'images','img_' + str(self.epoch).zfill(3) + '_' + str(batch) + '.jpg')\n",
    "#print(\"Prediction:\",vae.encoder.predict(img))\n",
    "#print(\"Prediction:\",vae.decoder.predict(vae.encoder.predict(img)))\n",
    "#print(vae.decoder.predict(vae.encoder.predict(img)).type\n",
    "#print(vae.encoder.predict)\n",
    "#plt.imshow(vae.decoder.predict(vae.encoder.predict(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.imshow(reconst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "testindex = 51\n",
    "img = x_test[testindex]\n",
    "plt.imshow(img)\n",
    "print(label_name(y_test_numerical[testindex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
